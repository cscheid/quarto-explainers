<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head>
  <meta charset="utf-8">
  <meta name="generator" content="quarto-(Local Development)">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Adjoints and Inverses</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="../site_libs/quarto-nav/quarto-nav.js"></script>
  <script src="../site_libs/quarto-html/quarto.js"></script>
  <script src="../site_libs/quarto-html/popper.min.js"></script>
  <script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
  <script src="../site_libs/quarto-html/clipboard.min.js"></script>
  <script src="../site_libs/quarto-html/anchor.min.js"></script>
  <script src="../site_libs/quarto-html/quarto-html.js"></script>
  <link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
  <link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
  <script src="../site_libs/bootstrap/bootstrap.min.js"></script>
  <link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
  <link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
<div id="quarto-search-results"></div>
 <!-- /navbar/sidebar -->
<div class="container-fluid quarto-container d-flex flex-column page-layout-article" id="quarto-content">
<div class="row flex-fill">
  <div id="quarto-toc-sidebar" class="col col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-toc order-last"><nav id="TOC" role="doc-toc">
<h2 id="toc-title">On this page</h2>
<ul>
<li><a href="#adjoints-are-nothing-like-inverses-except-when-they-are" class="nav-link active" data-scroll-target="#adjoints-are-nothing-like-inverses-except-when-they-are">Adjoints are (nothing) like inverses — except when they are</a>
<ul class="collapse">
<li><a href="#when-m-is-square-the-story-is-simple" class="nav-link" data-scroll-target="#when-m-is-square-the-story-is-simple">When <span class="math inline">\(M\)</span> is square, the story is simple</a></li>
<li><a href="#when-m-isnt-square" class="nav-link" data-scroll-target="#when-m-isnt-square">When <span class="math inline">\(M\)</span> isn’t square</a></li>
</ul></li>
<li><a href="#the-svd-always-rescues-us" class="nav-link" data-scroll-target="#the-svd-always-rescues-us">The SVD always rescues us</a></li>
<li><a href="#tying-it-back-together" class="nav-link" data-scroll-target="#tying-it-back-together">Tying it back together</a></li>
<li><a href="#acknowledgments" class="nav-link" data-scroll-target="#acknowledgments">Acknowledgments</a></li>
</ul>
</nav></div>
  <div class="col mx-auto col-sm-12 col-md-9 col-lg-7 px-lg-4 pe-xxl-4 ps-xxl-0">
<main>
<header id="title-block-header">
<div class="quarto-title-block"><div><h1 class="title">Adjoints and Inverses</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</header>

<p>tl;dr: Adjoints and inverses both relate column and row spaces. Often we think of them as being quite different, but also often we miraculously get away with using adjoints instead of inverses. Why? Because SVD.</p>
<p>“No, really, why?” Read on.</p>
<section id="adjoints-are-nothing-like-inverses-except-when-they-are" class="level2">
<h2 class="anchored" data-anchor-id="adjoints-are-nothing-like-inverses-except-when-they-are">Adjoints are (nothing) like inverses — except when they are</h2>
<p>A lot of data science and linear algebra revolves around finding ways to relate the vectors in the row space of a matrix to the vectors in the column space of a matrix. (In real matrix algebra, the adjoint is just the transpose. With complex matrix algebra you have to take complex conjugates of the entries as well.)</p>
<p>Let’s start with <span class="math inline">\(Mx = y\)</span>. Inverses give you <em>the</em> value <span class="math inline">\(x\)</span> from <span class="math inline">\(y\)</span>: <span class="math inline">\(x = M^{-1} y\)</span>. This only works sometimes, because <span class="math inline">\(M^{-1}\)</span> might not exist. Adjoints give you <em>one</em> value <span class="math inline">\(x'\)</span> in the column space <span class="math inline">\(x' = M^T y\)</span>, though <span class="math inline">\(x'\)</span> could be quite different from <span class="math inline">\(x\)</span>. This always works, because <span class="math inline">\(M^T\)</span> always exists.</p>
<p>Often, we get away with pretending the adjoint is “some kind of inverse”. For example, a number of “ML explanation” techniques, like <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4498753/">layer-wise relevance propagation</a>, are really “just” adjoints in disguise: they “invert” the ML classifier when a true inversion is not possible.</p>
<p>But why would that make any kind of sense, and when does that work well? Here we explain the situation for linear operators.</p>
<section id="when-m-is-square-the-story-is-simple" class="level3">
<h3 class="anchored" data-anchor-id="when-m-is-square-the-story-is-simple">When <span class="math inline">\(M\)</span> is square, the story is simple</h3>
<p>The inverse of a square matrix is the thing that says that if <span class="math inline">\(Mx = y\)</span>, then <span class="math inline">\(x = M^{-1} y\)</span>. But actually finding the inverse is hard, even if you don’t do it explicitly. If you know something about <span class="math inline">\(M\)</span>, often you want to compute something simpler.</p>
<p>For square matrices, if you know that <span class="math inline">\(M\)</span> is orthogonal (that is, <span class="math inline">\(\norm{Mx} = \norm{x}\)</span> for all <span class="math inline">\(x\)</span>), then <span class="math inline">\(M^T = M^{-1}\)</span>: the adjoint is precisely the inverse! In that case, recovering <span class="math inline">\(x\)</span> from <span class="math inline">\(y\)</span> is also computationally trivial.</p>
</section>
<section id="when-m-isnt-square" class="level3">
<h3 class="anchored" data-anchor-id="when-m-isnt-square">When <span class="math inline">\(M\)</span> isn’t square</h3>
<p>When if <span class="math inline">\(M\)</span> is not square, things get harder. The linear least squares problem is the direct generalization of “recover <span class="math inline">\(x\)</span> from <span class="math inline">\(y\)</span>” to non-square matrices. We are given a nonsquare matrix <span class="math inline">\(M\)</span> and “observations” <span class="math inline">\(y\)</span>, and then we set up the optimization problem</p>
<p><span class="math display">\[x^* = \argmin_x \norm{Mx - y}^2.\]</span></p>
<p>After some calculus and linear algebra, we get the answer:</p>
<p><span class="math display">\[x^* = (X^T X)^{-1} X^T y. \]</span></p>
<p>If <span class="math inline">\(X\)</span> has full column rank, then <span class="math inline">\(X^T X\)</span> is invertible. And here’s the interesting thing. If <span class="math inline">\((X^T X)^{-1} \approx I\)</span>, then <span class="math inline">\(x^* \approx X^T y\)</span>. This is the condition that makes the adjoint behave “like” the inverse!</p>
</section>
</section>
<section id="the-svd-always-rescues-us" class="level2">
<h2 class="anchored" data-anchor-id="the-svd-always-rescues-us">The SVD always rescues us</h2>
<p>One extra cool part about this is that if you replace the inverse <span class="math inline">\((X^T X)^{-1}\)</span> with the <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose pseudo-inverse</a>, <span class="math inline">\((X^T X)^+\)</span>, then the least-squares solution is always well-defined (and <span class="math inline">\(x^*\)</span> is then also the minimum-norm <span class="math inline">\(x\)</span> satisfying the <span class="math inline">\(\argmin\)</span> condition above).</p>
<p>Then, we remember that the Moore-Penrose pseudoinverse is very easy to state when we have the <a href="svd.html">SVD</a> of <span class="math inline">\(X\)</span>, <span class="math inline">\(X = U \Sigma V^T\)</span>:</p>
<p><span class="math display">\[X^+ = V \Sigma^+ U^T,\]</span></p>
<p>where</p>
<p><span class="math display">\[\Sigma^+_{ii} = \left \{ \begin{array}{rl} 1/\Sigma_{ii}, &amp;\textrm{if}\ \Sigma_{ii} \neq 0 \\ 0, &amp;\textrm{otherwise.} \end{array} \right .\]</span></p>
<p>We also know that <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal. With some more linear algebra, we can then conclude that <span class="math inline">\(X^T \approx (X^T X)^+ X^T\)</span> whenever the singular values of <span class="math inline">\(X\)</span> are close to <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span>.</p>
<p>This also works for square matrices. If all singular values are close to <span class="math inline">\(1\)</span>, then <span class="math inline">\(\norm{Mx} \approx \norm{x}\)</span>, and <span class="math inline">\(M^T x \\approx M^{-1} x\)</span>.</p>
</section>
<section id="tying-it-back-together" class="level2">
<h2 class="anchored" data-anchor-id="tying-it-back-together">Tying it back together</h2>
<p>The definition of orthogonal square matrices <span class="math inline">\(X^T = X^{-1}\)</span> (and so <span class="math inline">\(X^T X = I\)</span>) is equivalent to saying that all singular values are equal to <span class="math inline">\(1\)</span> exactly. When you allow non-square matrices, the “equality” between the adjoint <span class="math inline">\(X^T\)</span> and the “inverse” <span class="math inline">\(X^{-1}\)</span> (which cannot exist because <span class="math inline">\(X\)</span> is non-square!) also “allows” zero singular values.</p>
</section>
<section id="acknowledgments" class="level1">
<h1>Acknowledgments</h1>
<p>I learned this through <a href="http://www.reproducibility.org/RSF/book/bei/conj/paper.pdf">Claerbout’s article</a>, via <a href="https://twitter.com/sigfpe/status/1229073612042145793">Dan Piponi</a>.</p>
<!-- -->
<div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title anchored" id="quarto-embedded-source-code-modal-label" data-anchor-id="acknowledgments">Source Code</h5><button class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Adjoints and Inverses</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>tl;dr: Adjoints and inverses both relate column and row spaces. Often</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>we think of them as being quite different, but also often we</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>miraculously get away with using adjoints instead of inverses. Why?</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>Because SVD.</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>"No, really, why?" Read on.</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Adjoints are (nothing) like inverses --- except when they are</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>A lot of data science and linear algebra revolves around finding ways</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>to relate the vectors in the row space of a matrix to the vectors in</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>the column space of a matrix. (In real matrix algebra, the adjoint is</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>just the transpose. With complex matrix algebra you have to take complex</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>conjugates of the entries as well.)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>Let's start with $Mx = y$. Inverses give you *the* value $x$ from $y$:</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>$x = M^{-1} y$. This only works sometimes, because $M^{-1}$ might not</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>exist.  Adjoints give you *one* value $x'$ in the column space $x' =</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>M^T y$, though $x'$ could be quite different from $x$. This always works,</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>because $M^T$ always exists.</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>Often, we get away with pretending the adjoint is "some kind of</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>inverse". For example, a number of "ML explanation" techniques, like</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>[layer-wise relevance</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>propagation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4498753/),</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>are really "just" adjoints in disguise: they "invert" the ML</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>classifier when a true inversion is not possible.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>But why would that make any kind of sense, and when does that work</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>well? Here we explain the situation for linear operators.</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="fu">### When $M$ is square, the story is simple</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>The inverse of a square matrix is the thing that says that if $Mx =</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>y$, then $x = M^{-1} y$. But actually finding the inverse is hard,</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>even if you don't do it explicitly. If you know something about $M$,</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>often you want to compute something simpler. </span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>For square matrices, if you know that $M$ is orthogonal (that is,</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>$\norm{Mx} = \norm{x}$ for all $x$), then $M^T = M^{-1}$: the adjoint is</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>precisely the inverse! In that case, recovering $x$ from $y$ is also</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>computationally trivial.</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="fu">### When $M$ isn't square</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>When if $M$ is not square, things get harder. The linear least squares</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>problem is the direct generalization of "recover $x$ from $y$" to non-square</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>matrices. We are given a nonsquare matrix $M$ and "observations" $y$,</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>and then we set up the optimization problem</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>$$x^* = \argmin_x \norm{Mx - y}^2.$$</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>After some calculus and linear algebra, we get the answer:</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>$$x^* = (X^T X)^{-1} X^T y. $$</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>If $X$ has full column rank, then $X^T X$ is invertible. And here's</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>the interesting thing. If $(X^T X)^{-1} \approx I$, then $x^* \approx</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>X^T y$.  This is the condition that makes the adjoint behave</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>"like" the inverse!</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="fu">## The SVD always rescues us</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>One extra cool part about this is that if you replace the inverse</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>$(X^T X)^{-1}$ with the <span class="co">[</span><span class="ot">Moore-Penrose pseudo-inverse</span><span class="co">](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)</span>, $(X^T X)^+$,</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>then the least-squares solution is always well-defined (and $x^*$ is</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>then also the minimum-norm $x$ satisfying the $\argmin$ condition</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>above).</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>Then, we remember that the Moore-Penrose pseudoinverse is very easy to state</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>when we have the <span class="co">[</span><span class="ot">SVD</span><span class="co">](svd.html)</span> of $X$, $X = U \Sigma V^T$:</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>$$X^+ = V \Sigma^+ U^T,$$</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>where </span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>$$\Sigma^+_{ii} = \left \{ \begin{array}{rl} 1/\Sigma_{ii}, &amp;\textrm{if}\ \Sigma_{ii} \neq 0 <span class="sc">\\</span> 0, &amp;\textrm{otherwise.} \end{array} \right .$$</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>We also know that $U$ and $V$ are orthogonal. With some more linear algebra, we can then</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>conclude that $X^T \approx (X^T X)^+ X^T$ whenever the singular values of $X$ are</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>close to $1$ or $0$.</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>This also works for square matrices. If all singular values are close to $1$,</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>then $\norm{Mx} \approx \norm{x}$, and $M^T x <span class="sc">\\</span>approx M^{-1} x$.</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="fu">## Tying it back together</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>The definition of orthogonal square matrices $X^T = X^{-1}$</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>(and so $X^T X = I$) is equivalent to saying that all singular values</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>are equal to $1$ exactly. When you allow non-square matrices,</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>the "equality" between the adjoint $X^T$ and the "inverse" $X^{-1}$</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>(which cannot exist because $X$ is non-square!) also "allows" zero</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>singular values.</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="fu"># Acknowledgments</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>I learned this through [Claerbout's</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>article](http://www.reproducibility.org/RSF/book/bei/conj/paper.pdf),</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>via [Dan</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>Piponi](https://twitter.com/sigfpe/status/1229073612042145793).</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
<!-- -->
</section>
</main>
<div class="page-navigation ">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
  </div>
</div>
</div> <!-- /main column -->
</div> <!-- /row -->
</div> <!-- /container fluid -->


</body></html>