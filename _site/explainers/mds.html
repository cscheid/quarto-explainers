<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head>
  <meta charset="utf-8">
  <meta name="generator" content="quarto-(Local Development)">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Multidimensional Scaling</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="../site_libs/quarto-nav/quarto-nav.js"></script>
  <script src="../site_libs/quarto-html/quarto.js"></script>
  <script src="../site_libs/quarto-html/popper.min.js"></script>
  <script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
  <script src="../site_libs/quarto-html/clipboard.min.js"></script>
  <script src="../site_libs/quarto-html/anchor.min.js"></script>
  <script src="../site_libs/quarto-html/quarto-html.js"></script>
  <link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
  <link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
  <script src="../site_libs/bootstrap/bootstrap.min.js"></script>
  <link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
  <link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
<div id="quarto-search-results"></div>
 <!-- /navbar/sidebar -->
<div class="container-fluid quarto-container d-flex flex-column page-layout-article" id="quarto-content">
<div class="row flex-fill">
  <div id="quarto-toc-sidebar" class="col col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-toc order-last"><nav id="TOC" role="doc-toc">
<h2 id="toc-title">On this page</h2>
<ul>
<li><a href="#multidimensional-scaling" class="nav-link active" data-scroll-target="#multidimensional-scaling">Multidimensional scaling</a>
<ul class="collapse">
<li><a href="#classical-mds" class="nav-link" data-scroll-target="#classical-mds">Classical MDS</a></li>
<li><a href="#from-distances" class="nav-link" data-scroll-target="#from-distances">From distances</a></li>
<li><a href="#from-similarities" class="nav-link" data-scroll-target="#from-similarities">From similarities</a></li>
</ul></li>
<li><a href="#references" class="nav-link" data-scroll-target="#references">References</a></li>
</ul>
</nav></div>
  <div class="col mx-auto col-sm-12 col-md-9 col-lg-7 px-lg-4 pe-xxl-4 ps-xxl-0">
<main>
<header id="title-block-header">
<div class="quarto-title-block"><div><h1 class="title">Multidimensional Scaling</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</header>

<section id="multidimensional-scaling" class="level1">
<h1>Multidimensional scaling</h1>
<p>Multidimensional scaling refers to a wide range of techniques which, generally speaking, all try to take measurements of distances (or similarities) and turn those measurements into <strong>coordinates</strong> of points in a low-dimensional space.</p>
<section id="classical-mds" class="level2">
<h2 class="anchored" data-anchor-id="classical-mds">Classical MDS</h2>
<p>There are many, many variants of “multidimensional scaling”, with different assumptions on the measurements, and different ways to reconstruct the values. Here we will focus on only one of them, typically known as “Classical” multidimensional scaling, or CMDS. For more, the best reference is Borg and Groenen’s Modern Multidimensional Scaling<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
</section>
<section id="from-distances" class="level2">
<h2 class="anchored" data-anchor-id="from-distances">From distances</h2>
<p>Imagine that, instead of knowing the coordinates of a set of points, all you knew were the distance from each point to each other point. Intuitively, it seems like it should be possible to convert this information back into coordinates which respect the distances. Classical MDS attempts to do precisely that. Let’s start with the algorithm itself:</p>
<ol type="1">
<li><span class="math inline">\(D = (||X_i - X_j||^2)_{ij}\)</span>: let <span class="math inline">\(D\)</span> be a matrix that stores the <strong>squared</strong> distance from point <span class="math inline">\(i\)</span> to point <span class="math inline">\(j\)</span> (notice that we do not currently have the <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> vectors; we’re just using the notation to say what the entries of the matrix should store.</li>
<li><span class="math inline">\(P = (-1/2) H D H\)</span>: let <span class="math inline">\(P\)</span> be a row- and column-centered version of <span class="math inline">\(D\)</span>, multiplied by <span class="math inline">\(-1/2\)</span> (<span class="math inline">\(H\)</span> is the centering matrix, as described in the <a href="pca.html">PCA</a> section).</li>
<li>Let <span class="math inline">\(P = U \Lambda U^T\)</span>.</li>
<li>The coordinates of <span class="math inline">\(X\)</span> are given by the first few columns of <span class="math inline">\(U \Lambda^{1/2}\)</span>.</li>
</ol>
<p>The decision of how many coordinates to use for the MDS is similar to the decision of how many principal components to choose: the smaller the values in <span class="math inline">\(\Lambda\)</span>, the more precise the result will be (that is, the squared distance between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> will become closer to the actual value in <span class="math inline">\(D_{ij}\)</span>).</p>
<p>In fact, the easiest way to understand how CMDS works is to compare it directly to PCA. Specifically, let’s look at what happens if we started from an actual matrix <span class="math inline">\(X\)</span> from which to build <span class="math inline">\(D\)</span>. In this case,</p>
<p><span class="math display">\[\begin{align} D_{ij} &amp;=&amp; || X_i - X_j ||^2 \\ D_{ij} &amp;=&amp; \langle X_i - X_j, X_i - X_j \rangle\\ D_{ij} &amp;=&amp; \langle X_i, X_i \rangle + \langle X_j, X_j \rangle - 2 \langle X_i, X_j \rangle. \end{align}\]</span></p>
<p>So what we get is that every entry <span class="math inline">\(i, j\)</span> of the matrix <span class="math inline">\(D\)</span> can be represented by a sum of three terms. So we write the matrix <span class="math inline">\(D\)</span> as a sum of three matrices:</p>
<p><span class="math display">\[\begin{align} D &amp;=&amp; (\langle X_i, X_i \rangle)_{ij} + (\langle X_j, X_j \rangle)_{ij} + (-2 \langle X_i, X_j \rangle)_{ij}\\ D &amp;=&amp; A + B + C\end{align}\]</span></p>
<p>Now we follow step 2 of the algorithm above, and let</p>
<p><span class="math display">\[\begin{align} P &amp;=&amp; -1/2 H D H \\ P &amp;=&amp; -1/2 H (A + B + C) H \\ P &amp;=&amp; -(1/2) H A H -(1/2) H B H -(1/2) H C H \end{align}\]</span></p>
<p>Now note that all of the columns of <span class="math inline">\(A\)</span> are identical, because the values only depend on the row index <span class="math inline">\(i\)</span>. Similarly, all of the rows in <span class="math inline">\(B\)</span> are identical, because the values only depend on the column index <span class="math inline">\(j\)</span>. This means that <span class="math inline">\(HAH = HBH = 0\)</span>!, since column-centering <span class="math inline">\(A\)</span> will subtract the average column value; the analogous thing happens to row-centering <span class="math inline">\(B\)</span>. As a result,</p>
<p><span class="math display">\[\begin{align} P &amp;=&amp; -1/2 H D H \\ P &amp;=&amp; H (\langle X_i, X_j)_{ij} \rangle H \end{align}\]</span></p>
<p>So our <span class="math inline">\(P\)</span> matrix is now exactly equal to a centered matrix of inner products of <span class="math inline">\(X\)</span>, even though we never used the inner products directly – all we had access to was squared distances. So if we take <span class="math inline">\(P\)</span> to be the matrix <span class="math inline">\(M\)</span> on step 3 of our <a href="pca.html#PCA%20%through%20%a%20matrix%20of%20inner%20products">second PCA algorithm</a>, we get to recover <span class="math inline">\(X\)</span> exactly! This is really neat.</p>
</section>
<section id="from-similarities" class="level2">
<h2 class="anchored" data-anchor-id="from-similarities">From similarities</h2>
<p>The same algorithm also applies directly when all we have access to is a notion of <strong>similarity</strong> between points. Here the idea is even simpler. Let’s say that we have a way to give a numerical value of similarity between points such that if two points are similar to each other, the value is relatively large, and if two points are relatively dissimilar, the value is relatively small. Then we can pretend that this value is “like an inner product”, create a matrix of similarities, center the matrix in both rows and columns, and just compute the PCA directly like above. This will actually recover good coordinates the respect the similarities!</p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<!-- -->
<div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title anchored" id="quarto-embedded-source-code-modal-label" data-anchor-id="references">Source Code</h5><button class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Multidimensional Scaling</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu"># Multidimensional scaling</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>Multidimensional scaling refers to a wide range of techniques which,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>generally speaking, all try to take measurements of distances (or</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>similarities) and turn those measurements into **coordinates** of</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>points in a low-dimensional space. </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Classical MDS</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>There are many, many variants of "multidimensional scaling", with different assumptions on the measurements, and different ways to reconstruct the values. Here we will focus on only one of them, typically known as "Classical" multidimensional scaling, or CMDS. For more, the best reference is Borg and Groenen's Modern Multidimensional Scaling<span class="ot">[^1]</span>.</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">## From distances</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>Imagine that, instead of knowing the coordinates of a set of points,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>all you knew were the distance from each point to each other</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>point. Intuitively, it seems like it should be possible to convert</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>this information back into coordinates which respect the</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>distances. Classical MDS attempts to do precisely that. Let's start</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>with the algorithm itself:</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$D = (||X_i - X_j||^2)_{ij}$: let $D$ be a matrix that stores the **squared** distance</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>   from point $i$ to point $j$ (notice that we do not currently have</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>   the $X_i$ and $X_j$ vectors; we're just using the notation to say</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>   what the entries of the matrix should store.</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$P = (-1/2) H D H$: let $P$ be a row- and column-centered version of</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>   $D$, multiplied by $-1/2$ ($H$ is the centering matrix, as</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>   described in the <span class="co">[</span><span class="ot">PCA</span><span class="co">](pca.html)</span> section).</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Let $P = U \Lambda U^T$.</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>The coordinates of $X$ are given by the first few columns of $U \Lambda^{1/2}$.</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>The decision of how many coordinates to use for the MDS is similar to</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>the decision of how many principal components to choose: the smaller</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>the values in $\Lambda$, the more precise the result will be (that is,</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>the squared distance between $X_i$ and $X_j$ will become closer to the</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>actual value in $D_{ij}$).</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>In fact, the easiest way to understand how CMDS works is to compare it</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>directly to PCA. Specifically, let's look at what happens if we</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>started from an actual matrix $X$ from which to build $D$. In this</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>case,</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>$$\begin{align} D_{ij} &amp;=&amp; || X_i - X_j ||^2 <span class="sc">\\</span> D_{ij} &amp;=&amp; \langle X_i - X_j, X_i - X_j \rangle<span class="sc">\\</span> D_{ij} &amp;=&amp; \langle X_i, X_i \rangle + \langle X_j, X_j \rangle - 2 \langle X_i, X_j \rangle. \end{align}$$</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>So what we get is that every entry $i, j$ of the matrix $D$ can be represented by a sum of three terms. So we write the matrix $D$ as a sum of three matrices:</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>$$\begin{align} D &amp;=&amp; (\langle X_i, X_i \rangle)_{ij} + (\langle X_j, X_j \rangle)_{ij} + (-2 \langle X_i, X_j \rangle)_{ij}<span class="sc">\\</span> D &amp;=&amp; A + B + C\end{align}$$</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>Now we follow step 2 of the algorithm above, and let </span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>$$\begin{align} P &amp;=&amp; -1/2 H D H <span class="sc">\\</span> P &amp;=&amp; -1/2 H (A + B + C) H <span class="sc">\\</span> P &amp;=&amp; -(1/2) H A H -(1/2) H B H -(1/2) H C H \end{align}$$</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>Now note that all of the columns of $A$ are identical, because the values only depend on the row index $i$. Similarly, all of the rows in $B$ are identical, because the values only depend on the column index $j$. This means that $HAH = HBH = 0$!, since column-centering $A$ will subtract the average column value; the analogous thing happens to row-centering $B$. As a result,</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>$$\begin{align} P &amp;=&amp; -1/2 H D H <span class="sc">\\</span> P &amp;=&amp; H (\langle X_i, X_j)_{ij} \rangle H \end{align}$$</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>So our $P$ matrix is now exactly equal to a centered matrix of inner</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>products of $X$, even though we never used the inner products directly</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>-- all we had access to was squared distances. So if we take $P$ to be</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>the matrix $M$ on step 3 of our</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">second PCA algorithm</span><span class="co">](pca.html#PCA%20%through%20%a%20matrix%20of%20inner%20products)</span>,</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>we get to recover $X$ exactly! This is really neat.</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="fu">## From similarities</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>The same algorithm also applies directly when all we have access to is a notion of **similarity** between points. Here the idea is even simpler. Let's say that we have a way to give a numerical value of similarity between points such that if two points are similar to each other, the value is relatively large, and if two points are relatively dissimilar, the value is relatively small. Then we can pretend that this value is "like an inner product", create a matrix of similarities, center the matrix in both rows and columns, and just compute the PCA directly like above. This will actually recover good coordinates the respect the similarities!</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="fu"># References</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="ot">[^1]: </span>Borg, I. and Groenen, P.J.F. (2005). <span class="co">[</span><span class="ot">Modern multidimensional scaling. 2nd edition</span><span class="co">](http://people.few.eur.nl/groenen/mmds/)</span>. New York: Springer.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
<!-- -->
</section>
<section class="footnotes" role="doc-endnotes"><h2>Footnotes</h2>

<ol>
<li id="fn1" role="doc-endnote"><p>Borg, I. and Groenen, P.J.F. (2005). <a href="http://people.few.eur.nl/groenen/mmds/">Modern multidimensional scaling. 2nd edition</a>. New York: Springer.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</main>
<div class="page-navigation ">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
  </div>
</div>
</div> <!-- /main column -->
</div> <!-- /row -->
</div> <!-- /container fluid -->


</body></html>