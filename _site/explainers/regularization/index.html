<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head>
  <meta charset="utf-8">
  <meta name="generator" content="quarto-(Local Development)">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Regularization</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
  <script src="../../site_libs/quarto-html/quarto.js"></script>
  <script src="../../site_libs/quarto-html/popper.min.js"></script>
  <script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
  <script src="../../site_libs/quarto-html/clipboard.min.js"></script>
  <script src="../../site_libs/quarto-html/anchor.min.js"></script>
  <script src="../../site_libs/quarto-html/quarto-html.js"></script>
  <link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
  <link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
  <script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
  <link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
  <link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
  <script type="module" src="../../site_libs/quarto-ojs/ojs-bundle.js"></script>
  <link href="../../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
<div id="quarto-search-results"></div>
 <!-- /navbar/sidebar -->
<div class="container-fluid quarto-container d-flex flex-column page-layout-article" id="quarto-content">
<div class="row flex-fill">
  <div id="quarto-toc-sidebar" class="col col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-toc order-last"><nav id="TOC" role="doc-toc">
<h2 id="toc-title">On this page</h2>
<ul>
<li><a href="#ridge-regression" class="nav-link active" data-scroll-target="#ridge-regression">Ridge Regression</a>
<ul class="collapse">
<li><a href="#data-imputation" class="nav-link" data-scroll-target="#data-imputation">Data imputation</a></li>
<li><a href="#normalization-of-data" class="nav-link" data-scroll-target="#normalization-of-data">Normalization of data</a></li>
<li><a href="#effective-degrees-of-freedom" class="nav-link" data-scroll-target="#effective-degrees-of-freedom">Effective degrees of freedom</a></li>
</ul></li>
<li><a href="#ridge-regression-demo" class="nav-link" data-scroll-target="#ridge-regression-demo">Ridge Regression Demo</a></li>
</ul>
</nav></div>
  <div class="col mx-auto col-sm-12 col-md-9 col-lg-7 px-lg-4 pe-xxl-4 ps-xxl-0">
<main>
<header id="title-block-header">
<div class="quarto-title-block"><div><h1 class="title">Regularization</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</header>

<p>In the context of statistics, data mining, and machine learning – specifically when designing optimization-based methods for data fitting — regularization refers to the idea of choosing a model that purposefully does not fit the training data the best it could. The intuition is that, while we want complex models that can capture interesting features from the data, we want to prevent the model from fitting the noise in the training data, rather than the features.</p>
<p>In summary, regularization is a way to control the complexity of the model. In other words, we are talking about model selection. The first question one could ask is: “but why do we not control the model by explicitly choosing different models?” That is certainly one way to do model selection, but it is a surprisingly tricky one in practice. In simple cases (like linear regression), it is easy to compare two models to see which is more complex. But it’s not as simple to choose the <em>appropriate</em> model complexity.</p>
<p>In contrast, typical methods for regularization allow us to more easily connect the relationship of the amount of regularization to the amount of noise in the data.</p>
<p>The simplest example of regularization is known as “ridge regression”, and it builds on linear regression.</p>
<section id="ridge-regression" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression">Ridge Regression</h2>
<p>The ridge regression model is quite simple. Recall the typical linear least squares setup: <span class="math display">\[ X \beta = y \]</span></p>
<p>Here, we are looking to fit the best parameters <span class="math inline">\(\beta\)</span> to rows of the design matrix. Each input point <span class="math inline">\((v\_i, y\_i)\)</span> is mapped to some feature space encoded in the rows of <span class="math inline">\(X\)</span> (<span class="math inline">\(f(v\_i) = x\_{i\star}\)</span>). The <span class="math inline">\(\beta\)</span> parameters which minimize the expected squared error are: <span class="math display">\[ \hat{\beta} = (X^T X)^{-1} X^T y \]</span></p>
<p>Without regularization, if our design includes too many parameters (for example, if we try to fit a polynomial of too-high a degree), our model will <em>overfit</em>. This can be seen in the demo below by increasing the degree, the noise, and setting the regularization to a very low value. In ridge regression, we create an extra parameter <span class="math inline">\(\lambda\)</span>, and we want the extra parameter to control the complexity of the model. In short, the larger <span class="math inline">\(\lambda\)</span> is, the simpler we want our model to be. The error function for ridge regression is: <span class="math display">\[ E = || X \beta - y ||^2 + \lambda ||\beta||^2 \]</span></p>
<p>The solution for ridge regression is similar to that of linear least squares: <span class="math display">\[ \hat{\beta} = (X^T X + \lambda I)^{-1} X^T y \]</span></p>
<p>Notice that <span class="math inline">\(E\)</span> tries to balance two things: how bad the results are (the left term), and how large the vector of parameter values are. In other words, if we increase <span class="math inline">\(\lambda\)</span>, this new error term will tend to interpret large magnitudes in <span class="math inline">\(\beta\)</span> as a bad sign. At first, it’s puzzling that we would want the vector of parameter values to have a small magnitude.</p>
<section id="data-imputation" class="level3">
<h3 class="anchored" data-anchor-id="data-imputation">Data imputation</h3>
<p>Ridge regression is equivalent to doing typical least squares while adding “ghost” entries to the data set. If <span class="math inline">\(X\)</span> has <span class="math inline">\(n\)</span> columns, then you should be able to see that adding <span class="math inline">\(n\)</span> new data points to the dataset, where <span class="math inline">\(v\_{m+1} = (\sqrt{\lambda}, 0, \ldots, 0)\)</span>, <span class="math inline">\(v\_{m+2} = (0, \sqrt{\lambda}, 0, \ldots, 0)\)</span>, etc. and <span class="math inline">\(y\_{m+1} = y\_{m+2} = \cdots = 0\)</span>.</p>
<p>In this interpretation, we see that regularization is trying to push all parameters of <span class="math inline">\(\beta\)</span> uniformly to zero (since that’s the only way that <span class="math inline">\(\beta\)</span> will satisfy these specific values) by adding entries to the dataset that do not really exist.</p>
<p>In other words, regularization is equivalent to showing the training procedure a slight fiction (pessimistic towards zero), in order to not let the model get overexcited.</p>
</section>
<section id="normalization-of-data" class="level3">
<h3 class="anchored" data-anchor-id="normalization-of-data">Normalization of data</h3>
<p>When using ridge regression, it becomes important to make sure that your data is <em>normalized</em>: in other words, the values in each column should have mean zero, and variance 1.</p>
<p>This normalization can be seen to be necessary by considering the data imputation view.</p>
<p>If we do not set the mean of each column to zero, then regularization biases the model away from the data. That’s a very bad thing: if nothing else, our simplest models should be shooting for the average data point. Without normalization, they do not (you can confirm this by unchecking the normalization box in the demo below).</p>
<p>Without setting the variance of all the features to be the same, ridge regression will penalize some features more than others. This is easier to see again in the imputation view of ridge regression: each of the ghost entries pushes the solution equally to zero.</p>
<p>If the variance of the data is not one, then things <em>mostly</em> work, but regularization values become hard to compare across datasets, because the amount of regularization becomes relative to the variance on the specific datasets.</p>
</section>
<section id="effective-degrees-of-freedom" class="level3">
<h3 class="anchored" data-anchor-id="effective-degrees-of-freedom">Effective degrees of freedom</h3>
<p>The degrees of freedom of a model can be recovered from the trace of the Hat matrix. So we can look at the trace of the Hat matrix of the Ridge regression to recover the <em>effective</em> degrees of freedom. This notion of model complexity is more directly comparable across different models (the full story is more complicated, but this is a very useful fiction). The formula for the effective degrees of freedom in a model is: <span class="math display">\[ \textrm{eff-df} = \sum_i \left . \frac{\sigma_i^2}{\sigma_i^2 + \lambda} \right . \]</span></p>
<p>Play around with the demo below, and notice how models with different dimensions (measured by the degree of the polynomials being fit) but with the same effective degrees of freedom, tend to <em>look the same</em>.</p>
</section>
</section>
<section id="ridge-regression-demo" class="level2">
<h2 class="anchored" data-anchor-id="ridge-regression-demo">Ridge Regression Demo</h2>
<div id="div-ridge">

</div>
<p>Degree: <span id="span-degree"></span>.</p>
<div id="slider-degree" style="width: 300px; margin-bottom: 1em">

</div>
<p>Regularization: <span id="span-regularization"></span>.</p>
<div id="slider-regularization" style="width: 300px; margin-bottom: 1em">

</div>
<p>Noise level: <span id="span-noise"></span>.</p>
<div id="slider-noise" style="width: 300px; margin-bottom: 1em">

</div>
<div id="button-reseed" style="margin-top:1em">

</div>
<p>Effective degrees of freedom: <span id="span-effdf"></span></p>
<p>Normalize columns: <span id="span-normalize"></span></p>
<div class="cell">
<div class="cell-output-display hidden">
<div>
<div id="ojs-cell-1-1">

</div>
</div>
</div>
<div class="cell-output-display hidden">
<div>
<div id="ojs-cell-1-2">

</div>
</div>
</div>
<div class="cell-output-display hidden">
<div>
<div id="ojs-cell-1-3">

</div>
</div>
</div>
</div>
<!-- -->
<div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title anchored" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Regularization</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">echo:</span><span class="co"> false</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>In the context of statistics, data mining, and machine learning --</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>specifically when designing optimization-based methods for data</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>fitting --- regularization refers to the idea of choosing a model that</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>purposefully does not fit the training data the best it could. The</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>intuition is that, while we want complex models that can capture</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>interesting features from the data, we want to prevent the model from</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>fitting the noise in the training data, rather than the</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>features. </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>In summary, regularization is a way to control the complexity of the</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>model. In other words, we are talking about model selection. The first</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>question one could ask is: "but why do we not control the model by</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>explicitly choosing different models?" That is certainly one way to do</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>model selection, but it is a surprisingly tricky one in practice. In</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>simple cases (like linear regression), it is easy to compare two</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>models to see which is more complex. But it's not as simple to choose</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>the *appropriate* model complexity.</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>In contrast, typical methods for regularization allow us to more</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>easily connect the relationship of the amount of regularization to the</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>amount of noise in the data.</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>The simplest example of regularization is known as "ridge regression",</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>and it builds on linear regression.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ridge Regression</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>The ridge regression model is quite simple. Recall the typical linear</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>least squares setup:</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>$$ X \beta = y $$</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>Here, we are looking to fit the best parameters $\beta$ to rows of the</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>design matrix. Each input point $(v<span class="sc">\_</span>i, y<span class="sc">\_</span>i)$ is mapped to some</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>feature space encoded in the rows of $X$ ($f(v<span class="sc">\_</span>i) = x<span class="sc">\_</span>{i\star}$). The</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>$\beta$ parameters which minimize the expected squared error are:</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>$$ \hat{\beta} = (X^T X)^{-1} X^T y $$</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>Without regularization, if our design includes too many parameters</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>(for example, if we try to fit a polynomial of too-high a degree), our</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>model will *overfit*. This can be seen in the demo below by</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>increasing the degree, the noise, and setting the regularization to a</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>very low value. In ridge regression, we create an extra parameter</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>$\lambda$, and we want the extra parameter to control the complexity</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>of the model. In short, the larger $\lambda$ is, the simpler we want</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>our model to be. The error function for ridge regression is:</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>$$ E = || X \beta - y ||^2 + \lambda ||\beta||^2 $$</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>The solution for ridge regression is similar to that of linear least</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>squares:</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>$$ \hat{\beta} = (X^T X + \lambda I)^{-1} X^T y $$</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>Notice that $E$ tries to balance two things: how bad the results are</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>(the left term), and how large the vector of parameter values are. In</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>other words, if we increase $\lambda$, this new error term will tend</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>to interpret large magnitudes in $\beta$ as a bad sign. At first, it's</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>puzzling that we would want the vector of parameter values to have a</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>small magnitude.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="fu">### Data imputation</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>Ridge regression is equivalent to doing typical least squares while</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>adding "ghost" entries to the data set. If $X$ has $n$ columns, then</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>you should be able to see that adding $n$ new data points to the</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>dataset, where $v<span class="sc">\_</span>{m+1} = (\sqrt{\lambda}, 0, \ldots, 0)$, $v<span class="sc">\_</span>{m+2} =</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>(0, \sqrt{\lambda}, 0, \ldots, 0)$, etc. and $y<span class="sc">\_</span>{m+1} = y<span class="sc">\_</span>{m+2} =</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>\cdots = 0$.</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>In this interpretation, we see that regularization is trying to push</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>all parameters of $\beta$ uniformly to zero (since that's the only way</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>that $\beta$ will satisfy these specific values) by adding entries to</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>the dataset that do not really exist. </span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>In other words, regularization is equivalent to showing the training</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>procedure a slight fiction (pessimistic towards zero), in order to not</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>let the model get overexcited.</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normalization of data</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>When using ridge regression, it becomes important to make sure that</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>your data is *normalized*: in other words, the values in each column</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>should have mean zero, and variance 1. </span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>This normalization can be seen to be necessary by considering the data</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>imputation view.</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>If we do not set the mean of each column to zero, then regularization</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>biases the model away from the data. That's a very bad thing: if</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>nothing else, our simplest models should be shooting for the average</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>data point. Without normalization, they do not (you can confirm this</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>by unchecking the normalization box in the demo below). </span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>Without setting the variance of all the features to be the same, ridge</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>regression will penalize some features more than others. This is</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>easier to see again in the imputation view of ridge regression: each</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>of the ghost entries pushes the solution equally to zero.</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>If the variance of the data is not one, then things *mostly* work,</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>but regularization values become hard to compare across datasets,</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>because the amount of regularization becomes relative to the variance</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>on the specific datasets.</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="fu">### Effective degrees of freedom</span></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>The degrees of freedom of a model can be recovered from the trace of</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>the Hat matrix. So we can look at the trace of the Hat matrix of the</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>Ridge regression to recover the *effective* degrees of freedom. This</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>notion of model complexity is more directly comparable across</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>different models (the full story is more complicated, but this is a</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>very useful fiction). The formula for the effective degrees of freedom</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>in a model is:</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>$$ \textrm{eff-df} = \sum_i \left . \frac{\sigma_i^2}{\sigma_i^2 + \lambda} \right . $$</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>Play around with the demo below, and notice how models with different</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>dimensions (measured by the degree of the polynomials being fit) but</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>with the same effective degrees of freedom, tend to *look the same*.</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ridge Regression Demo</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">id</span><span class="ot">=</span><span class="st">"div-ridge"</span><span class="kw">&gt;&lt;/div&gt;</span></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>Degree: <span class="kw">&lt;span</span> <span class="er">id</span><span class="ot">=</span><span class="st">"span-degree"</span><span class="kw">&gt;&lt;/span&gt;</span>.</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">style</span><span class="ot">=</span><span class="st">"width: 300px; margin-bottom: 1em"</span> <span class="er">id</span><span class="ot">=</span><span class="st">"slider-degree"</span><span class="kw">&gt;&lt;/div&gt;</span></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>Regularization: <span class="kw">&lt;span</span> <span class="er">id</span><span class="ot">=</span><span class="st">"span-regularization"</span><span class="kw">&gt;&lt;/span&gt;</span>.</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">style</span><span class="ot">=</span><span class="st">"width: 300px; margin-bottom: 1em"</span> <span class="er">id</span><span class="ot">=</span><span class="st">"slider-regularization"</span><span class="kw">&gt;&lt;/div&gt;</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>Noise level: <span class="kw">&lt;span</span> <span class="er">id</span><span class="ot">=</span><span class="st">"span-noise"</span><span class="kw">&gt;&lt;/span&gt;</span>.</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">style</span><span class="ot">=</span><span class="st">"width: 300px; margin-bottom: 1em"</span> <span class="er">id</span><span class="ot">=</span><span class="st">"slider-noise"</span><span class="kw">&gt;&lt;/div&gt;</span></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">id</span><span class="ot">=</span><span class="st">"button-reseed"</span> <span class="er">style</span><span class="ot">=</span><span class="st">"margin-top:1em"</span><span class="kw">&gt;&lt;/div&gt;</span></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>Effective degrees of freedom: <span class="kw">&lt;span</span> <span class="er">id</span><span class="ot">=</span><span class="st">"span-effdf"</span><span class="kw">&gt;&lt;/span&gt;</span></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>Normalize columns: <span class="kw">&lt;span</span> <span class="er">id</span><span class="ot">=</span><span class="st">"span-normalize"</span><span class="kw">&gt;&lt;/span&gt;</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="in">```{ojs}</span></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a><span class="in">//| output: false</span></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a><span class="in">import { cscheid } from "/js/cscheid/cscheid.js";</span></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a><span class="in">import { main } from "./main.js";</span></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a><span class="in">{</span></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a><span class="in">  cscheid.setup.setupGlobals({ d3 });</span></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a><span class="in">  main();</span></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
<!-- -->
</section>
</main>
<div class="page-navigation ">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
  </div>
</div>
</div> <!-- /main column -->
</div> <!-- /row -->
</div> <!-- /container fluid -->
<script type="ojs-module-contents">
{"contents":[{"methodName":"interpret","cellName":"ojs-cell-1","inline":false,"source":"import { cscheid } from \"/js/cscheid/cscheid.js\";\nimport { main } from \"./main.js\";\n{\n  cscheid.setup.setupGlobals({ d3 });\n  main();\n}"}]}
</script>
<script type="module">
window._ojs.paths.runtimeToDoc = "../../explainers/regularization";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "../..";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>


</body></html>