<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang=""><head>
  <meta charset="utf-8">
  <meta name="generator" content="quarto-(Local Development)">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <title>Support Vector Machines</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
  <script src="../../site_libs/quarto-html/quarto.js"></script>
  <script src="../../site_libs/quarto-html/popper.min.js"></script>
  <script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
  <script src="../../site_libs/quarto-html/clipboard.min.js"></script>
  <script src="../../site_libs/quarto-html/anchor.min.js"></script>
  <script src="../../site_libs/quarto-html/quarto-html.js"></script>
  <link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
  <link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
  <script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
  <link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
  <link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="styles.css">
</head>
<body>
<div id="quarto-search-results"></div>
 <!-- /navbar/sidebar -->
<div class="container-fluid quarto-container d-flex flex-column page-layout-article" id="quarto-content">
<div class="row flex-fill">
  <div id="quarto-toc-sidebar" class="col col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-toc order-last"><nav id="TOC" role="doc-toc">
<h2 id="toc-title">On this page</h2>
<ul>
<li><a href="#demo" class="nav-link active" data-scroll-target="#demo">Demo</a></li>
</ul>
</nav></div>
  <div class="col mx-auto col-sm-12 col-md-9 col-lg-7 px-lg-4 pe-xxl-4 ps-xxl-0">
<main>
<header id="title-block-header">
<div class="quarto-title-block"><div><h1 class="title">Support Vector Machines</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</header>

<p>The soft-margin support vector machine (SVM) is a classic method for supervised learning.</p>
<p>It looks to find a <em>large-margin</em> classifier: one for which its decision boundary is far from the examplars. This is unlike the linear perceptron, which will not in general guarantee large margins. The SVM model minimizes the following loss:</p>
<p><span class="math display">\[ L(w) = \lambda ||w||^2 + \sum_{(x, y)} L_H(w, x, y) \]</span></p>
<p>Here, <span class="math inline">\(H_L\)</span> denotes the <em>hinge loss</em>:</p>
<p><span class="math display">\[ L_H(w, x, y) = \left \{ \begin{array}{rl} 1 - y \langle w, x \rangle &amp;, \textrm{if} \ y \langle w, x \rangle \le 1 \\ 0&amp;, \textrm{otherwise} \end{array} \right . \]</span></p>
<p>The hinge loss is particularly useful for classification because of two reasons. First, it is convex, which means that there exist algorithms that minimize <span class="math inline">\(L(w)\)</span> efficiently. Second, in the region where the misclassification loss (of a linear classifier) returns zero, the hinge loss has compact support, specifically in a way such that for points that are sufficiently far from the decision boundary, the hinge loss is zero.</p>
<p>Crucially, that second reason implies that the position of correctly classified points that are sufficiently away from the decision boundary <em>does not matter for the classifier</em>. One way to see this intuitively is that if you are given a classifier that attains the minimum, together with one point for which this classifier gives a hinge loss of zero, then if you wiggle this point in any direction, the loss will still be zero, and that means that the classifier will still be optimal, even with this wiggled point. (Of course, if you wiggle the point so much that it crosses into the region of the hinge loss where the value is non-zero, then you’ll potentially change the classifier.)</p>
<p>As a result, after the training procedure finishes, we can identify the subset of input points which influence the decision: these are the <strong>support vectors</strong> (they “support” the decision). Support vectors are particularly important in the “kernelized” formulation of the SVM. A general kernelized linear classifier needs to potentially access all training points to make a test-time prediction. The SVM, in contrast, needs only to store the support vectors, which might be a small fraction of all the input points.</p>
<p>You can see the support vectors in the example below, where we train a support vector machine with 2D data (using a quadratic polynomial kernel). As <span class="math inline">\(\lambda\)</span> gets smaller, the penalty for making hinge loss errors gets comparatively larger, so the margin of the classifier itself gets smaller. As a result, the number of <strong>support vectors</strong> gets smaller. Note how the support vectors are all points that are either misclassified, or are points inside the <span class="math inline">\([-1, 1]\)</span> range of the classification values: “within the margin”.</p>
<section id="demo" class="level2">
<h2 class="anchored" data-anchor-id="demo">Demo</h2>
<a name="#svm-plot"></a>
<div class="center-scaffold">
<div id="svm-surface">

</div>
<br>
<div>
<span class="math inline">\(\lambda\)</span>: <span id="svm-lambda" class="editable" contenteditable="true">0.1</span><br>(<a href="#svm-plot" id="svm-increase-lambda">increase</a>) (<a href="#svm-plot" id="svm-decrease-lambda">decrease</a>)
</div>
</div>
<script type="module" src="./main.js"></script>
<!-- -->
<div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title anchored" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Support Vector Machines</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>The soft-margin support vector machine (SVM) is a classic method for</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>supervised learning. </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>It looks to find a *large-margin* classifier: one for which its</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>decision boundary is far from the examplars. This is unlike the linear</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>perceptron, which will not in general guarantee large margins. The SVM</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>model minimizes the following loss:</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>$$ L(w) = \lambda ||w||^2 + \sum_{(x, y)} L_H(w, x, y) $$</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>Here, $H_L$ denotes the *hinge loss*: </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>$$ L_H(w, x, y) = \left <span class="sc">\{</span> \begin{array}{rl} 1 - y \langle w, x \rangle &amp;, \textrm{if} \ y \langle w, x \rangle \le 1 <span class="sc">\\</span> 0&amp;, \textrm{otherwise} \end{array} \right . $$</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>The hinge loss is particularly useful for classification because of</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>two reasons. First, it is convex, which means that there exist</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>algorithms that minimize $L(w)$ efficiently. Second, in the region</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>where the misclassification loss (of a linear classifier) returns</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>zero, the hinge loss has compact support, specifically in a way such</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>that for points that are sufficiently far from the decision boundary,</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>the hinge loss is zero.</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>Crucially, that second reason implies that the position of correctly</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>classified points that are sufficiently away from the decision</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>boundary *does not matter for the classifier*. One way to see this</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>intuitively is that if you are given a classifier that attains the</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>minimum, together with one point for which this classifier gives a</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>hinge loss of zero, then if you wiggle this point in any direction,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>the loss will still be zero, and that means that the classifier will</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>still be optimal, even with this wiggled point. (Of course, if you</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>wiggle the point so much that it crosses into the region of the hinge</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>loss where the value is non-zero, then you'll potentially change the</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>classifier.)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>As a result, after the training procedure finishes, we can identify the subset</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>of input points which influence the decision: these are</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>the **support vectors** (they "support" the decision). </span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>Support vectors are particularly</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>important in the "kernelized" formulation of the SVM.</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>A general kernelized linear classifier needs to potentially access all training points to</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>make a test-time prediction. The SVM, in contrast, needs only to store the</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>support vectors, which might be a small fraction of all the</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>input points.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>You can see the support vectors in the example below, where we train a</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>support vector machine with 2D data (using a quadratic polynomial</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>kernel). As $\lambda$ gets smaller, the penalty for making hinge loss</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>errors gets comparatively larger, so the margin of the classifier</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>itself gets smaller. As a result, the number of **support vectors**</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>gets smaller. Note how the support vectors are all points that are</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>either misclassified, or are points inside the $<span class="co">[</span><span class="ot">-1, 1</span><span class="co">]</span>$ range of the</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>classification values: "within the margin".</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="fu">## Demo</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;a</span> <span class="er">name</span><span class="ot">=</span><span class="st">"#svm-plot"</span><span class="kw">&gt;&lt;/a&gt;</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">class</span><span class="ot">=</span><span class="st">"center-scaffold"</span><span class="kw">&gt;&lt;div</span> <span class="er">id</span><span class="ot">=</span><span class="st">"svm-surface"</span><span class="kw">&gt;&lt;/div&gt;&lt;br&gt;&lt;div&gt;</span>$\lambda$: <span class="kw">&lt;span</span> <span class="er">class</span><span class="ot">=</span><span class="st">"editable"</span> <span class="er">contenteditable</span><span class="ot">=</span><span class="st">"true"</span> <span class="er">id</span><span class="ot">=</span><span class="st">"svm-lambda"</span><span class="kw">&gt;</span>0.1<span class="kw">&lt;/span&gt;&lt;br&gt;</span>(<span class="kw">&lt;a</span> <span class="er">href</span><span class="ot">=</span><span class="st">"#svm-plot"</span> <span class="er">id</span><span class="ot">=</span><span class="st">"svm-increase-lambda"</span><span class="kw">&gt;</span>increase<span class="kw">&lt;/a&gt;</span>) (<span class="kw">&lt;a</span> <span class="er">href</span><span class="ot">=</span><span class="st">"#svm-plot"</span> <span class="er">id</span><span class="ot">=</span><span class="st">"svm-decrease-lambda"</span><span class="kw">&gt;</span>decrease<span class="kw">&lt;/a&gt;</span>)<span class="kw">&lt;/div&gt;&lt;/div&gt;</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;script</span><span class="ot"> type=</span><span class="st">"module"</span> <span class="er">src</span><span class="ot">=</span><span class="st">"./main.js"</span><span class="kw">&gt;&lt;/script&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
<!-- -->
</section>
</main>
<div class="page-navigation ">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
  </div>
</div>
</div> <!-- /main column -->
</div> <!-- /row -->
</div> <!-- /container fluid -->


</body></html>