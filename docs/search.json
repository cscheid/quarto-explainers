[
  {
    "href": "index.html",
    "title": "Index",
    "section": "",
    "text": "Go to the main index page.\n\nSource Code\n---\ntitle: \"Index\"\n---\n\nGo to [the main index page](explainers/index.qmd)."
  },
  {
    "href": "explainers/logistic-regression/index.html#why-logistic-regression",
    "title": "Logistic Regression",
    "section": "Why Logistic Regression?",
    "text": "There are many ways to ask the question “why logistic regression?”. One natural question to ask is: if we already know how to build linear regression models, but now are trying to build classifiers, why not simply use a linear regression model and round the prediction to a binary value?\nTBF: demo of why this idea doesn’t work.\n\nSingle-class Logistic Regression\n\n\n\n\n\nMulti-class Logistic Regression\nTBF.\n\n\n\n\nSource Code\n---\ntitle: Logistic Regression\nlayout: d3_project\n---\n\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.4/lodash.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js\"></script>\n<script src=\"https://cscheid.net/js/cscheid/blas.js\"></script>\n<script src=\"https://cscheid.net/js/cscheid/plot.js\"></script>\n\n# Logistic Regression\n\nLogistic regression is one of the simplest ways to build\n*classifiers*: models that predict binary outcomes (in contrast to\n[regression models](../linear_regression.html), which predict numerical\nvalues).\n\n## Why Logistic Regression?\n\nThere are many ways to ask the question \"why logistic\nregression?\". One natural question to ask is: if we already know how\nto build linear regression models, but now are trying to build\nclassifiers, why not simply use a linear regression model and round\nthe prediction to a binary value?\n\nTBF: demo of why this idea doesn't work.\n\n### Single-class Logistic Regression\n\n<div id=\"div-plot\"></div>\n\n### Multi-class Logistic Regression\n\nTBF. <div id=\"div-plot2\"></div>"
  },
  {
    "href": "explainers/ova-ava/index.html#ova",
    "title": "OVA and AVA",
    "section": "OVA",
    "text": "TODO: show the decision boundaries of three OVA logistic classifiers"
  },
  {
    "href": "explainers/ova-ava/index.html#ava",
    "title": "OVA and AVA",
    "section": "AVA",
    "text": "TODO: show the decision boundaries of three AVA logistic classifiers\nTODO: derive AVA proof bound\n\nSource Code\n---\ntitle: OVA and AVA\nlayout: bootstrap_wide\n---\n\n# OVA and AVA\n\n## OVA\n\nTODO: show the decision boundaries of three OVA logistic classifiers\n\n## AVA\n\nTODO: show the decision boundaries of three AVA logistic classifiers\n\nTODO: derive AVA proof bound"
  },
  {
    "href": "explainers/aliasing.html",
    "title": "Aliasing",
    "section": "",
    "text": "This tweet shows the phenomenon of aliasing through a very effective animation. The notebook port here provides a little bit of interactivity so you can explore what happens when a signal is not sampled often enough in a reconstruction:\n\nA well-sampled signal must be sampled at least twice as often as the highest frequency in the original signal. It’s not obvious yet always possible to reconstruct a signal exactly if it’s been sampled at a sufficiently-high rate.\nA signal sampled less frequently than that will alias: no matter what the algorithm, it’s not possible to always reconstruct the correct frequency of the true signal.\n\n(I first posted this directly on Observable.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource Code\n---\ntitle: \"Aliasing\"\nexecute:\n  echo: false\n---\n\n```{ojs}\nviewof signalWidth = Inputs.range([4, 100], {value: 40, step: 0.01, label: \"Signal Bandwidth\"})\n```\n\n```{ojs}\ndiagram = {\n  debugger;\n  const sliderPos = document.querySelectorAll(\"input[type='range']\")[0].getBoundingClientRect();\n  const mainPos = document.querySelector(\"main\").getBoundingClientRect().x;\n  const svg = htl.svg`<svg width=\"${width}\" height=\"20\"></svg>`;\n  const xLeft = sliderPos.x - mainPos; // account for margin :shrug:\n  const xRight = xLeft + sliderPos.width;\n  const xScale = d3.scaleLinear().domain([4, 100]).range([xLeft, xRight]);\n  const criticalBandWidth = xScale(signalWidth);\n  const goodOpacity = 2 * combWidth > signalWidth ? 0.2 : 1.0;\n  const badOpacity = 2 * combWidth > signalWidth ? 1.0 : 0.2;\n  \n  const badColor = d3.lab(70, 30, 0);\n  const goodColor = d3.lab(70, -30, 0);\n  const goodSampling = htl.svg`<path stroke=\"${goodColor}\" opacity=\"${goodOpacity}\" fill=\"none\"></path>`;\n  const badSampling = htl.svg`<path stroke=\"${badColor}\" opacity=\"${badOpacity}\" fill=\"none\"></path>`;\n  const line = d3.line();\n  goodSampling.setAttribute(\"d\", line([[xLeft, 18], [criticalBandWidth-2, 18], [criticalBandWidth-2, 2]]));\n  badSampling.setAttribute(\"d\", line([[xRight, 18], [criticalBandWidth+2, 18], [criticalBandWidth+2, 2]]));\n  const goodLabel = htl.svg`<text x=${criticalBandWidth-5} opacity=\"${goodOpacity}\" fill=\"${goodColor}\" font-size=\".9em\" y=15 text-anchor=\"end\">Good</text>`;\n  const badLabel = htl.svg`<text x=${criticalBandWidth+5} opacity=\"${badOpacity}\" fill=\"${badColor}\" font-size=\".9em\" y=15>Aliasing</text>`;\n  svg.appendChild(goodLabel);\n  svg.appendChild(badLabel);\n  svg.appendChild(goodSampling);\n  svg.appendChild(badSampling);\n//  svg.appendChild(htl.svg`<line x1=${xLeft} x2=${xLeft} y1=0 y2=20 stroke=\"black\"></line>`);\n//  svg.appendChild(htl.svg`<line x1=${xRight} x2=${xRight} y1=0 y2=20 stroke=\"black\"></line>`);\n//  svg.appendChild(htl.svg`\n  return svg;\n}\n```\n\n```{ojs}\nviewof combWidth = Inputs.range([2, 50], {value: 37, step: 0.01, label: \"Sampling Bandwidth\"})\n```\n\n```{ojs}\nviewof animated = Inputs.toggle({label: \"Animate signal\", value: true})\n```\n\n```{ojs}\nsvg = {\n  d3.select(staticSVG.el).selectAll(\"path\").attr(\"transform\", `translate(${-phase}, 50)`);\n  return staticSVG.el;\n}\n```\n\n[This tweet](https://twitter.com/jagarikin/status/1419165889950142465) shows the phenomenon of [*aliasing*](https://en.wikipedia.org/wiki/Aliasing)  through a very effective animation. The notebook port here provides a little bit of interactivity so you can explore what happens when a signal is not sampled often enough in a reconstruction:\n\n* A well-sampled signal must be sampled at least twice as often as the highest frequency in the original signal. It's not obvious yet [always possible](https://en.wikipedia.org/wiki/Whittaker%E2%80%93Shannon_interpolation_formula) to reconstruct a signal exactly if it's been sampled at a sufficiently-high rate.\n\n* A signal sampled less frequently than that will **alias**: no matter what the algorithm, it's not possible to always reconstruct the correct frequency of the true signal.\n\n(I first posted this directly [on Observable](https://observablehq.com/@cscheid/aliasing/).)\n\n```{ojs}\n//| output: false\nphase = {\n  let result = 0;\n  if (animated) {\n    result = (elapsed * signalWidth) % signalWidth;\n  }\n  return result;\n}\nnowAtDocumentBoot = {\n  return Number(new Date()); // we need this to avoid numerical issues with huge phases\n}\nelapsed = (now - nowAtDocumentBoot) / 1000;\nstaticSVG = {\n  console.log(\"Foo\");\n  const svg = htl.svg`<svg width=${width} height=100></svg>`\n  const sel = d3.select(svg);\n  sel\n    .append(\"path\")\n    .attr(\"d\", signal)\n    .attr(\"stroke\", \"red\")\n    .attr(\"fill\", \"none\");\n  const coverage = (combWidth - 2) / combWidth;\n  const count = ~~(width / 2) / combWidth;\n  combMask(sel, coverage, combWidth, count)\n    .attr(\"transform\", \"translate(100, 0)\");\n  return { el: svg };\n}\nfunction sineCurve(freqInInvPixels, amplitudeInPixels) {\n  const nPoints = 2 * width; \n  const curveScale = d3.scaleLinear().domain([0, nPoints]).range([0, width * Math.PI * freqInInvPixels])\n  const xScale = d3.scaleLinear().domain([0, nPoints]).range([0, width]);\n  const line = d3.line();\n  const points = d3.range(~~(2 * width + 4/freqInInvPixels)).map(x => [xScale(x), Math.sin(curveScale(x)) * amplitudeInPixels]);\n  return line(points);\n}\nsignal = sineCurve(2/signalWidth, 30)\nfunction combMask(svg, coverage, width, count) {\n  return svg\n      .append(\"g\")\n      .selectAll(\"rect\")\n      .data(d3.range(count))\n      .join(\"rect\")\n      .attr(\"x\", d => d * width)\n      .attr(\"width\", width * coverage)\n      .attr(\"fill\", \"rgb(240, 240, 240)\")\n      .attr(\"y\", 0)\n      .attr(\"height\", 100)\n}\n```"
  },
  {
    "href": "explainers/eigenvectors/index.html#points-transformed-by-a-symmetric-2x2-matrix",
    "title": "Eigenwhat?",
    "section": "Points transformed by a symmetric 2x2 matrix",
    "text": "Enter the values for M here\n\n\n\n\\(M_{00}\\) \n\\(M_{01}\\) \n\n\n\\(M_{10}\\) 1\n\\(M_{11}\\) \n\n\n\n\n\nEigenvectors and eigenvalues\n\n\n\n\\(U_{00}\\) \n\\(U_{01}\\) \n\n\n\\(U_{10}\\) \n\\(U_{11}\\) \n\n\n\n\n\n\n\\(\\lambda_0\\) = \n\\(\\lambda_1\\) ="
  },
  {
    "href": "explainers/eigenvectors/index.html#known-bugs",
    "title": "Eigenwhat?",
    "section": "Known bugs",
    "text": "Eigenvalue multiplicity (two eigenvectors not aligned with one another with equal eigenvalues) will mean that there are more than 2 unit-length eigenvectors, and my crappy power iteration algorithm stops working in that case.\nThe rotation transition for \\(U\\) and \\(U^T\\) should be an actual rotation. For small rotations the linear interpolation looks fine, but for bigger ones it’s hard to see what’s going on.\nWe should arbitrarily flip the eigenvector signs such that we get a smaller rotation on \\(U^T\\) and \\(U\\). Sometimes my crappy power iteration algorithm gives the “bad” eigenvector, and that makes it hard to see what’s going on.\nIf one of the eigenvalues is zero, my crappy power iteration algorithm gives a bad eigenvector (notice a theme here?)"
  },
  {
    "href": "explainers/eigenvectors/index.html#more-reading",
    "title": "Eigenwhat?",
    "section": "More reading",
    "text": "If you really want to understand eigenvectors and eigenvalues, the best thing to read continues to be chapter 5 of Shewchuk’s classic Introduction to the Conjugate Gradient Method Without the Agonizing Pain.\nThe particular presentation in this demo was inspired by Blinn’s also-classic Consider the lowly 2x2 matrix.\n\n\n\nSource Code\n---\ntitle: Eigenwhat?\n---\n\nThis demo will help you build intuition for the behavior of\neigenvectors and eigenvalues of a 2x2 symmetric real matrix. \n\nAn **eigenvector** $v$ of a matrix $M$ is any vector that satisfies\nthe following equation:\n\n$$Mv = \\lambda v$$\n\nIn words, if you transform a vector $v$ by a matrix $M$ and you end up\nwith a scaled version of itself, then $v$ is an eigenvector. The\namount by which the vector is scaled is called an eigenvalue of $M$.\n\nWhen the 2x2 matrix is symmetric, then there exist two eigenvectors\nthat are orthogonal to each other. In that case, we can write the\nmatrix as:\n\n$$ M = U \\Sigma U^T $$\n\nwhere $U$ is a matrix holding the eigenvectors and $\\Sigma$ is a\ndiagonal matrix where the entries in the diagonal are the\neigenvalues. If you'll remember from linear algebra, every time you\nhave a square matrix whose rows (or columns) are orthogonal to each\nother, that is a **rotation** matrix and, in addition, rotation\nmatrices are such that their transposes are their inverses. So a good\nway to think about this is that eigenvectors give you a **decomposition** of the matrix M into simpler matrices.\n\nIn other words, the operation of every symmetric matrix $M$ on a\nvector $v$ is $Mv = U \\Sigma U^T v$, or:\n\n* $U^T v$: transform the vector $v$ to the \"eigenspace\": this is a rotation\n* $\\Sigma U^T v$: in the eigenspace, scale the vector's coordinates by the eigenvalues\n* $Mv = U \\Sigma U^T v$: transform the scaled vector back to the original basis\n\nIn the interactive demo below, the unit-length eigenvectors are\nrepresented by the red dots.\n\n## Points transformed by a symmetric 2x2 matrix\n\n<div id=\"transform\"></div>\n<div id=\"matrix-operation\"></div>\n\n### Enter the values for M here\n\n----------------------------------------------------------------------------------   ----------------------------------------------------------------------------------\n$M_{00}$   <input type=\"number\" id=\"m00\" min=\"-10\" max=\"10\" value=\"4\" step=\"0.05\">   $M_{01}$   <input type=\"number\" id=\"m01\" min=\"-10\" max=\"10\" value=\"1\" step=\"0.05\">\n$M_{10}$   <span id=\"m10\">1</span>                                                   $M_{11}$   <input type=\"number\" id=\"m11\" min=\"-10\" max=\"10\" value=\"2\" step=\"0.05\">\n----------------------------------------------------------------------------------   ----------------------------------------------------------------------------------\n\n### Eigenvectors and eigenvalues\n\n-------------------------------    -------------------------------\n$U_{00}$ <span id=\"u00\"></span>    $U_{01}$ <span id=\"u01\"></span>\n$U_{10}$ <span id=\"u10\"></span>    $U_{11}$ <span id=\"u11\"></span>\n-------------------------------    -------------------------------\n\n-----------------------------------    -----------------------------------\n$\\lambda_0$ = <span id=\"l0\"></span>    $\\lambda_1$ = <span id=\"l1\"></span>\n-----------------------------------    -----------------------------------\n\n## Known bugs\n\n* Eigenvalue multiplicity (two eigenvectors not aligned with one\n  another with equal eigenvalues) will mean that there are more than 2\n  unit-length eigenvectors, and my crappy power iteration algorithm\n  stops working in that case.\n* The rotation transition for $U$ and $U^T$ should be an actual\n  rotation. For small rotations the linear interpolation looks fine,\n  but for bigger ones it's hard to see what's going on.\n* We should arbitrarily flip the eigenvector signs such that we get a\n  smaller rotation on $U^T$ and $U$. Sometimes my crappy power\n  iteration algorithm gives the \"bad\" eigenvector, and that makes it\n  hard to see what's going on.\n* If one of the eigenvalues is zero, my crappy power iteration\n  algorithm gives a bad eigenvector (notice a theme here?)\n\n## More reading\n\nIf you really want to understand eigenvectors and eigenvalues, the\nbest thing to read continues to be chapter 5 of Shewchuk's classic\n[Introduction to the Conjugate Gradient Method Without the Agonizing Pain](http://www.cs.cmu.edu/~./quake-papers/painless-conjugate-gradient.pdf).\n\nThe particular presentation in this demo was inspired by Blinn's also-classic [Consider the lowly 2x2 matrix](http://ieeexplore.ieee.org/document/486688/).\n\n<script src=\"./gl-matrix.js\"></script>\n<script type=\"module\" src=\"./main.js\"></script>"
  },
  {
    "href": "explainers/probabilities.html",
    "title": "Probabilities and probability spaces",
    "section": "",
    "text": "Probabilities and probability spaces\nThere are entire textbooks entirely dedicated to probability (and many, many university courses), so of course a single webpage will not do it justice.\n(To be written.)\n\nSource Code\n---\ntitle: Probabilities and probability spaces\n---\n\n# Probabilities and probability spaces\n\nThere are entire textbooks entirely dedicated to probability (and\nmany, many university courses), so of course a single webpage will not\ndo it justice.\n\n(To be written.)"
  },
  {
    "href": "explainers/nearest-neighbors/index.html",
    "title": "Nearest-neighbor classification",
    "section": "",
    "text": "Source Code\n---\ntitle: Nearest-neighbor classification\n---\n\n<div id=\"knn-surface\"></div>\n\n```{ojs}\n//| echo: false\nviewof k = Inputs.range([1, 100], {label: \"number of neighbors\", value: 30, step: 1});\n```\n\n```{ojs}\n//| echo: false\n//| output: false\n{\n  window.updatePlot(k);\n}\n```\n\n<script type=\"module\" src=\"./main.js\"></script>"
  },
  {
    "href": "explainers/trigonometry/index.html#the-basics",
    "title": "Trigonometry",
    "section": "The Basics",
    "text": "Just like everyone else on twitter, when I saw this diagram below my reaction was: “why haven’t I been shown this 25 years ago?” The lengths of the lines correspond to the values of the trigonometric functions. Drag the point to change the diagram around."
  },
  {
    "href": "explainers/trigonometry/index.html#complex-exponentials-ftw",
    "title": "Trigonometry",
    "section": "Complex exponentials FTW",
    "text": "This is my favorite trig trick. Never memorize a formula for sines of sums, differences, or fractions again. Start from \\[\\begin{eqnarray*}e^{ix} &=& \\cos x + i \\sin x\\\\e^{-ix} &=& \\cos -x + i \\sin -x = \\cos x - i \\sin x\\end{eqnarray*}\\]\nFrom these two, you get that \\[\\begin{eqnarray*}\\cos x &=& \\frac{e^{ix} + e^{-ix}}{2}\\\\\\sin x &=& \\frac{e^{ix} - e^{-ix}}{2i}\\end{eqnarray*}\\]\nNow expressions like \\(\\sin (a+b)\\) are obvious to work out instead of big and scary. You only need memorize those formulas above, and from them you can derive many of the annoying high school formulas."
  },
  {
    "href": "explainers/trigonometry/index.html#references",
    "title": "Trigonometry",
    "section": "References",
    "text": "Trigonometry and Complex Exponentials, William Stein.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource Code\n---\ntitle: Trigonometry\necho: false\n---\n\n## The Basics\n\n[Just like everyone else on twitter](https://twitter.com/divbyzero/status/927517766369804289),\nwhen I saw this diagram below my reaction was: \"why haven't I been\nshown this 25 years ago?\" The lengths of the lines correspond to the\nvalues of the trigonometric functions. Drag the point to change the\ndiagram around.\n\n```{ojs}\nbasicTrig = htl.html`<div id=\"basic-trig\"></div>`\n```\n\n## Complex exponentials FTW\n\nThis is my favorite trig trick. Never memorize a formula for sines of sums, differences, or fractions again. Start from\n\n$$\\begin{eqnarray*}e^{ix} &=& \\cos x + i \\sin x\\\\e^{-ix} &=& \\cos -x + i \\sin -x = \\cos x - i \\sin x\\end{eqnarray*}$$\n\nFrom these two, you get that \n\n$$\\begin{eqnarray*}\\cos x &=& \\frac{e^{ix} + e^{-ix}}{2}\\\\\\sin x &=& \\frac{e^{ix} - e^{-ix}}{2i}\\end{eqnarray*}$$\n\nNow expressions like $\\sin (a+b)$ are obvious to work out instead of\nbig and scary. You only need memorize those formulas above, and from\nthem you can derive many of the annoying high school formulas.\n\n## References\n\n1. [Trigonometry and Complex Exponentials](http://wstein.org/edu/winter06/20b/notes/html/node30.html), William Stein.\n\n```{ojs}\n//| output: false\nimport { cscheid } from \"/js/cscheid/cscheid.js\";\nimport { draw } from \"./main.js\";\n{\n  cscheid.setup.setupGlobals({ d3 });\n  draw(basicTrig);\n}\n```"
  },
  {
    "href": "explainers/automatic-differentiation/index.html#introduction",
    "title": "Automatic Differentiation",
    "section": "Introduction",
    "text": "Derivatives show up everywhere in data science, and most notably in optimization, because of gradient descent (and its variants). Writing derivatives by hand is error-prone and annoying, especially for complicated expressions. Automatic differentiation (“autodiff”) is an algorithm that solves all of these problems: evaluating gradients is usually only as expensive as evaluating the functions themselves, and since it’s the computer doing it, you’re never going to miss a term again. Autodiff is also the backbone of libraries like PyTorch and TensorFlow.\nAutomatic differentiation works by traversing a data structure representing the value we want to compute. People call this “expression graph”, “expression tree”, or an “computation graph”: they’re the same thing. Usually, we ask Python to compute the value of an expression directly:\n>>> 3 + (5 * 6)\n33\nInstead, we can build a data structure representing that, and ask Python to derive the value:\nclass Variable:\n    def __init__(self, value):\n        self.v = value\n    def evaluate(self):\n        return self.v\n\nclass Sum:\n    def __init__(self, left, right):\n        self.l = left\n        self.r = right\n    def evaluate(self):\n        return self.l.evaluate() + self.r.evaluate()\n\nclass Mult:\n    def __init__(self, left, right):\n        self.l = left\n        self.r = right\n    def evaluate(self):\n        return self.l.evaluate() * self.r.evaluate()\n\n>>> x = Sum(Variable(3), Mult(Variable(5), Variable(6)))\n>>> x.evaluate()\n33\n(This is, in fact, pretty much how Python actually evaluates expressions internally.)"
  },
  {
    "href": "explainers/automatic-differentiation/index.html#given-a-computation-graph-derivatives-are-local",
    "title": "Automatic Differentiation",
    "section": "Given a computation graph, derivatives are local",
    "text": "The first important bit of intuition on how derivatives can be computed comes from the way the chain rule works. You’ve probably seen the chain rule written in this way: \\[ \\frac{df}{dx} = \\frac{df}{du} \\times \\frac{du}{dx} \\]\nThis version of the chain rule says that if \\(f\\) depends on \\(x\\) only through \\(u\\), and \\(u\\) depends on \\(x\\), then the derivative \\(df/dx\\) is the product of the “immediate derivative” from \\(f\\) to \\(u\\), \\(df/du\\), and the “immediate derivative” from \\(u\\) to \\(x\\), \\(du/dx\\).\nThis lets us compute the derivative of \\(f = \\sin (\\cos x)\\) knowing only the base case rules that if \\(u = \\sin x\\), then \\(du/dx = \\cos x\\), and if \\(v = \\cos x\\), then \\(dv/dx = -\\cos{x}\\): \\[ df/dx = \\cos (\\cos x) \\times - \\sin (x) \\]"
  },
  {
    "href": "explainers/automatic-differentiation/index.html#just-carry-the-derivatives-with-the-values",
    "title": "Automatic Differentiation",
    "section": "Just carry the derivatives with the values",
    "text": "If you inspect the source code for the Python evaluation library we wrote above, the evaluation of each node needs only to know the values of its immediate neighbors. Forward-mode autodiff simply takes this idea one step further, and carries the derivatives together with the values:\nclass Variable:\n    def __init__(self, value, derivative):\n        self.v = value\n        self.d = derivative\n    def evaluate(self):\n        return (self.v, self.d)\n\nclass Sum:\n    def __init__(self, left, right):\n        self.l = left\n        self.r = right\n    def evaluate(self):\n        (left_v, left_d) = self.l.evaluate()\n        (right_v, right_d) = self.r.evaluate()\n        return (left_v + right_v, left_d + right_d)\n\nclass Mult:\n    def __init__(self, left, right):\n        self.l = left\n        self.r = right\n    def evaluate(self):\n        (left_v, left_d) = self.l.evaluate()\n        (right_v, right_d) = self.r.evaluate()\n        return (left_v * right_v,\n                left_v * right_d + left_d * right_v) # product rule\n\n>>> a = Variable(3, 0)\n>>> b = Variable(5, 1)\n>>> c = Variable(6, 0)\n>>> x = Sum(a, Mult(b, c))\n>>> x.evaluate()\n(33, 6)\nThere is a bit of a trick going on, which is how we encode which variable we are taking the derivative over. We simply ask variables to store the values of their derivatives with respect to whatever variable we’re interested in. In our example, we want to take a derivative over \\(b\\), and so \\(da/db = 0, db/db = 1, dc/db = 0\\).\nOf course, you have to teach your library about other expressions and derivative rules as well:\nclass Sin:\n    def __init__(self, v):\n        self.v = v\n    def evaluate(self):\n        (v, d) = self.v.evaluate()\n        return (math.sin(v), math.cos(v) * d)\n\nclass Cos:\n    def __init__(self, v):\n        self.v = v\n    def evaluate(self):\n        (v, d) = self.v.evaluate()\n        return (math.cos(v), -math.sin(v) * d)\n\n# class Exp, Log, etc.\nThis is called forward-mode autodiff because the derivatives “flow forward” with the computation of values. You can see the algorithm in action here, with uniformly random values between 0 and 1 assigned to the variables:\n\nExpression: Compute d/da\n\n\n\n\nThis works well for simple expressions. However, consider the case of computing the gradient of a function: the vector of partial derivatives with respect to all variables. If we want to use forward-mode autodiff to compute the gradient, we end up having to evaluate the same expression over and over again, just changing the values of the d fields of the variables a, b, and c. That’s quite inefficient, so let’s try to identify where the inefficiency comes from.\nPay attention to the partial derivatives we are computing with forward-mode autodiff. For every invocation, we choose one variable to compute the derivative over. We are keeping the “denominator” of the derivative fixed, and varying the “numerator” over all possible computation nodes. But a gradient does the opposite. It keeps the “numerator” fixed, and varies the “denominator” over all variables of the function: \\(\\nabla f(x, y, z, w) = [ \\partial f / \\partial x, \\partial f / \\partial y, \\partial f / \\partial z, \\partial f / \\partial w ]\\). So if we use forward-mode autodiff to compute gradients, all the computations of the derivatives of the intermediate nodes are wasted.\nThere is a better way to do this! It’s called reverse-mode autodiff."
  },
  {
    "href": "explainers/automatic-differentiation/index.html#pushing-derivatives-up",
    "title": "Automatic Differentiation",
    "section": "“Pushing derivatives up”",
    "text": "There’s a trick that highly simplifies the implementation of reverse-mode autodiff. The main issue with the above explanation is that the base case needs to be handled differently for every kind of expression, and there can be very many different expression kinds to handle: sums, products, sines, cosines, etc.\nIt’s much easier to write the code if every base case behaves the same way. The trick to make everything uniform is simply to posit that the “reverse-mode pass” always starts at a special variable (let’s call it \\(g\\)), and that \\(g\\) is defined to be equal to the expression you care about (in our case \\(g = f\\)).\nWe’re almost ready to write the algorithm. But we need two reminders. First, in forward-mode autodiff, the derivative values of a node \\(v\\) stored \\(\\partial v/\\partial x\\), where \\(x\\) was the chosen variable of interest. In reverse-mode autodiff, the derivative values of a node \\(v\\) will store \\(\\partial f/\\partial v\\), where \\(f\\) is the overall expression. Second, the version of the chain rule which we need for reverse-mode autodiff is a little more general than what we’ve seen before. Specifically, we need to handle a situation when \\(f\\) depends on \\(x\\) through multiple “intermediate paths”. In that case, the chain rule sums over all of terms of each independent path: \\[ \\frac{\\partial f}{\\partial x} = \\sum_v \\frac{\\partial f}{\\partial v} \\frac{\\partial v}{\\partial x} \\]\nWith that in mind, here’s the algorithm for reverse-mode autodiff:\n\nEvaluate the expression tree for the values as you would do in forward-mode autodiff, but without computing derivatives. This is the “forward pass”.\nInitialize the derivative values of all nodes to 0, except \\(g\\), which is initialized to 1.\nTraverse the computation graph in some topological order, from the node of the final expression (the “root” if it were a tree) up. The invariant we seek here is that by traversing the graph in this way, for every node we visit, the derivative of \\(g\\) with respect to that node will have been fully computed by the time we visit it.\nWhen we visit a node, we “push derivatives up” the tree, adding the appropriate derivative values to the derivative of the node’s parents.\nWhen this “backward pass” is finished, each node’s derivative values will hold the value of the derivative of \\(g\\) with respect to the node itself.\n\nLet’s make things more concrete. Consider, for example, the case where we visit the node \\(d = c \\times b\\). By our invariant, at that point we will have fully computed \\(\\partial f / \\partial d\\). By taking the two possible derivatives, we see that \\(\\partial d / \\partial b = c\\), and \\(\\partial d / \\partial c = b\\). The chain rule says \\(\\partial f / \\partial b = \\sum_v (\\partial f / \\partial v) (\\partial v / \\partial b)\\). Setting \\(v = b\\) gives us one term of the sum, so we need to increment the currently-stored value of \\(\\partial f / \\partial b\\) by \\((\\partial f / \\partial d) \\times c\\). We increment (as opposed to storing) because there could be other nodes in the graph that also use \\(b\\), and those will eventually have to add their own contribution to the chain rule sum.\nAs a result, reverse-mode autodiff “pushes derivatives” up the tree based on specific rules for each kind of expression, and you can derive each one of them by taking derivatives with respect to the possible parameters. Here are a few examples of the behavior of the algorithm when visiting nodes of different types, always assuming that \\(g\\) is the variable representing the entire expression:\n\n\\(a = b + c\\) adds \\(\\partial g / \\partial a\\) to both \\(\\partial g / \\partial b\\) and \\(\\partial g / \\partial c\\),\n\\(a = b - c\\) adds \\(\\partial g / \\partial a\\) to \\(\\partial g / \\partial b\\), and subtracts \\(\\partial g / \\partial a\\) from \\(\\partial g / \\partial c\\),\n\\(a = \\sin b\\) adds \\((\\partial g / \\partial a) \\times \\cos b\\) to \\(\\partial g / \\partial b\\),\n\\(a = \\log b\\) adds \\((\\partial g / \\partial a) \\times (1 / b)\\) to \\(\\partial g / \\partial b\\), etc.\n\nYou can see this new algorithm in action below, again with uniformly random values between 0 and 1 assigned to the variables. First, the algorithm performs a forward pass to compute the expression values, and then the algorithm performs a backward pass, to propagate the derivatives up the tree back to the variables. Note that the derivatives with respect to the variable nodes here are represented in text field separate from the variable nodes. We do this so that different branches can refer to the same variable, and the derivative with respect to that variable can accumulate correctly.\n\nExpression: Compute gradient"
  },
  {
    "href": "explainers/automatic-differentiation/index.html#backpropagation",
    "title": "Automatic Differentiation",
    "section": "Backpropagation?",
    "text": "In neural networks, “reverse-mode autodiff” is often referred to as “error back-propagation”, from the paper that made it popular for neural networks. The general algorithm for reverse-mode automatic differentiation was known before the specific use-case was published for neural networks, and was developed in a 1970 master’s thesis (!)."
  },
  {
    "href": "explainers/fagans-nomogram/index.html#references",
    "title": "Fagan’s Nomogram",
    "section": "References",
    "text": "Fagan TJ. Letter: Nomogram for Bayes theorem. NEJM, July 1975.\nWikipedia article on Nomograms.\nCasscells W, Schoenberger A, Graboys TB. Interpretation by physicians of clinical laboratory results. NEJM 299(18):999-1001, Nov. 1978.\n\n\n\nSource Code\n---\ntitle: Fagan's Nomogram\n---\n\nThe Fagan nomogram [1] is a nomogram [2] that computes the probability\nof the presence of some condition based on an imperfect test and\nvarying pre-test probabilities. It is a very handy tool to understand\nBayes's Theorem \"physically\". People typically have a sense that the\nless powerful the test, the less likely it is that a\npositive test result means the presence of the condition. But\npeople are much less likely to grasp the role of the pre-test\nprobability [3]. \n\nAs an illustration, consider the example illustrated in the default\nsetting of the nomogram below. If only 10% of the population exhibit a\nparticular kind of condition, then even if a test gives a ratio of\ntrue positives to false positives at 10 to 1, only 50% of the people\ntested positive will actually exhibit the condition.\n\nSimilarly, if only 10% of the population exhibits the condition, then\nin order to be 90% sure that a positive result indicates the presence\nof the condition, the test can give a false-positive result only\n1 every 100 times it gives true positive results. Intuitively, what's\ngoing on is that the base-10 logarithm of the \"likelihood ratio\" (10\nand 100 respectively in the examples above) is the \"number of nines\nadded to the baseline probability\", interpreting a probability of 0.1\nas having \"negative 1 nine\".\n\nThe original nomogram was meant to be used with a physical ruler to do\nthe calculations. Here, you can grab the circles and move them around\nto change the settings.\n\n<div id=\"main\"></div>\n\n## References\n\n1. Fagan TJ. \n   [Letter: Nomogram for Bayes theorem](https://www.ncbi.nlm.nih.gov/pubmed/1143310). \n   NEJM, July 1975.\n\n2. Wikipedia article on\n   [Nomograms](https://en.wikipedia.org/wiki/Nomogram).\n\n3. Casscells W, Schoenberger A, Graboys TB.\n   [Interpretation by physicians of clinical laboratory results](https://www.ncbi.nlm.nih.gov/pubmed/692627). NEJM 299(18):999-1001, Nov. 1978.\n\n<script type=\"module\" src=\"./main.js\"></script>"
  },
  {
    "href": "explainers/randomized-response/index.html#estimating-the-true-response-rate",
    "title": "Randomized Response",
    "section": "Estimating the true response rate",
    "text": "The result of the randomized response is a typical “biased” response, in the sense that the response we get is skewed in a particular way, and our goal is to “undo” this skew. In this case, there is one very simple analysis. If we have \\(r\\) respondents, we know that the number of truthful answers will not be very far from \\(r/2\\), and so we estimate that \\(r/2\\) of the answers are truthful. We then need to estimate the number of “truthful yes”s. We know that the number of “random yes”s is going to be about \\(r/4\\), from getting heads then heads from the coin flips.\nLet’s say that our biased survey has \\(Y\\) yes’s and \\(N\\) no’s. We simply “invent” an “estimated true survey” with a new number of responses \\(Y' = Y - r/4\\) and \\(N' = N - r/4\\), with total respondents \\(r' = Y' + N'\\). Now we estimate that the fraction of people who truthfully would answer yes is \\(Y'/r'\\)."
  },
  {
    "href": "explainers/randomized-response/index.html#risks",
    "title": "Randomized Response",
    "section": "Risks",
    "text": "The neat thing about this protocol is that it does not require the respondent to trust the survey creator: the properties hold independent of the behavior of each party. Although it might intuitively seem that it doesn’t matter at all whether or not each individual’s responses are leaked, there is an important catch. This protection against leakage only works if the respondents don’t answer multiple independent surveys asking the same question. Imagine an extreme case where each respondent answers 1000 of these surveys and respects the protocol each time.\nClearly, if only one of these datasets leak, then each of the respondents are not at high risk. But each additional leakage of a new dataset of the same questions does erode the privacy of the answer because for every individual respondent, we start to be able to accumulate a large number of independent runs of that protocol.\nCrucially, the same technique that we used to estimate the response rate for a group also works to estimate the true response of an individual. With a single run, there’s too much noise to make any real statement. But with many runs, eventually the true answer becomes clear and the respondent’s privacy is effectively ruined."
  },
  {
    "href": "explainers/randomized-response/index.html#references",
    "title": "Randomized Response",
    "section": "References",
    "text": "Warner, S.L. Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias. Journal of the American Statistical Association, 60, 63–69. Taylor & Francis, 1965.\n\n\nSource Code\n---\ntitle: Randomized Response\n---\n\n# Randomized Response\n\nImagine we want to estimate how many people in a population use a drug by asking them.\nPeople have an incentive to not answer truthfully, they're concerned they might be implicated if there is a leak of that private information.\nRandomized response is a way to set up such a survey with sensitive questions to protects the respondents from privacy leaks.\nAlthough it was invented by Stanley Warner way back in the 60s, it is a good starting point in understanding \"privacy-preserving data analysis\", which is a modern research area.\n\nThe idea is to give respondents a plausible, \"innocent\" reason for answering \"yes\" to the implicating question, so that if a leak does happen, there is plausible deniability from the perspective of the respondent. \nA basic protocol for randomized response is to ask respondents to perform the following steps:\n\n* Before answering the question, flip a coin.\n  * If it comes out heads, answer the question truthfully.\n  * If it comes out tails, then flip another coin.\n    * If the second flip comes out heads, respond \"yes\".\n    * If the second flip comes out tails, respond \"no\".\n\nOn average, only half the respondents will be asked to respond truthfully.\nThe other half will simply write the results of the coin flip.\nThere are two important properties of this protocol.\n\n1. Even if the analyst discloses the entire database, each respondent is \"safe\", because there's a 50-50 chance that their first coin flip came out heads, and so they responded at random. In other words, disclosing a \"yes\" answer is unlikely to incriminate anyone, because everyone answers \"yes\" with probability at least 1/2.\n2. We know that, on average, half the respondents will answer with the coin flip, while the other half will answer truthfully. This means we can recover a very good approximation of the results of the \"full survey\"\n\n## Estimating the true response rate\n\nThe result of the randomized response is a typical \"biased\" response, in the sense that the response we get is skewed in a particular way, and our goal is to \"undo\" this skew. \nIn this case, there is one very simple analysis. \nIf we have $r$ respondents, we know that the number of truthful answers will not be very far from $r/2$, and so we estimate that $r/2$ of the answers are truthful. \nWe then need to estimate the number of \"truthful yes\"s. We know that the number of \"random yes\"s is going to be about $r/4$, from getting heads then heads from the coin flips. \n\nLet's say that our biased survey has $Y$ yes's and $N$ no's. \nWe simply \"invent\" an \"estimated true survey\" with a new number of responses $Y' = Y - r/4$ and $N' = N - r/4$, with total respondents $r' = Y' + N'$. \nNow we estimate that the fraction of people who truthfully would answer yes is $Y'/r'$.\n\n## Risks\n\nThe neat thing about this protocol is that it does not require the respondent to trust the survey creator: the properties hold independent of the behavior of each party. \nAlthough it might intuitively seem that it doesn't matter *at all* whether or not each individual's responses are leaked, there is an important catch.\nThis protection against leakage only works if the respondents don't answer multiple independent surveys asking the same question.\nImagine an extreme case where each respondent answers 1000 of these surveys and respects the protocol each time.\n\nClearly, if only one of these datasets leak, then each of the respondents are not at high risk. \nBut each additional leakage of a new dataset of the same questions *does* erode the privacy of the answer because for every individual respondent, we start to be able to accumulate a large number of independent runs of that protocol. \n\nCrucially, the same technique that we used to estimate the response rate for a _group_ also works to estimate the true response of an individual. \nWith a single run, there's too much noise to make any real statement. \nBut with many runs, eventually the true answer becomes clear and the respondent's privacy is effectively ruined.\n\n# Takeaways\n\nRandomized response shows that it's possible to create protocols that enable \"privacy-preserving data analysis\". \nThey're not a panacea, though, as the risks show.\n\nThe risks also teach us that privacy loss is not always a \"discrete\" phenomenon: privacy losses can compound gradually.\nIf data leaks happen, repeated randomized responses over related data will necessarily erode the privacy.\nIn addition, there is a tradeoff going on between the accuracy we can obtain for some questions, and the privacy loss incurred.\n\nThe best, modern answer we have in data analysis for this problem is the notion of *differential privacy*. \nDifferential privacy *also* involves adding noise to answers, but in a fancier way.\n\n## References\n\n* Warner, S.L. Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias. Journal of the American Statistical Association, 60, 63--69. Taylor & Francis, 1965."
  },
  {
    "href": "explainers/bilinear-interpolation/index.html",
    "title": "Bilinear interpolation",
    "section": "",
    "text": "Source Code\n---\ntitle: Bilinear interpolation\necho: false\n---\n\n<script src=\"../../js/lux.js\"></script>\n<script src=\"../../js/jquery-2.1.1.min.js\"></script>\n<div id=\"bilinear\"></div>\n\n```{ojs}\n//| output: false\nimport { cscheid } from \"/js/cscheid/cscheid.js\";\nimport { main } from \"./main.js\";\n{\n  debugger;\n  cscheid.setup.setupGlobals({ d3 });\n  main();\n}\n```"
  },
  {
    "href": "explainers/regularization/index.html#ridge-regression",
    "title": "Regularization",
    "section": "Ridge Regression",
    "text": "The ridge regression model is quite simple. Recall the typical linear least squares setup: \\[ X \\beta = y \\]\nHere, we are looking to fit the best parameters \\(\\beta\\) to rows of the design matrix. Each input point \\((v\\_i, y\\_i)\\) is mapped to some feature space encoded in the rows of \\(X\\) (\\(f(v\\_i) = x\\_{i\\star}\\)). The \\(\\beta\\) parameters which minimize the expected squared error are: \\[ \\hat{\\beta} = (X^T X)^{-1} X^T y \\]\nWithout regularization, if our design includes too many parameters (for example, if we try to fit a polynomial of too-high a degree), our model will overfit. This can be seen in the demo below by increasing the degree, the noise, and setting the regularization to a very low value. In ridge regression, we create an extra parameter \\(\\lambda\\), and we want the extra parameter to control the complexity of the model. In short, the larger \\(\\lambda\\) is, the simpler we want our model to be. The error function for ridge regression is: \\[ E = || X \\beta - y ||^2 + \\lambda ||\\beta||^2 \\]\nThe solution for ridge regression is similar to that of linear least squares: \\[ \\hat{\\beta} = (X^T X + \\lambda I)^{-1} X^T y \\]\nNotice that \\(E\\) tries to balance two things: how bad the results are (the left term), and how large the vector of parameter values are. In other words, if we increase \\(\\lambda\\), this new error term will tend to interpret large magnitudes in \\(\\beta\\) as a bad sign. At first, it’s puzzling that we would want the vector of parameter values to have a small magnitude.\n\nData imputation\nRidge regression is equivalent to doing typical least squares while adding “ghost” entries to the data set. If \\(X\\) has \\(n\\) columns, then you should be able to see that adding \\(n\\) new data points to the dataset, where \\(v\\_{m+1} = (\\sqrt{\\lambda}, 0, \\ldots, 0)\\), \\(v\\_{m+2} = (0, \\sqrt{\\lambda}, 0, \\ldots, 0)\\), etc. and \\(y\\_{m+1} = y\\_{m+2} = \\cdots = 0\\).\nIn this interpretation, we see that regularization is trying to push all parameters of \\(\\beta\\) uniformly to zero (since that’s the only way that \\(\\beta\\) will satisfy these specific values) by adding entries to the dataset that do not really exist.\nIn other words, regularization is equivalent to showing the training procedure a slight fiction (pessimistic towards zero), in order to not let the model get overexcited.\n\n\nNormalization of data\nWhen using ridge regression, it becomes important to make sure that your data is normalized: in other words, the values in each column should have mean zero, and variance 1.\nThis normalization can be seen to be necessary by considering the data imputation view.\nIf we do not set the mean of each column to zero, then regularization biases the model away from the data. That’s a very bad thing: if nothing else, our simplest models should be shooting for the average data point. Without normalization, they do not (you can confirm this by unchecking the normalization box in the demo below).\nWithout setting the variance of all the features to be the same, ridge regression will penalize some features more than others. This is easier to see again in the imputation view of ridge regression: each of the ghost entries pushes the solution equally to zero.\nIf the variance of the data is not one, then things mostly work, but regularization values become hard to compare across datasets, because the amount of regularization becomes relative to the variance on the specific datasets.\n\n\nEffective degrees of freedom\nThe degrees of freedom of a model can be recovered from the trace of the Hat matrix. So we can look at the trace of the Hat matrix of the Ridge regression to recover the effective degrees of freedom. This notion of model complexity is more directly comparable across different models (the full story is more complicated, but this is a very useful fiction). The formula for the effective degrees of freedom in a model is: \\[ \\textrm{eff-df} = \\sum_i \\left . \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda} \\right . \\]\nPlay around with the demo below, and notice how models with different dimensions (measured by the degree of the polynomials being fit) but with the same effective degrees of freedom, tend to look the same."
  },
  {
    "href": "explainers/regularization/index.html#ridge-regression-demo",
    "title": "Regularization",
    "section": "Ridge Regression Demo",
    "text": "Degree: .\n\n\n\nRegularization: .\n\n\n\nNoise level: .\n\n\n\n\n\n\nEffective degrees of freedom: \nNormalize columns: \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource Code\n---\ntitle: Regularization\necho: false\n---\n\nIn the context of statistics, data mining, and machine learning --\nspecifically when designing optimization-based methods for data\nfitting --- regularization refers to the idea of choosing a model that\npurposefully does not fit the training data the best it could. The\nintuition is that, while we want complex models that can capture\ninteresting features from the data, we want to prevent the model from\nfitting the noise in the training data, rather than the\nfeatures. \n\nIn summary, regularization is a way to control the complexity of the\nmodel. In other words, we are talking about model selection. The first\nquestion one could ask is: \"but why do we not control the model by\nexplicitly choosing different models?\" That is certainly one way to do\nmodel selection, but it is a surprisingly tricky one in practice. In\nsimple cases (like linear regression), it is easy to compare two\nmodels to see which is more complex. But it's not as simple to choose\nthe *appropriate* model complexity.\n\nIn contrast, typical methods for regularization allow us to more\neasily connect the relationship of the amount of regularization to the\namount of noise in the data.\n\nThe simplest example of regularization is known as \"ridge regression\",\nand it builds on linear regression.\n\n## Ridge Regression\n\nThe ridge regression model is quite simple. Recall the typical linear\nleast squares setup:\n\n$$ X \\beta = y $$\n\nHere, we are looking to fit the best parameters $\\beta$ to rows of the\ndesign matrix. Each input point $(v\\_i, y\\_i)$ is mapped to some\nfeature space encoded in the rows of $X$ ($f(v\\_i) = x\\_{i\\star}$). The\n$\\beta$ parameters which minimize the expected squared error are:\n\n$$ \\hat{\\beta} = (X^T X)^{-1} X^T y $$\n\nWithout regularization, if our design includes too many parameters\n(for example, if we try to fit a polynomial of too-high a degree), our\nmodel will *overfit*. This can be seen in the demo below by\nincreasing the degree, the noise, and setting the regularization to a\nvery low value. In ridge regression, we create an extra parameter\n$\\lambda$, and we want the extra parameter to control the complexity\nof the model. In short, the larger $\\lambda$ is, the simpler we want\nour model to be. The error function for ridge regression is:\n\n$$ E = || X \\beta - y ||^2 + \\lambda ||\\beta||^2 $$\n\nThe solution for ridge regression is similar to that of linear least\nsquares:\n\n$$ \\hat{\\beta} = (X^T X + \\lambda I)^{-1} X^T y $$\n\nNotice that $E$ tries to balance two things: how bad the results are\n(the left term), and how large the vector of parameter values are. In\nother words, if we increase $\\lambda$, this new error term will tend\nto interpret large magnitudes in $\\beta$ as a bad sign. At first, it's\npuzzling that we would want the vector of parameter values to have a\nsmall magnitude.\n\n### Data imputation\n\nRidge regression is equivalent to doing typical least squares while\nadding \"ghost\" entries to the data set. If $X$ has $n$ columns, then\nyou should be able to see that adding $n$ new data points to the\ndataset, where $v\\_{m+1} = (\\sqrt{\\lambda}, 0, \\ldots, 0)$, $v\\_{m+2} =\n(0, \\sqrt{\\lambda}, 0, \\ldots, 0)$, etc. and $y\\_{m+1} = y\\_{m+2} =\n\\cdots = 0$.\n\nIn this interpretation, we see that regularization is trying to push\nall parameters of $\\beta$ uniformly to zero (since that's the only way\nthat $\\beta$ will satisfy these specific values) by adding entries to\nthe dataset that do not really exist. \n\nIn other words, regularization is equivalent to showing the training\nprocedure a slight fiction (pessimistic towards zero), in order to not\nlet the model get overexcited.\n\n### Normalization of data\n\nWhen using ridge regression, it becomes important to make sure that\nyour data is *normalized*: in other words, the values in each column\nshould have mean zero, and variance 1. \n\nThis normalization can be seen to be necessary by considering the data\nimputation view.\n\nIf we do not set the mean of each column to zero, then regularization\nbiases the model away from the data. That's a very bad thing: if\nnothing else, our simplest models should be shooting for the average\ndata point. Without normalization, they do not (you can confirm this\nby unchecking the normalization box in the demo below). \n\nWithout setting the variance of all the features to be the same, ridge\nregression will penalize some features more than others. This is\neasier to see again in the imputation view of ridge regression: each\nof the ghost entries pushes the solution equally to zero.\n\nIf the variance of the data is not one, then things *mostly* work,\nbut regularization values become hard to compare across datasets,\nbecause the amount of regularization becomes relative to the variance\non the specific datasets.\n\n### Effective degrees of freedom\n\nThe degrees of freedom of a model can be recovered from the trace of\nthe Hat matrix. So we can look at the trace of the Hat matrix of the\nRidge regression to recover the *effective* degrees of freedom. This\nnotion of model complexity is more directly comparable across\ndifferent models (the full story is more complicated, but this is a\nvery useful fiction). The formula for the effective degrees of freedom\nin a model is:\n\n$$ \\textrm{eff-df} = \\sum_i \\left . \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda} \\right . $$\n\nPlay around with the demo below, and notice how models with different\ndimensions (measured by the degree of the polynomials being fit) but\nwith the same effective degrees of freedom, tend to *look the same*.\n\n## Ridge Regression Demo\n\n<div id=\"div-ridge\"></div>\n\nDegree: <span id=\"span-degree\"></span>.\n\n<div style=\"width: 300px; margin-bottom: 1em\" id=\"slider-degree\"></div>\n\nRegularization: <span id=\"span-regularization\"></span>.\n\n<div style=\"width: 300px; margin-bottom: 1em\" id=\"slider-regularization\"></div>\n\nNoise level: <span id=\"span-noise\"></span>.\n\n<div style=\"width: 300px; margin-bottom: 1em\" id=\"slider-noise\"></div>\n\n<div id=\"button-reseed\" style=\"margin-top:1em\"></div>\n\nEffective degrees of freedom: <span id=\"span-effdf\"></span>\n\nNormalize columns: <span id=\"span-normalize\"></span>\n\n\n```{ojs}\n//| output: false\nimport { cscheid } from \"/js/cscheid/cscheid.js\";\nimport { main } from \"./main.js\";\n{\n  cscheid.setup.setupGlobals({ d3 });\n  main();\n}\n\n```"
  },
  {
    "href": "explainers/odes/index.html#euler-integration",
    "title": "Integrating Ordinary Differential Equations Numerically",
    "section": "Euler integration",
    "text": "Euler integration is by far the simplest method for solving ODEs numerically. The idea behind Euler integration is very simple: if at a given point we know the value of a vector field, and the vector field is continuous, then we expect it to not change very much in the neighborhood. More drastically, we assume it’s locally constant, so we can a step in the direction of the vector field. That lands us in a new spot in the plane, from which we sample the vector field again, and so on. The only free parameter is the size of the step we take, which we call \\(h\\) here:\ndef euler_integration(vector_field, initial_x, initial_y, h):\n    t = 0\n    px = initial_x\n    py = initial_y\n    while True:\n        report_point(px, py, t)\n        (vx, vy) = vector_field(px, py)\n        px += vx * h\n        py += vy * h\n        t += h\n(In practice, of course, we’d stop the process with some criterion) At first sight, this seems a reasonable way to compute approximations: as \\(h\\) gets smaller, we take smaller steps, and if the vector field is continuous, then we should expect Euler integration to get progressively better. Here’s an example. Drag the black circle around to see different curves approximated with Euler integration:\n\n\n\nThings seem ok, right? But look at what happens when we try this much simpler vector field:\n\n\n\nThis is the vector field \\(v(x,y) = (-y, x)\\). The integral lines of this vector field are circles (this is easy to see from \\(\\cos' = -\\sin, \\sin' = \\cos\\)), but the approximation lines we get from Euler’s integration are spirals. Intuitively, this is relatively easy to understand: a finite step along the vector field always increases the distance from the origin. More importantly, the fraction with which the distance from the origin increases is the same at every step (by a simple argument of congruent triangles). This means that, as a function of \\(t\\), our approximation \\(\\tilde{x}(t), \\tilde{y}(t)\\) is such that the distance from the origin increases exponentially as \\(t\\) increases.\nThis happens for real-world, more complicated vector fields. Consider the Lotka-Volterra equations, a pair of non-linear differential equations which model predator-prey relationships, \\(u' = u(2-v), v' = v (u - 1)\\).\nThe integral lines of this model should be cycles, and yet,\n\n\n\nThis is a serious problem. But how do we solve it?"
  },
  {
    "href": "explainers/odes/index.html#step-size-control",
    "title": "Integrating Ordinary Differential Equations Numerically",
    "section": "Step-size control",
    "text": "One way to make the solution more accurate is to reduce the size of each step. However, reducing the step size by half doubles the amount of points which are used to cover a given interval in \\(t\\). Note, also, that in the Lotka-Volterra example above that the lengths of each of the steps can change quite a bit. So if reduce the parameter \\(h\\) so that the largest distance between each pair of sampled values is small, then the smallest distance will be very small, which means that covering a lot of \\(t\\) will take a huge number of steps.\nInstead, we could try to estimate the error we’re accruing at every step. One estimate of this error is one half of the length of the difference between the two consecutive vectors we sampled along the vector field, multiplied by \\(h\\) (this comes from a Taylor series expansion of the approximations). So, if we set our parameter to be, instead of \\(h\\), a maximum tolerance for error \\(T\\), then we can make the steps bigger every time our error estimate says our integration is conservative, and make the steps smaller every time our error estimate says our integration is too aggressive:\n\n\n\nNotice how now the points are more evenly spaced around the curve, which means that as we make the step-size smaller, we use our samples more efficiently. Still, it remains the case that no matter how small we want our error is, it keeps getting compounded fairly fast. We need a different approach."
  },
  {
    "href": "explainers/odes/index.html#predictor-corrector-methods",
    "title": "Integrating Ordinary Differential Equations Numerically",
    "section": "Predictor-corrector methods",
    "text": "The first algorithm that is good enough to be used in practice is so simple that it’s a bit magical. The idea is as follows. At each time step, we evaluate the vector field at the current point, which gives a future position using the Euler integration rules. Now, instead of simply jumping over to that position, we evaluate the vector field there, and take the average of the two vectors as the vector to use.\ndef euler_integration_pc(vector_field, initial_x, initial_y, h):\n    t = 0\n    px = initial_x\n    py = initial_y\n    while True:\n        report_point(px, py, t)\n        (vx,  vy)  = vector_field(px, py)\n        (vx2, vy2) = vector_field(px+h*vx, py+h*vy)\n        px += ((vx + vx2) / 2) * h\n        py += ((vy + vy2) / 2) * h\n        t += h\nThis seems like a minor change, but notice this:\n\n\n\nEven with a very step size, the accumulation error has effectively disappeared! What gives? To get an intuition for the situation, let’s consider a different algorithm. Imagine that, instead of taking the average of (vx, vy) and (vx2, vy2), we just took (vx2, vy2) as the value:\ndef weird_euler_integration(vector_field, initial_x, initial_y, h):\n    t = 0\n    px = initial_x\n    py = initial_y\n    while True:\n        report_point(px, py, t)\n        (vx,  vy)  = vector_field(px, py)\n        (vx2, vy2) = vector_field(px+h*vx, py+h*vy)\n        px += vx2 * h\n        py += vy2 * h\n        t += h\nWhat happens then?\n\n\n\nNow, instead of spiraling away from the center of the cyclic behavior, the approximation spirals into it. This is intuitively clear in the case of the circular vector field: using the vector from the “future” position of Euler integration makes the curve “turn too early”. Clearly, using the vector field from the present position makes the curve “turn too late” (since that’s the problem with Euler integration to begin with). So it must be the case that some average of the two approaches cancels the error out. What’s not so clear is that the correct weight is 1) independent of the actual vector field, and 2) equal to \\(1/2\\). While we won’t go into the derivation here, the easiest way to arrive at it is to first check that the averaging rule works assuming that the vector field is a linear function of its coordinates, and then generalize by taking appropriate Taylor series: you’ll get that the error for the method is essentially proportional to \\(h^2\\), and to the dominating term of the quadratic parameter of the Taylor series fit."
  },
  {
    "href": "explainers/odes/index.html#runge-kutta-methods",
    "title": "Integrating Ordinary Differential Equations Numerically",
    "section": "Runge-Kutta methods",
    "text": "What if we want a method that converges well if the vector field has significant quadratic terms? We use more sophisticated versions of the above idea, probing neighboring values of the vector field and averaging them appropriately. The most common such method is known as RK4, standing for Runge-Kutta’s 4th order method.\nTBF.\n\nButcher tableaus\nTBF."
  },
  {
    "href": "explainers/odes/index.html#implicit-methods",
    "title": "Integrating Ordinary Differential Equations Numerically",
    "section": "Implicit methods",
    "text": "TBF."
  },
  {
    "href": "explainers/variance.html#properties",
    "title": "Variance",
    "section": "Properties",
    "text": "Variance is invariant to translation: $ = $\nVariance scales quadratically: $ = k^2 $"
  },
  {
    "href": "explainers/variance.html#covariance",
    "title": "Variance",
    "section": "Covariance",
    "text": "Sometimes we’re interested not in measuring the spread of a single variable, but in the way two different variables relate to each other. If we slightly rewrite variance to be $ = E[(X - E[X])(X - E[X])] $, then we can see that we’re measuring the expectation of the product of the difference of the expectation to itself, with itself. But it can be useful to measure this product of the difference from the expectation between two different expressions, and this is the covariance:\n\\[ \\Cov[X, Y] = E[(X - E[X])(Y - E[Y])] \\]\n\nSource Code\n---\ntitle: Variance\nlayout: bootstrap_wide\n---\n\n# Variance\n\n*Variance* is the simplest mathematical tool we have to measure the\n\"spread\" of a variable of interest in a dataset. The definition tries\nto capture how much we a value tends to change as we pick different\nelements in the dataset. It's easiest to define in terms of an\n[expectation](expectation.html). The variance is defined to be the\n*expected squared difference from the expected value*:\n\n$$ \\Var[X] = E[(X - E[X])^2] $$\n\n## Properties\n\nVariance is invariant to translation: $ \\Var[X + k] =\n\\Var[X] $\n\nVariance scales quadratically: $ \\Var[kX] = k^2 \\Var[X] $\n\n## Covariance\n\nSometimes we're interested not in measuring the spread of a single\nvariable, but in the way two different variables relate to each\nother. If we slightly rewrite variance to be $ \\Var[X] =\nE[(X - E[X])(X - E[X])] $, then we can see that we're measuring the\nexpectation of the product of the difference of the expectation to\nitself, *with itself*. But it can be useful to measure this product of\nthe difference from the expectation between two different expressions,\nand this is the *covariance*:\n\n$$ \\Cov[X, Y] = E[(X - E[X])(Y - E[Y])] $$"
  },
  {
    "href": "explainers/pade-approximant/index.html",
    "title": "Padé Approximants",
    "section": "",
    "text": "tl;dr: instead of approximating something with a polynomial (something like Taylor series), we approximate it with a rational function. Presumably this works better when your function has a pole, because you can match the pole of the function to the pole of the rational approximation.\nSupposedly these approximations also work better than Taylor series for the same order of polynomial/rational function. That shouldn’t be surprising, since polynomials are a subset of rational functions.\nObviously I don’t understand them at all so I’m just going to plot some Padé approximants instead 🤷 :\n\n\n\n\n\nSource Code\n---\ntitle: \"Padé Approximants\"\nfrom: markdown+emoji\n---\n\ntl;dr: instead of approximating something with a polynomial (something like\nTaylor series), we approximate it with a rational\nfunction. Presumably this works better when your function has a pole,\nbecause you can match the pole of the function to the pole of the\nrational approximation. \n\nSupposedly these approximations also work better than Taylor series\nfor the same order of polynomial/rational function. That shouldn't be\nsurprising, since polynomials are a subset of rational functions.\n\nObviously I don't understand them at all so I'm just going to plot some\nPadé approximants instead :shrug: :\n\n<div id=\"pade-approximant\"></div>\n\n<script type=\"module\" src=\"./index.js\"></script>"
  },
  {
    "href": "explainers/error-correcting-output-codes.qmd/index.html#prelude-binary-tree-reductions",
    "title": "Error-Correcting Output Codes",
    "section": "Prelude: Binary-tree reductions",
    "text": "Let’s first introduce a slightly different, but very simple reduction. Let’s work, for now, with the case of an 8-class problem, so the actual class to be predicted is one of 0, 1, 2, 3, 4, 5, 6, or 7. What we will do is create a binary tree of decisions, where we will train a binary classifier for each of the nodes. The decisions of each classifier will determine the path that we will walk down the tree, and downstream branches will progressively narrow the set of possible choices. The leaves of this tree represent single classes, and the decision of the overall classifier.\nGraphically, it looks like this:\n\nIf you compare this scheme to the OVA and AVA ideas, you can notice a few similarities and a few differences.\nFirst, the number of classifiers you need to train in this scheme is comparable to that of OVA (in a binary tree with \\(k\\) leaves, there are always \\(k-1\\) nodes).\nIn OVA, all classifiers are unbalanced, in that you’re training “class-\\(i\\) vs. class-not-\\(i\\)” subproblems. Here, your classification problems are all balanced. The leaves of the binary tree classifier are identical to some of the AVA classifiers, but the internal nodes combine multiple classes. This can be both good and bad: it’s bad in that if you have multiple classes that look very different from one another, you’ll be trying to “teach your classifier more than one thing at once”. That’s not great. But if the classes in the same side of the internal nodes are similar enough, then combining them means that the training process for that node will be able to “share knowledge from its lessons”. And that’s a good thing.\nUnlike OVA or AVA, the number of decisions you need to make at test time is quite small. Here, the path from the tree root to a leaf has \\(O(\\log k)\\) nodes, while OVA needs \\(O(k)\\) classifiers, and AVA, \\(O(k^2)\\). This means that if your binary classifiers achieve an error rate of \\(\\epsilon\\), then you can trivially see that the multiclass classifier will achieve an error rate of \\(O(\\log k) \\epsilon\\)."
  },
  {
    "href": "explainers/error-correcting-output-codes.qmd/index.html#reducing-the-number-of-base-classifiers",
    "title": "Error-Correcting Output Codes",
    "section": "Reducing the number of base classifiers",
    "text": "Now let’s make things a little weird. Say that you think your base binary classifier procedure is so good, that it will be able to distinguish between any partition of classes from the original problem that you give it. What’s the minimum number of classifiers you have to train? It is easy to see what that number must be. If you think of the decision of the final multiclass classifier as producing a word in binary, then clearly we need \\(O(\\log_2 k)\\) binary digits to write out that word. But a binary digit is the best we can get out of a binary classifier, so we need \\(O(\\log_2 k)\\) of them as well.\nThis also clearly suggests the way to build the classifier: write out the multiple class labels in binary representation, and now the job of the \\(i\\)-th of \\(\\log_2 k\\) binary classifiers is to predict the \\(i\\)-th digit of that binary representation. This is the same thing as building, for each of the level of the trees in the above example, a single classifier, where that classifier predicts whether to take the branch on the left or the right.\nIn other words, for the same 8-class problem we had before, we build classifiers that distinguish between:\n\n0123 vs. 4567\n0145 vs. 2367\n0246 vs. 1357\n\nNotice how there’s only one class that’s unique along any possible set of decisions of the binary classifiers. Let’s call this the “binary transmission” method (TODO: find the actual name used in ML)."
  },
  {
    "href": "explainers/error-correcting-output-codes.qmd/index.html#lets-not-repeat-digits",
    "title": "Error-Correcting Output Codes",
    "section": "Let’s not repeat digits",
    "text": "The code I’m going to describe to you works on 4-bit words, so for now let’s pretend that our original problem had 16 classes instead of 8. The encoding process is itself very easy to describe. We send the 4-bit word into a 7-bit space:\n\n$ A = d_1  d_2  d_4 $\n$ B = d_1  d_3  d_4 $\n$ C = d_2  d_3  d_4 $\n$ D = d_1 $\n$ E = d_2 $\n$ F = d_3 $\n$ G = d_4 $\n\nHere, \\(d\\_1\\) through \\(d\\_4\\) are the digits of the word we want to send. As we argued above, it must be the case that this code is redundant. Indeed, any valid word respects the following equations:\n\n$ A  D  E  G = 0 $\n$ B  D  F  G = 0 $\n$ C  E  F  G = 0 $\n\nThese equations are easy to prove: remember that \\(\\textrm{xor}\\) is commutatitive and associative, and that \\(a\\ \\textrm{xor}\\ a = 0\\). Moreover, the equations are also the key to making error detection work. Here’s a minor modification we will make. Let’s assume that an error can happen on any digit we send, so we replace \\(A\\) with \\(\\tilde{A}\\), etc:\n\n$  = _A  d_1  d_2  d_4 $\n$  = _B  d_1  d_3  d_4 $\n$  = _C  d_2  d_3  d_4 $\n$  = _D  d_1 $\n$  = _E  d_2 $\n$  = _F  d_3 $\n$  = _G  d_4 $\n\nHere, we use \\(\\epsilon\\_A\\), \\(\\epsilon\\_B\\), etc. to model errors in our transmission. The code we are describing here can only correct mistakes if only one of them happens at a time. That means that for every transmission, either \\(\\epsilon\\_A = \\epsilon\\_B = \\cdots = 0\\), or a single one of the \\(\\epsilon\\) variables is one and all the others are still zero.\nThis is where the magic happens. Let’s say \\(\\epsilon_A = 1\\). Although it seems like it will be hard to figure out that something went wrong (since the 7-bit word we transmitted “looks fine”), let’s look at those three equations that we derived which describe the redundancy in the code. Now, consider their equivalents with \\(\\tilde{A}\\), etc. I’m also giving each term a name now, \\(c\\_0\\) through \\(c\\_2\\):\n\n$ c_0 =        = _{A}  _{D}  _{E}  _{G} $\n$ c_1 =        = _{B}  _{D}  _{F}  _{G} $\n$ c_2 =        = _{C}  _{E}  _{F}  _{G} $\n\nNotice two very important things on the right hand sides: every error variable appears at least once, and no error variable was cancelled out from appearing more than once in the left-hand sides.\nThere’s also a less immediate property of these \\(c\\) variables: every possible mistake case creates a different configuration of \\(c\\_0\\), \\(c\\_1\\), and \\(c\\_2\\)! This is truly remarkable, and you should check it for yourself, by setting the values of \\(\\epsilon\\_A\\) through \\(\\epsilon\\_G\\) to 1. For example, you should be able to convince yourself that if \\(c\\_0 = 0\\), \\(c\\_1 = 1\\), and \\(c\\_2 = 1\\), then it must be the case that \\(\\epsilon\\_{F} = 1\\), which means that the mistake happened on \\(\\tilde{F}\\), which in turn means that all we have to do is flip \\(\\tilde{F}\\) to obtain \\(F\\), and all other digits must be correct! If all \\(c\\)s are zero, then no error happened. (Again, we are assuming that errors happen only one at a time. Fate is not always this kind.)"
  },
  {
    "href": "explainers/error-correcting-output-codes.qmd/index.html#now-to-turn-this-into-a-multiclass-reduction",
    "title": "Error-Correcting Output Codes",
    "section": "Now to turn this into a multiclass reduction",
    "text": "The final remarkable property of this error-correcting code is that each digit in the new transmission is formed by a particular combination of the digits in the original problem. Let’s think of the binary classification problems in the “binary transmission” reduction. (Here, I’ll slip into hexadecimal notation and call the 16 classes 0123456789abcdef):\n\n\\(d_1\\): 01234567        vs. the negative:         89abcdef\n\\(d_2\\): 0123    89ab    vs. the negative:     4567    cdef\n\\(d_3\\): 01  45  89  cd  vs. the negative:   23  67  ab  ef\n\\(d_4\\): 0 2 4 6 8 a c e vs. the negative:  1 3 5 7 9 b d f\n\n(The whitespace is there simply to make the logic more apparent.) These are our “base-digits” classifiers. The “checksum-digit” classifiers, that is, the redundant classifiers we will build to let us fix mistakes, will use the \\(\\textrm{xor}\\) operation as well. Concretely speaking, the classifier for \\(d\\_1\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_4\\) (that is, our transmitted digit \\(\\tilde{A}\\) above) should return a positive label when \\(d\\_1\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_4 = 1\\), or equivalently, it should separate the classes 02579bce from 13468adf.\nTake a moment to convince yourself of why that makes sense: that split separates the classes that appear in \\(d\\_1\\), \\(d\\_2\\), or \\(d\\_4\\) an odd number of times from those that appear there an even number of times. In other words, this classifier will “attempt to send 1” if \\(d\\_1\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_4 = 1\\), and “attempt to send 0” if \\(d\\_1\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_4 = 0\\), exactly like our communication example!\nNow, every time any one of our classifiers makes an isolated mistake, we will be able to spot it and fix it, at the expense of training only three more classifiers. And all of these classifiers are trained on different label distributions, and so we don’t run into the issue of repeated classification mistakes (like we would if we used the “repeat transmission” code).\nIsn’t that amazing?"
  },
  {
    "href": "explainers/error-correcting-output-codes.qmd/index.html#notes",
    "title": "Error-Correcting Output Codes",
    "section": "Notes",
    "text": "The reason we can’t use the “repeat the same message more than once” trick is that we assumed we were always training the same classifier. If you have two binary classifiers for the same problem that both predict reasonably well, and whose predictions are relatively uncorrelated, then you can absolutely use this “repeat message” method. This turns out to work incredibly well in practice!\n\n\nSource Code\n---\ntitle: Error-Correcting Output Codes\n---\n\n# Error Correcting Output Codes\n\nOften it will be the case that we can model a process we care about as\ntwo parties communicating with one another over a *noisy channel*. Sometimes\nthis is very straightforward: if you're sending the content of a website to\na cellphone, the radio in the cellphone will need to be able to figure out\nwhen the message was garbled, and will need to figure out how to fix it.\nSurprisingly, some techniques in data science also benefit from this\nchange in perspective.\n\nConsider the problem of creating a classifier that can distinguish\nbetween more than two classes. We could try to solve this problem from\nfirst principles, but if we want to reuse some of our hard-earned\nknowledge, we could try to turn the problem of multiple classification\ninto (many instances of) binary classification. The typical ways of\ndoing this are called [\"OVA\" and \"AVA\"](../ova_ava/index.html), but there's a\ndifferent method that sometimes works better, is elegant, simple to\nexplain, and connects directly to the idea of two parties communicating over\na noisy channel.\n\n## Prelude: Binary-tree reductions\n\nLet's first introduce a slightly different, but very simple\nreduction. Let's work, for now, with the case of an 8-class problem,\nso the actual class to be predicted is one of `0`, `1`, `2`, `3`, `4`,\n`5`, `6`, or `7`. What we will do is create a binary tree of\ndecisions, where we will train a binary classifier for each of the\nnodes. The decisions of each classifier will determine the path that\nwe will walk down the tree, and downstream branches will progressively\nnarrow the set of possible choices. The leaves of this tree represent\nsingle classes, and the decision of the overall classifier.\n\nGraphically, it looks like this:\n\n<img style=\"width:70%\" src=\"binary-tree.png\">\n\nIf you compare this scheme to the OVA and AVA ideas, you can notice a\nfew similarities and a few differences.\n\nFirst, the number of classifiers you need to train in this scheme is\ncomparable to that of OVA (in a binary tree with $k$ leaves, there are\nalways $k-1$ nodes).\n\nIn OVA, all classifiers are unbalanced, in that you're training\n\"class-$i$ vs. class-not-$i$\" subproblems. Here, your classification\nproblems are all balanced. The leaves of the binary tree classifier\nare identical to some of the AVA classifiers, but the internal nodes\ncombine multiple classes. This can be both good and bad: it's\nbad in that if you have multiple classes that look very different from\none another, you'll be trying to \"teach your classifier more than one\nthing at once\". That's not great. But if the classes in the same side\nof the internal nodes are similar enough, then combining them means\nthat the training process for that node will be able to \"share\nknowledge from its lessons\". And that's a good thing.\n\nUnlike OVA or AVA, the number of decisions you need to make at test\ntime is quite small. Here, the path from the tree root to a leaf has\n$O(\\log k)$ nodes, while OVA needs $O(k)$ classifiers, and AVA,\n$O(k^2)$. This means that if your binary classifiers achieve an error\nrate of $\\epsilon$, then you can trivially see that the multiclass\nclassifier will achieve an error rate of $O(\\log k) \\epsilon$.\n\n## Reducing the number of base classifiers\n\nNow let's make things a little weird. Say that you think your base\nbinary classifier procedure is so good, that it will be able to\ndistinguish between *any* partition of classes from the original\nproblem that you give it. What's the minimum number of classifiers you\nhave to train? It is easy to see what that number must be. If you think of\nthe decision of the final multiclass classifier as producing a word in\nbinary, then clearly we need $O(\\log_2 k)$ binary digits to write out that word. But a binary\ndigit is the best we can get out of a binary classifier, so we need\n$O(\\log_2 k)$ of them as well.\n\nThis also clearly suggests the way to build the classifier: write out\nthe multiple class labels in binary representation, and now the job of\nthe $i$-th of $\\log_2 k$ binary classifiers is to predict the $i$-th\ndigit of that binary representation. This is the same thing as\nbuilding, for each of the level of the trees in the above example, a\n*single classifier*, where that classifier predicts whether to take\nthe branch on the left or the right.\n\nIn other words, for the same 8-class problem we had before, we build\nclassifiers that distinguish between: \n\n* `0123` vs. `4567`\n* `0145` vs. `2367`\n* `0246` vs. `1357`\n\nNotice how there's only one class that's unique along any possible set\nof decisions of the binary classifiers. Let's call this the \"binary\ntransmission\" method (TODO: find the actual name used in ML).\n\n# The magic of error correction\n\nThis is pretty neat, but it turns out that the best is still ahead of\nus!  The main thing we will address now is that it's not always the\ncase that our classifiers will be correct. And unlike the case with\nAVA and OVA, base-level mistakes in the binary tree and binary\ntransmission methods are always catastrophic.\n\nThe problem of transmitting information in the presence of noise has\nbeen deeply studied since the early 20th century. The most magical\nthing about changing perspectives is that it instantly gives us a lot\nof new tools to think about a problem. So what if we pretend that each\nbinary classifier is sending its result along a telegraph, in which\nmistakes can happen along the way? Then, the problem of fixing a\ntransmission mistake looks equivalent to the problem of making a\nclassifier error. So, *if we can fix a transmission mistake even if\nevery digit can fail, we might be able to fix a multiclassifier\nmistake, even if every binary classifier can fail!*.\n\nIn the following, we will look at a very simple error-correcting\ncode. The [state of the art for error correcting in multiclass\nclassifiers](http://hunch.net/~beygel/tournament.pdf) is much more\ncomplicated than this, but the principle is the same.\n\nThe idea itself behind error correcting codes is simple. Say we want\nto transmit a word that can be one of `000`, `001`, `010`, `011`,\n`100`, `101`, `110`, or `111`. We will model a transmission error as\nflipping one of the bits of those words. If we send this\nrepresentation directly, then any errors that happen in the\ntransmission will become indistinguishable from a successful\ntransmission of a different message. This is a problem, because we\nwon't even know when something bad happens. It needs to be possible\nfor our communication process to know, by looking at the transmitted\nword, that something wrong happened. So there needs to be enough\n*room* in the space of possible messages for the case where \"bad stuff\nhappened\". This means that we will have to *encode* the word we want\nto send into a bigger space of possible transmissions, a space with more room.\n\nIn addition, in this new space of possible transmission, every\ncodeword should be far away from one another (where we [measure\ndistance by flipping\nbits](https://en.wikipedia.org/wiki/Hamming_distance), the same\nprocess that we assume was introducing mistakes), because we want to\ngive every codeword a \"safety cushion\" for errors, so that if we get a\nbad transmission, there's only one codeword nearby, so we will know\nhow to correct that mistake.\n\nThere are simple error detection methods, like sending the same\nmessage three times. If any of the words disagree with one another,\nthen we know a mistake happened somewhere (and we correct it by\nmajority voting).\n\nThis, however, is not a very efficient code, and most importantly, it\nwill not work in the case of multiclass classification! When sending\nmessages on radio, each error happens ([well, kinda](https://en.wikipedia.org/wiki/Burst_error)) independently of one another.\nBut in our case, a mistake happens when a classifier does badly for a given example. So if we just try to \"send the same digit three times\", it'll be the case\nthat if we make a mistake once, then we will make it three times! We need to come up with a better method.\n(Specifically, I will describe below the [Hamming(7,4)\ncode](https://en.wikipedia.org/wiki/Hamming(7,4)).)\n\n## Let's not repeat digits\n\nThe code I'm going to describe to you works on 4-bit words, so for now\nlet's pretend that our original problem had 16 classes instead\nof 8. The encoding process is itself very easy to describe. We send\nthe 4-bit word into a 7-bit space:\n\n* $ A = d\\_1\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_4 $\n* $ B = d\\_1\\ \\textrm{xor}\\ d\\_3\\ \\textrm{xor}\\ d\\_4 $\n* $ C = d\\_2\\ \\textrm{xor}\\ d\\_3\\ \\textrm{xor}\\ d\\_4 $\n* $ D = d\\_1 $\n* $ E = d\\_2 $\n* $ F = d\\_3 $\n* $ G = d\\_4 $\n\nHere, $d\\_1$ through $d\\_4$ are the digits of the word we want to\nsend. As we argued above, it must be the case that this code is\nredundant. Indeed, any valid word respects the following equations:\n\n* $ A\\ \\textrm{xor}\\ D\\ \\textrm{xor}\\ E\\ \\textrm{xor}\\ G = 0 $\n* $ B\\ \\textrm{xor}\\ D\\ \\textrm{xor}\\ F\\ \\textrm{xor}\\ G = 0 $\n* $ C\\ \\textrm{xor}\\ E\\ \\textrm{xor}\\ F\\ \\textrm{xor}\\ G = 0 $\n\nThese equations are easy to prove: remember that $\\textrm{xor}$ is\ncommutatitive and associative, and that $a\\ \\textrm{xor}\\ a = 0$.\nMoreover, the equations are also the key to making error detection work.\nHere's a minor modification we will make. Let's assume that an error\ncan happen on any digit we send, so we replace $A$ with $\\tilde{A}$,\netc:\n\n* $ \\tilde{A} = \\epsilon_A\\ \\textrm{xor}\\ d\\_1\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_4 $\n* $ \\tilde{B} = \\epsilon_B\\ \\textrm{xor}\\ d\\_1\\ \\textrm{xor}\\ d\\_3\\ \\textrm{xor}\\ d\\_4 $\n* $ \\tilde{C} = \\epsilon_C\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_3\\ \\textrm{xor}\\ d\\_4 $\n* $ \\tilde{D} = \\epsilon_D\\ \\textrm{xor}\\ d\\_1 $\n* $ \\tilde{E} = \\epsilon_E\\ \\textrm{xor}\\ d\\_2 $\n* $ \\tilde{F} = \\epsilon_F\\ \\textrm{xor}\\ d\\_3 $\n* $ \\tilde{G} = \\epsilon_G\\ \\textrm{xor}\\ d\\_4 $\n\nHere, we use $\\epsilon\\_A$, $\\epsilon\\_B$, etc. to model errors in our\ntransmission. The code we are describing here can only correct\nmistakes if only one of them happens at a time. That means that for\nevery transmission, either $\\epsilon\\_A = \\epsilon\\_B = \\cdots = 0$,\nor a single one of the $\\epsilon$ variables is one and all the others\nare still zero.\n\nThis is where the magic happens. Let's say $\\epsilon_A = 1$. Although\nit seems like it will be hard to figure out that something went wrong\n(since the 7-bit word we transmitted \"looks fine\"), let's look at\nthose three equations that we derived which describe the redundancy in the code. Now, consider their equivalents with\n$\\tilde{A}$, etc. I'm also giving each term a name now, $c\\_0$ through $c\\_2$:\n\n* $ c\\_0 = \\tilde{A}\\ \\textrm{xor}\\ \\tilde{D}\\ \\textrm{xor}\\ \\tilde{E}\\ \\textrm{xor}\\ \\tilde{G} = \\epsilon\\_{A}\\ \\textrm{xor}\\ \\epsilon\\_{D}\\ \\textrm{xor}\\ \\epsilon\\_{E}\\ \\textrm{xor}\\ \\epsilon\\_{G} $\n* $ c\\_1 = \\tilde{B}\\ \\textrm{xor}\\ \\tilde{D}\\ \\textrm{xor}\\ \\tilde{F}\\ \\textrm{xor}\\ \\tilde{G} = \\epsilon\\_{B}\\ \\textrm{xor}\\ \\epsilon\\_{D}\\ \\textrm{xor}\\ \\epsilon\\_{F}\\ \\textrm{xor}\\ \\epsilon\\_{G} $\n* $ c\\_2 = \\tilde{C}\\ \\textrm{xor}\\ \\tilde{E}\\ \\textrm{xor}\\ \\tilde{F}\\ \\textrm{xor}\\ \\tilde{G} = \\epsilon\\_{C}\\ \\textrm{xor}\\ \\epsilon\\_{E}\\ \\textrm{xor}\\ \\epsilon\\_{F}\\ \\textrm{xor}\\ \\epsilon\\_{G} $\n\nNotice two very important things on the right hand sides: every\nerror variable appears at least once, and no error variable was\ncancelled out from appearing more than once in the left-hand sides.\n\nThere's also a less immediate property of these $c$ variables: *every\npossible mistake case creates a different configuration of $c\\_0$,\n$c\\_1$, and $c\\_2$*! This is truly remarkable, and you should check it\nfor yourself, by setting the values of $\\epsilon\\_A$ through\n$\\epsilon\\_G$ to 1. For example, you should be able to convince\nyourself that if $c\\_0 = 0$, $c\\_1 = 1$, and $c\\_2 = 1$, then it must\nbe the case that $\\epsilon\\_{F} = 1$, which means that the mistake\nhappened on $\\tilde{F}$, which in turn means that all we have to do is\nflip $\\tilde{F}$ to obtain $F$, and all other digits must be correct!\nIf all $c$s are zero, then no error happened. (Again, we are assuming\nthat errors happen only one at a time. Fate is not always this kind.)\n\n## Now to turn this into a multiclass reduction\n\nThe final remarkable property of this error-correcting code is that\neach digit in the new transmission is formed by a particular\ncombination of the digits in the original problem. Let's think of the binary classification problems in the \"binary transmission\" reduction. (Here, I'll slip into hexadecimal notation and call the 16 classes `0123456789abcdef`):\n\n* $d_1$: <tt>01234567&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt> vs. the negative: <tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;89abcdef</tt>\n* $d_2$: <tt>0123&nbsp;&nbsp;&nbsp;&nbsp;89ab&nbsp;&nbsp;&nbsp;</tt> vs. the negative: <tt>&nbsp;&nbsp;&nbsp;&nbsp;4567&nbsp;&nbsp;&nbsp;&nbsp;cdef</tt>\n* $d_3$: <tt>01&nbsp;&nbsp;45&nbsp;&nbsp;89&nbsp;&nbsp;cd&nbsp;</tt> vs. the negative: <tt>&nbsp;&nbsp;23&nbsp;&nbsp;67&nbsp;&nbsp;ab&nbsp;&nbsp;ef</tt>\n* $d_4$: <tt>0&nbsp;2&nbsp;4&nbsp;6&nbsp;8&nbsp;a&nbsp;c&nbsp;e</tt> vs. the negative: <tt>&nbsp;1&nbsp;3&nbsp;5&nbsp;7&nbsp;9&nbsp;b&nbsp;d&nbsp;f</tt>\n\n(The whitespace is there simply to make the logic more apparent.)\nThese are our \"base-digits\" classifiers. The \"checksum-digit\" classifiers, that is, the redundant classifiers we will build to let us fix mistakes, will use the $\\textrm{xor}$ operation as well. \nConcretely speaking, the classifier for $d\\_1\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_4$ (that is, our transmitted digit $\\tilde{A}$ above) should return a positive label when \n$d\\_1\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_4 = 1$, or equivalently, it should\nseparate the classes `02579bce` from `13468adf`. \n\nTake a moment to convince yourself of why that makes sense: that split\nseparates the classes that appear in $d\\_1$, $d\\_2$, or $d\\_4$ an odd number of times from those that appear there an\neven number of times.  In other words, this classifier will \"attempt\nto send 1\" if $d\\_1\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_4 = 1$, and\n\"attempt to send 0\" if $d\\_1\\ \\textrm{xor}\\ d\\_2\\ \\textrm{xor}\\ d\\_4 =\n0$, *exactly like our communication example!*\n\nNow, every time any one of our classifiers makes an isolated mistake,\nwe will be able to spot it and fix it, at the expense of training only\nthree more classifiers. And all of these classifiers are trained on different label distributions, and so we don't run into the issue of repeated classification mistakes (like we would if we used the \"repeat transmission\" code).\n\nIsn't that amazing?\n\n\n# Further reading\n\nThere are a number of details that have to be handled when actually\ndeploying this strategy. The main one is that the choice on how to\norder the classes to create those splits is quite important, both\nbecause we need the individual classifiers to be good, and because we\nneed digit errors to be relatively uncorrelated to one another.\n\nIn practice, what the literature suggests is to *invent a new custom\ncode for every multiclass classification problem*. This is easier than\nit looks, and if you're interested, then Dietterich and\nBakiri's [seminal paper](https://arxiv.org/pdf/cs/9501101.pdf) is what\nyou should read next. This technique is now known as ECOC\n(\"error-correcting output codes\").\n\nThe question of whether ECOC's are the best we can do remained open\nfor a while, and although it's still not completely settled, a (much\nmore complicated) solution that combines error correction with the\nbinary tournament idea from above gets pretty close to optimal. These\nare [error-correcting tournaments](https://arxiv.org/abs/0902.3176),\nproposed by Beygelzimer et al. I confess I don't actually understand\nECTs all that well, but the good news is that at least they're\nimplemented in [Vowpal\nWabbit](https://github.com/VowpalWabbit/vowpal_wabbit). If\neach of the base classifiers achieves an average error rate of\n$\\epsilon$, error-correcting tournaments achieve a multiclass error\nbound of $2 \\epsilon$. This is independent of the number of classes in\nthe multiclass problem, which is quite incredible.\n\n## Notes\n\n* The reason we can't use the \"repeat the same message more than once\" trick\n  is that we assumed we were always training the same\n  classifier. If you have two binary classifiers for the same problem that\n  both predict reasonably well, and whose predictions are\n  relatively uncorrelated, then you can absolutely use this \"repeat\n  message\" method. This turns out to work [incredibly well in practice](https://en.wikipedia.org/wiki/Bootstrap_aggregating)!"
  },
  {
    "href": "explainers/index.html#basics",
    "title": "Explainers",
    "section": "Basics",
    "text": "Trigonometry"
  },
  {
    "href": "explainers/index.html#probability",
    "title": "Explainers",
    "section": "Probability",
    "text": "Bayes’s Rule and Fagan’s Nomogram"
  },
  {
    "href": "explainers/index.html#supervised-learning",
    "title": "Explainers",
    "section": "Supervised Learning",
    "text": "Methods which use data to make predictions about new, unseen data.\n\nLinear Regression\nLogistic Regression\nSupport Vector Machines\nError-correcting output codes\n\\(k\\)-nearest neighbor classification\nPerceptron"
  },
  {
    "href": "explainers/index.html#unsupervised-learning",
    "title": "Explainers",
    "section": "Unsupervised Learning",
    "text": "Methods which, given a dataset that has a complex representation, create simpler versions of the dataset.\n\nPrincipal Components Analysis\nMultidimensional Scaling\n\\(k\\)-means"
  },
  {
    "href": "explainers/index.html#linear-algebra",
    "title": "Explainers",
    "section": "Linear Algebra",
    "text": "Singular Value Decomposition\nAdjoints and Inverses\nEigenwhat?"
  },
  {
    "href": "explainers/index.html#other",
    "title": "Explainers",
    "section": "Other",
    "text": "Regularization\nA simple illustration of duality\nConvolutions\nAutomatic Differentiation\nRandomized Response"
  },
  {
    "href": "explainers/index.html#data-visualization",
    "title": "Explainers",
    "section": "Data visualization",
    "text": "Function fitting and reconstruction:\n\nB-Splines. Intro to B-Splines.\nLinear Function Spaces. Brief intro to using linear function spaces and reconstruction kernels to represent continuous functions in a computer.\nLagrange’s interpolating polynomials.\nNewton’s interpolating polynomials.\nPadé approximants.\n\nODE integration. Intro to numerical integration of ordinary differential equations."
  },
  {
    "href": "explainers/index.html#signal-processing",
    "title": "Explainers",
    "section": "Signal Processing",
    "text": "Aliasing. A simple illustration of aliasing.\n\n\nSource Code\n---\ntitle: Explainers\n---\n\nThis is a collection of small explanatory notes and interactive\ndemos. They started with a focus on data science, but have sprawled\ninto \"math and CS explainers\".\n\nThese notes are probably more informal than what you may be\nexpecting. If you're looking for textbooks, I'm sure you can find\nexcellent ones.\n\n## Basics\n\n* [Trigonometry](trigonometry/index.qmd)\n\n## Probability\n\n* [Bayes's Rule and Fagan's Nomogram](fagans-nomogram/index.qmd) \n\n## Supervised Learning\n\nMethods which use data to make predictions about new, unseen data.\n\n* [Linear Regression](linear-regression.qmd)\n* [Logistic Regression](logistic-regression/index.qmd)\n* [Support Vector Machines](svm/index.qmd)\n* [Error-correcting output codes](error-correcting-output-codes/index.qmd)\n* [$k$-nearest neighbor classification](nearest-neighbors/index.qmd)\n* [Perceptron](perceptron/index.qmd)\n\n## Unsupervised Learning\n\nMethods which, given a dataset that has a complex representation,\ncreate simpler versions of the dataset.\n\n* [Principal Components Analysis](pca.qmd)\n* [Multidimensional Scaling](mds.qmd)\n* [$k$-means](kmeans/index.qmd)\n\n## Linear Algebra\n\n* [Singular Value Decomposition](svd.qmd)\n* [Adjoints and Inverses](adjoints-and-inverses.qmd)\n* [Eigenwhat?](eigenvectors/index.qmd)\n\n## Other\n\n* [Regularization](regularization/index.qmd)\n* [A simple illustration of duality](duality.qmd)\n* [Convolutions](convolution/index.qmd)\n* [Automatic Differentiation](automatic-differentiation/index.qmd)\n* [Randomized Response](randomized-response/index.qmd)\n\n## Data visualization\n\n* Function fitting and reconstruction:\n  * [B-Splines](b-splines/index.qmd). Intro to B-Splines.\n  * [Linear Function Spaces](linear-function-spaces/index.qmd). Brief\n    intro to using linear function spaces and reconstruction kernels to\n    represent continuous functions in a computer.\n  * [Lagrange's interpolating polynomials](lagrange-polynomials.qmd).\n  * [Newton's interpolating polynomials](newton-polynomials.qmd).\n  * [Padé approximants](pade-approximant/index.qmd).\n* [ODE integration](odes/index.qmd). Intro to numerical integration\n  of ordinary differential equations.\n\n## Signal Processing\n\n* [Aliasing](aliasing.qmd). A simple illustration of aliasing."
  },
  {
    "href": "explainers/tda/PH.html",
    "title": "Persistent Homology",
    "section": "",
    "text": "NB: This is not finished.\n\n\n\n\n\n\nSource Code\n---\ntitle: Persistent Homology\n---\n\nNB: This is not finished.\n\n<script src=\"delaunay.js\"></script>\n\n<div id=\"div-ph\"></div>\n\n<script type=\"module\" src=\"./main.js\"></script>"
  },
  {
    "href": "explainers/tda/index.html",
    "title": "Topological Data Analysis",
    "section": "",
    "text": "PH\n\n\nSource Code\n---\ntitle: Topological Data Analysis\n---\n\n* [PH](ph.qmd)"
  },
  {
    "href": "explainers/earth-movers-distance.html",
    "title": "Earth Mover’s Distance",
    "section": "",
    "text": "In many real-life situations, \\(L_p\\) are not very useful metrics. If your vector space has a good ground metric (think of “distances between pixels” in an image), then the “earth mover’s distance” (EMD) is better. Mathematicians call this the Wasserstein metric.\nGive a cost from moving “stuff” from one pixel to another, and define the distance between two images as the minimal cost to move stuff so that the first image equals the second. This better matches our intuition that two images that differ only by nearby pixels should be closer than two images that differ by pixels that are far away from one another. The \\(L_2\\) metric is invariant to shuffling the pixels in an image (this is very weird when you think about it); the EMD is not.\nHere we demonstrate the difference between interpolating along the “optimal transport” (a shortest path in the EMD metric) and linear interpolation (a shortest path in the \\(L_2\\) metric).\n\n\n\n\nlinear interpolation\n\noptimal transport\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource Code\n---\ntitle: Earth Mover's Distance\nresources:\n  - ../data/datasets/mnist/*-ubyte\n---\n\nIn many real-life situations, $L_p$ are not very useful metrics. If\nyour vector space has a good ground metric (think of \"distances\nbetween pixels\" in an image), then the \"earth mover's distance\" (EMD)\nis better. Mathematicians call this the Wasserstein metric.\n\nGive a cost from moving \"stuff\" from one pixel to another, and define\nthe distance between two images as the minimal cost to move stuff so\nthat the first image equals the second. This better matches our\nintuition that two images that differ only by nearby pixels should be\ncloser than two images that differ by pixels that are far away from\none another. The $L_2$ metric is invariant to shuffling the pixels in\nan image (this is very weird when you think about it); the EMD is not.\n\nHere we demonstrate the difference between interpolating along the\n\"optimal transport\" (a shortest path in the EMD metric) and\nlinear interpolation (a shortest path in the $L_2$ metric).\n\n<div id=\"emd-mnist-canvas\"></div>\n\n<button id=\"lerp\">linear interpolation</button> <button id=\"emd\">optimal transport</button>\n\n```{ojs}\n//| output: false\n//| echo: false\nimport { cscheid } from \"/js/cscheid/cscheid.js\";\nimport { main } from \"./earth-movers-distance.js\";\n{\n  cscheid.setup.setupGlobals({ d3 });\n  main();\n}\n```"
  },
  {
    "href": "explainers/svm/index.html#demo",
    "title": "Support Vector Machines",
    "section": "Demo",
    "text": "\\(\\lambda\\): 0.1(increase) (decrease)\n\n\n\n\nSource Code\n---\ntitle: Support Vector Machines\n---\n\nThe soft-margin support vector machine (SVM) is a classic method for\nsupervised learning. \n\nIt looks to find a *large-margin* classifier: one for which its\ndecision boundary is far from the examplars. This is unlike the linear\nperceptron, which will not in general guarantee large margins. The SVM\nmodel minimizes the following loss:\n\n$$ L(w) = \\lambda ||w||^2 + \\sum_{(x, y)} L_H(w, x, y) $$\n\nHere, $H_L$ denotes the *hinge loss*: \n\n$$ L_H(w, x, y) = \\left \\{ \\begin{array}{rl} 1 - y \\langle w, x \\rangle &, \\textrm{if} \\ y \\langle w, x \\rangle \\le 1 \\\\ 0&, \\textrm{otherwise} \\end{array} \\right . $$\n\nThe hinge loss is particularly useful for classification because of\ntwo reasons. First, it is convex, which means that there exist\nalgorithms that minimize $L(w)$ efficiently. Second, in the region\nwhere the misclassification loss (of a linear classifier) returns\nzero, the hinge loss has compact support, specifically in a way such\nthat for points that are sufficiently far from the decision boundary,\nthe hinge loss is zero.\n\nCrucially, that second reason implies that the position of correctly\nclassified points that are sufficiently away from the decision\nboundary *does not matter for the classifier*. One way to see this\nintuitively is that if you are given a classifier that attains the\nminimum, together with one point for which this classifier gives a\nhinge loss of zero, then if you wiggle this point in any direction,\nthe loss will still be zero, and that means that the classifier will\nstill be optimal, even with this wiggled point. (Of course, if you\nwiggle the point so much that it crosses into the region of the hinge\nloss where the value is non-zero, then you'll potentially change the\nclassifier.)\n\nAs a result, after the training procedure finishes, we can identify the subset\nof input points which influence the decision: these are\nthe **support vectors** (they \"support\" the decision). \nSupport vectors are particularly\nimportant in the \"kernelized\" formulation of the SVM.\nA general kernelized linear classifier needs to potentially access all training points to\nmake a test-time prediction. The SVM, in contrast, needs only to store the\nsupport vectors, which might be a small fraction of all the\ninput points.\n \nYou can see the support vectors in the example below, where we train a\nsupport vector machine with 2D data (using a quadratic polynomial\nkernel). As $\\lambda$ gets smaller, the penalty for making hinge loss\nerrors gets comparatively larger, so the margin of the classifier\nitself gets smaller. As a result, the number of **support vectors**\ngets smaller. Note how the support vectors are all points that are\neither misclassified, or are points inside the $[-1, 1]$ range of the\nclassification values: \"within the margin\".\n\n## Demo\n\n<a name=\"#svm-plot\"></a>\n<div class=\"center-scaffold\"><div id=\"svm-surface\"></div><br><div>$\\lambda$: <span class=\"editable\" contenteditable=\"true\" id=\"svm-lambda\">0.1</span><br>(<a href=\"#svm-plot\" id=\"svm-increase-lambda\">increase</a>) (<a href=\"#svm-plot\" id=\"svm-decrease-lambda\">decrease</a>)</div></div>\n\n<script type=\"module\" src=\"./main.js\"></script>"
  },
  {
    "href": "explainers/linear-function-spaces/two-d/index.html",
    "title": "Two-dimensional linear function spaces",
    "section": "",
    "text": "Source Code\n---\ntitle: Two-dimensional linear function spaces\n---\n\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js\"></script>\n<script src=\"../../../js/lux.js\"></script>\n\n<div id=\"bilinear-reconstruction\" class=\"chart\"></div>\n\n<script type=\"module\" src=\"./main.js\"></script>"
  },
  {
    "href": "explainers/linear-function-spaces/index.html#basic-example",
    "title": "Linear function spaces",
    "section": "Basic Example",
    "text": "def constant(x):\n    return 1\n\ndef line(x):\n    return x\n\ndef line(a, b):\n    def f(x):\n        return a * constant(x) + b * line(x)\n    return f\n\n# line(1, 3)(5) == 16\n\n\n\nEach different function in this space, then, is completely characterized by the array of weights we use, making the space very convenient for storage and representation on computers."
  },
  {
    "href": "explainers/linear-function-spaces/index.html#polynomials-are-not-great-for-this-job",
    "title": "Linear function spaces",
    "section": "Polynomials are not great for this job",
    "text": "One standard way of creating new functions (so that our basis gets richer and we can create more interesting linear function spaces) is to take powers of existing functions. For example, taking linear combinations of non-negative powers of the identity function yields the space of all polynomials. This is a very rich space, but is not very good for approximation when you have equally-spaced values. Let’s see what happens when we try to approximate the Runge function, \\(f(x) = 1 / (1 + 25x^2)\\), using progressively larger-degree polynomials (depicted by darker, thicker lines), such that your polynomial always matches the values of the function that you have observed:\n\n\n\nAs you can see, the higher-degree polynomial fits match the sampled data at more points, but they start to “wiggle” more, such that in between the observed points, the values oscillate wildly."
  },
  {
    "href": "explainers/linear-function-spaces/index.html#shifted-versions-of-the-same-function",
    "title": "Linear function spaces",
    "section": "Shifted Versions of the same function",
    "text": "Instead, we’ll build our space by taking simple functions that have finite support, and shifting them around:\ndef square(x):\n    if x <= -1/2: return 0\n    if x >= 1/2: return 0\n    return 1\n\ndef shifted_square(i):\n    def f(x):\n        return shifted_square(x - i)\n    return f\n\n\n\nNow linear function spaces of (progressively more) shifted squares do something more interesting:\n\n\n\nWhen we try to approximate the same function as before, the sampled values (and the rectangles) track the function progressively better and better, and the approximation function does not oscillate. Of course, we lost something in this exchange. The polynomial fits we obtained were smooth: this means that we had access to function derivatives, which are useful in a variety of settings. The function fits we got aren’t even continuous; that’s not ideal.\nFortunately, this is a situation which we can easily fix by creating better “simple functions”. Also, from now on, we will call these simple functions reconstruction kernels, or kernels for short. (Notice that the term kernel is used in statistics to denote a completely different notion. The confusion is most often avoided by noticing that reconstruction kernels take a single parameter while similarity kernels from statistics are always two-parameter functions that compare values.)"
  },
  {
    "href": "explainers/linear-function-spaces/index.html#b-spline-kernels",
    "title": "Linear function spaces",
    "section": "B-Spline kernels",
    "text": "def b0(x):\n    if x <= -1/2: return 0\n    if x >= 1/2: return 0\n    return 1\ndef b1(x):\n    if x <= -1: return 0\n    if x <= 0:  return 1+x\n    if x <= 1:  return 1-x\n    return 0\ndef b2(x):\n    # ...\nSo, how do we create function spaces that aren’t prone to oscillation, but allow smooth functions? We use kernels that are themselves continuous, or smooth. In the examples below, we are creating functions by giving weights \\([0.4, 0.3, 0.5, 0.7, 0.6]\\) to each of the basis functions in order. Because the basis functions are different, the reconstructed function is itself different.\n\nB-Spline of order 0\n\n\n\n\n\nLinear (order-1) B-Spline\n\n\n\n\n\nQuadratic (order-2) B-Spline\n\n\n\nA proper discussion of reconstruction kernels is far beyond the scope of this (and any one) piece; see this piece instead.\nNote, though, that as the reconstruction kernels get smoother, so do the reconstructions. This happens because of a trivial, but very important property of linear function spaces: since these are linear spaces, function operations that are themselves linear will factor through to the basis functions. For example, the derivative is a linear operator. As a result, the derivative of a function from a linear function space is necessarily a member of a different linear function space, whose basis vectors are the derivatives of the basis vectors of the original function space:\n\\[d/dx \\left ( \\sum_i c_i \\phi_i(x) \\right ) = \\sum_i c_i \\left (\\frac{d \\phi(x)}{dx} \\right)\\]\nThe same thing happens for integrals, expectations, convolutions, etc. This makes linear function spaces very computationally convenient."
  },
  {
    "href": "explainers/linear-function-spaces/index.html#multi-dimensional-linear-function-spaces",
    "title": "Linear function spaces",
    "section": "Multi-dimensional linear function spaces",
    "text": "So far, we’ve seen function spaces whose domain are the real numbers (a one-dimensional space). It is straightforward to extend the notion to multidimensional functions: we just change the domain of the function to operate on \\(R^n\\) instead of \\(R\\). The only significant change is that our reconstruction kernels need to be themselves two-dimensional.\nDo note here a bit of terminological ambiguity: in this context, we are using “multi-dimensional” to refer to the domain of the function, not to the dimension of the space (as in its rank).\n\nSeparable kernels\nThe most common way to create multi-dimensional reconstruction kernels is to do it one dimension at a time. Concretely speaking, the space of all possible multidimensional reconstruction kernels is very large, and one natural solution is to look for kernels that factor:\n\\[K(x,y) = k_x(x) k_y(y)\\]\nUsing B-Splines as the separable filters is very common, and very convenient.\nTODO: Add illustrations of a simple 2D function using \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) as the separable filters."
  },
  {
    "href": "explainers/b-splines/index.html#properties",
    "title": "B-Splines",
    "section": "Properties",
    "text": "They’re built by self-convolution\nThey’re piecewise polynomials, each easily expressible in the Bernstein basis\nThey integrate to 1\nDerivatives are linear combinations of lower-order splines\n\n\nSource Code\n---\ntitle: B-Splines\n---\n\n# B-Splines\n\nThis is to be written.\n\n## Properties\n\n* They're built by self-convolution\n* They're piecewise polynomials, each easily expressible in the\n  Bernstein basis\n* They integrate to 1\n* Derivatives are linear combinations of lower-order splines"
  },
  {
    "href": "explainers/mds.html#classical-mds",
    "title": "Multidimensional Scaling",
    "section": "Classical MDS",
    "text": "There are many, many variants of “multidimensional scaling”, with different assumptions on the measurements, and different ways to reconstruct the values. Here we will focus on only one of them, typically known as “Classical” multidimensional scaling, or CMDS. For more, the best reference is Borg and Groenen’s Modern Multidimensional Scaling1."
  },
  {
    "href": "explainers/mds.html#from-distances",
    "title": "Multidimensional Scaling",
    "section": "From distances",
    "text": "Imagine that, instead of knowing the coordinates of a set of points, all you knew were the distance from each point to each other point. Intuitively, it seems like it should be possible to convert this information back into coordinates which respect the distances. Classical MDS attempts to do precisely that. Let’s start with the algorithm itself:\n\n\\(D = (||X_i - X_j||^2)_{ij}\\): let \\(D\\) be a matrix that stores the squared distance from point \\(i\\) to point \\(j\\) (notice that we do not currently have the \\(X_i\\) and \\(X_j\\) vectors; we’re just using the notation to say what the entries of the matrix should store.\n\\(P = (-1/2) H D H\\): let \\(P\\) be a row- and column-centered version of \\(D\\), multiplied by \\(-1/2\\) (\\(H\\) is the centering matrix, as described in the PCA section).\nLet \\(P = U \\Lambda U^T\\).\nThe coordinates of \\(X\\) are given by the first few columns of \\(U \\Lambda^{1/2}\\).\n\nThe decision of how many coordinates to use for the MDS is similar to the decision of how many principal components to choose: the smaller the values in \\(\\Lambda\\), the more precise the result will be (that is, the squared distance between \\(X_i\\) and \\(X_j\\) will become closer to the actual value in \\(D_{ij}\\)).\nIn fact, the easiest way to understand how CMDS works is to compare it directly to PCA. Specifically, let’s look at what happens if we started from an actual matrix \\(X\\) from which to build \\(D\\). In this case,\n\\[\\begin{align} D_{ij} &=& || X_i - X_j ||^2 \\\\ D_{ij} &=& \\langle X_i - X_j, X_i - X_j \\rangle\\\\ D_{ij} &=& \\langle X_i, X_i \\rangle + \\langle X_j, X_j \\rangle - 2 \\langle X_i, X_j \\rangle. \\end{align}\\]\nSo what we get is that every entry \\(i, j\\) of the matrix \\(D\\) can be represented by a sum of three terms. So we write the matrix \\(D\\) as a sum of three matrices:\n\\[\\begin{align} D &=& (\\langle X_i, X_i \\rangle)_{ij} + (\\langle X_j, X_j \\rangle)_{ij} + (-2 \\langle X_i, X_j \\rangle)_{ij}\\\\ D &=& A + B + C\\end{align}\\]\nNow we follow step 2 of the algorithm above, and let\n\\[\\begin{align} P &=& -1/2 H D H \\\\ P &=& -1/2 H (A + B + C) H \\\\ P &=& -(1/2) H A H -(1/2) H B H -(1/2) H C H \\end{align}\\]\nNow note that all of the columns of \\(A\\) are identical, because the values only depend on the row index \\(i\\). Similarly, all of the rows in \\(B\\) are identical, because the values only depend on the column index \\(j\\). This means that \\(HAH = HBH = 0\\)!, since column-centering \\(A\\) will subtract the average column value; the analogous thing happens to row-centering \\(B\\). As a result,\n\\[\\begin{align} P &=& -1/2 H D H \\\\ P &=& H (\\langle X_i, X_j)_{ij} \\rangle H \\end{align}\\]\nSo our \\(P\\) matrix is now exactly equal to a centered matrix of inner products of \\(X\\), even though we never used the inner products directly – all we had access to was squared distances. So if we take \\(P\\) to be the matrix \\(M\\) on step 3 of our second PCA algorithm, we get to recover \\(X\\) exactly! This is really neat."
  },
  {
    "href": "explainers/mds.html#from-similarities",
    "title": "Multidimensional Scaling",
    "section": "From similarities",
    "text": "The same algorithm also applies directly when all we have access to is a notion of similarity between points. Here the idea is even simpler. Let’s say that we have a way to give a numerical value of similarity between points such that if two points are similar to each other, the value is relatively large, and if two points are relatively dissimilar, the value is relatively small. Then we can pretend that this value is “like an inner product”, create a matrix of similarities, center the matrix in both rows and columns, and just compute the PCA directly like above. This will actually recover good coordinates the respect the similarities!"
  },
  {
    "href": "explainers/expectation.html#expectations-over-expressions",
    "title": "Expectation",
    "section": "Expectations over expressions",
    "text": "Expectations are useful because they are easy to manipulate. So we can write things like: \\(E[X^2 - X]\\) or \\(E[(X+Y)^2]\\).\nSome of these expressions can get confusing. For example, what if \\(X\\) denotes the value of one die roll, and \\(Y\\) the value of another roll? The rule to keep in mind is that the expectation operator is always taken “over a single dataset” (more formally, it’s always over a given, single random variable and its associated probability space). So in the example above, there is an implicit operation of turning the two dice rolls into a single experiment, and in that case you need to be careful to think about the probabilities that each particular case gets (read more on probability spaces).\nThe main feature of the expectation is that it’s a linear operator:\n\\[E[X + Y] = E[X] + E[Y]\\]\n\\[E[kX] = k E[X]\\]\n\\[E[k] = k\\]\nIt means that we can understand complicated expectation expressions by breaking them into sums, and turning those into sums of (simpler) expectations. And this is really why they are so popular, even when sometimes it doesn’t make sense to use them.\n\nSource Code\n---\ntitle: Expectation\n---\n\n# Expectation\n\nWhen we have a dataset of interest, we often seek to find a way to\nsummarize the information in the data. The principle we are following here\nis that we want to somehow explain the phenomenon that generated the\ndata, and we believe that this explanation requires \"less data\" than\nthe entire dataset. We will produce a summary of the dataset\n(somehow), and our explanations will only refer to the summary. The\nassumption here is that \"the stuff we left out\" of the explanation is,\nsomehow, less important. It might be noise, or it might be a\nparticular aspect of the data we don't care about.\n\nThe *expectation* of a variable is the simplest way to describe a\nsummary of data, and is as ubiquitous as it useful. In its simplest\nform, the expectation of a set of numbers is just the arithmetic mean\nof their values:\n\n$$E[X] = (1/|X|) \\sum_{x \\in X} x$$\n\nThe expectation (or \"expected value\") gives a notion of centrality:\nwhat a value tends to be. It's important to know right away that there\nare many such notions: for example, the mode and the median are two\nother useful notions. So even though you *can* use expectation to\ngive a single value that somehow stands for the entire dataset, it\ndoesn't mean you *should*.\n\nThe first way in which expectations can be generalized from the\nformula above is that we often want to talk about scenarions in which\nsome situations are more likely than others. Intuitively, if elements\nin our dataset have a notion of *probability* associated with them, we\nwould like our notion of expectation to somehow take this into\naccount. \n\nFor example, if you have a fair 6-sided die, then the\nexpectation of the value of the roll is just $1/6 * (1+2+3+4+5+6) =\n21/6$. But if your friend wants to sell you a bet with you\nwhere he flips a loaded coin that's five times as likely to give tails than\nheads, and he'll pay you a dollar only if the coin lands on heads,\nthen it seems wrong to think that the expectation will be $1/2 * (0 +\n1)$. Instead, what we want to do is to associate with each element of\nthe dataset (or \"each outcome of the experiment\") a real number, such\nthat these numbers sum to 1. These are the probabilities. The\nexpectation is then the sum of the values multiplied by the\nprobability associated to each value.\n\n## Expectations over expressions\n\nExpectations are useful because they are easy to manipulate. So we can\nwrite things like: $E[X^2 - X]$ or $E[(X+Y)^2]$.\n\nSome of these expressions can get confusing. For example, what if $X$\ndenotes the value of one die roll, and $Y$ the value of another roll?\nThe rule to keep in mind is that the expectation operator is always taken \"over a\nsingle dataset\" (more formally, it's always over a given, single\nrandom variable and its associated probability space). So in the example\nabove, there is an implicit operation of turning the two dice rolls\ninto a *single* experiment, and in that case you need to be careful to\nthink about the probabilities that each particular case gets (read\n[more on probability spaces](probabilities.html)).\n\nThe main feature of the expectation is that it's a *linear* operator:\n\n$$E[X + Y] = E[X] + E[Y]$$\n\n$$E[kX] = k E[X]$$\n\n$$E[k] = k$$\n\nIt means that we can understand complicated expectation expressions by\nbreaking them into sums, and turning those into sums of\n(simpler) expectations. And this is really why they are so popular,\neven when sometimes it doesn't make sense to use them."
  },
  {
    "href": "explainers/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Source Code\n---\ntitle: Perceptron\n---\n\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.4/lodash.min.js\"></script>\n\n<div id=\"div-perceptron\"></div>\n\n<div><span id=\"button-step\" style=\"margin-top:1em\"></span> <span id=\"button-reset\" style=\"margin-top:1em\"></span></div>\n\n<script src=\"./index.js\" type=\"module\"></script>"
  },
  {
    "href": "explainers/svd.html#problems",
    "title": "Singular Value Decomposition",
    "section": "Problems",
    "text": "In the problem of linear least squares, we are given a matrix \\(M\\) (whose \\(n\\) rows and \\(m\\) columns encode \\(n\\) data points each with \\(m\\) features) and a vector \\(b\\), and we seek a vector \\(x\\) such that the length of \\(||Mx - b||\\) is minimized.\n\n\nSource Code\n---\ntitle: Singular Value Decomposition\n---\n\n# Singular Value Decomposition\n\nAny matrix $M$ can be written as\n\n$$M = U \\Sigma V^T, $$\n\nwhere $U$ and $V$ are orthogonal, and $\\Sigma$ is diagonal with\nnon-negative entries.\n\nBecause orthogonal and diagonal matrices have many convenient\nproperties, it's often simpler to replace a matrix with its SVD in\norder to analyze something.\n\n## Problems\n\n- In the problem of linear least squares, we are given a matrix\n  $M$ (whose $n$ rows and $m$ columns encode $n$ data points each with\n  $m$ features) and a vector $b$, and we seek a vector $x$ such that\n  the length of $||Mx - b||$ is minimized."
  },
  {
    "href": "explainers/lagrange-polynomials.html#polynomials-are-vector-spaces-and-thats-almost-all-we-need",
    "title": "Lagrange Polynomials",
    "section": "Polynomials are vector spaces, and that’s almost all we need",
    "text": "It’s useful to think polynomials as linear combinations of the monomials \\(x \\mapsto x^k\\), and (for example) subsets of those monomials forming different spaces. In this sense, the set \\(\\{x \\mapsto x^0, x \\mapsto x^1, x \\mapsto x^2\\}\\) forms a basis for all quadratic polynomials, since we can form any quadratic polynomial by weighting the basis vectors and adding them up.\nBut we can take linear combinations of any sets of polynomials to create other vector (sub) spaces. For example, the set \\(\\{(1-x)^0, (1-x), (1-x)^2\\}\\) also forms a basis for all quadratic polynomials; the only difference is that in order to express a quadratic in this basis, we need different weights than the monomial basis.\nThe Lagrange interpolating polynomial is easiest to study by describing how to think of the basis it creates.\nImagine first if we had polynomials \\(b_i\\) for each input point \\(x_i\\), such that \\(b_i(x_i) = 1\\), and if \\(i \\neq j\\), \\(b_i(x_j) = 0\\). If that were the case, then we could just use \\(y_i\\) as the weights for combining the \\(b_i\\) polynomials: \\(f(x) = \\sum_i y_i b_i(x)\\). At a given \\(x_i\\) point, all but \\(b_i\\) is non-zero, which when multiplied by \\(y_i\\) instantly gives \\(f(x_i) = y_i\\).\nSo all that’s left is for us to find a way to construct these polynomials. That turns out not to be too difficult. Let’s say we have three points, \\((x_0 = 1, y_0 = 3)\\), \\((x_1 = 3, y_1 = 6)\\), and \\((x_2 = 5, y_2 = 2)\\). We need \\(b_0(1) = 1\\), \\(b_0(3) = 0\\), \\(b_0(5) = 0\\). As we’ve seen before, it’s easy to create a polynomial if all we need to do is control its roots. In this case, \\(\\tilde{b}_0(x) = (x - 3)(x - 5)\\) guarantees that the polynomial is zero exactly where we need it to be zero. Unfortunately, \\(\\tilde{b}_0(1)\\) is not equal to one. But we know it is equal to \\((1-3)(1-5)\\). So we simply divide \\(\\tilde{b}_0\\) by that, which doesn’t change the value at the roots since they were zero anyway. Do the same thing for \\(b_1\\) and \\(b_2\\), and then add all three of them together, and you get the Lagrange polynomial."
  },
  {
    "href": "explainers/lagrange-polynomials.html#why-do-we-care",
    "title": "Lagrange Polynomials",
    "section": "Why do we care?",
    "text": "The generalizable principle here is that any time we fit functions to observations, it’s worth it to study the procedure by understanding what it does to datasets where the function is equal to \\(1\\) at one of the points equal to \\(0\\) elsewhere, because this usually creates a basis in the sense of linear algebra: a finite set of elements whose linear combinations covers (“spans”) the set of all possible elements we’re concerned with.\nIn the case of Lagrange’s polynomials, this basis is data-dependent: more specifically, the basis changes as a function of the \\(x\\) coordinates of the inputs, and the weights change as a function of the \\(y\\) coordinates of the inputs. Seeing the method in this perspective lets us better understand its behavior.\nFor example, in this case, we see that sufficiently far away from the points \\(x_i\\), this function will grow without bounds, because each of the \\(b_i\\) basis functions grows to infinity by themselves; this makes Lagrange polynomials especially risky to use as extrapolation methods. Other methods have different such properties.\nFor example, one way to think of LOWESS is that it forces these basis functions to be zero by multiplying the polynomial by a function that goes to zero away from the points at a rate faster than the polynomials grow."
  },
  {
    "href": "explainers/lagrange-polynomials.html#demo",
    "title": "Lagrange Polynomials",
    "section": "Demo",
    "text": "TBF.\n\nSource Code\n---\ntitle: Lagrange Polynomials\n---\n\nGiven a set of $n$ points $(x_i, y_i)$, we can create a polynomial\n$f(x)$ of degree at most $n-1$ such that $f(x_i) = y_i$. Although\nit's almost always better to do this through plain [linear\nregression](linear-regression.qmd), it's sometimes useful to know\nthis can be done.\n\nIn addition, the trick that we use to generate the polynomial is\nuseful in other settings, and gives a different perspective on other\ndata-fitting methods.\n\n\n### Prologue: Determining roots is easy\n\nIf all we wanted was to find a non-trivial polynomial with given\nroots, then that would be dead simple. For every value $x_i$ for\nwhich we wanted the polynomial to be zero, we create a term $(x -\nx_i)$, and then take the product of all of those, since if any of the\nterms are zero, the product is zero.\n\nBut for this interpolating polynomial, we don't want the values to be\nzero; we want them to be $y_i$. The Lagrange polynomials are built on\na simple extension of this idea (if you were to take the $(x - x_i)$\ntrick more directly head on, you would probably arrive at the idea of\n[Newton's interpolating polynomial](newton-polynomials.qmd)).\n\n\n## Polynomials are vector spaces, and that's almost all we need\n\nIt's useful to think polynomials as linear combinations of the\nmonomials $x \\mapsto x^k$, and (for example) subsets of those\nmonomials forming different spaces. In this sense, the set $\\{x\n\\mapsto x^0, x \\mapsto x^1, x \\mapsto x^2\\}$ forms a *basis* for all\nquadratic polynomials, since we can form any quadratic polynomial by\nweighting the basis vectors and adding them up.\n\nBut we can take linear combinations of *any* sets of polynomials to\ncreate other vector (sub) spaces. For example, the set $\\{(1-x)^0,\n(1-x), (1-x)^2\\}$ *also* forms a basis for all quadratic polynomials;\nthe only difference is that in order to express a quadratic in this\nbasis, we need different weights than the monomial basis.\n\nThe Lagrange interpolating polynomial is easiest to study by\ndescribing how to think of the basis it creates.\n\nImagine first if we had polynomials $b_i$ for each input point\n$x_i$, such that $b_i(x_i) = 1$, and if $i \\neq j$, $b_i(x_j) =\n0$. If that were the case, then we could just use $y_i$ as the\nweights for combining the $b_i$ polynomials: $f(x) = \\sum_i y_i\nb_i(x)$. At a given $x_i$ point, all but $b_i$ is non-zero, which\nwhen multiplied by $y_i$ instantly gives $f(x_i) = y_i$.\n\nSo all that's left is for us to find a way to construct these\npolynomials. That turns out not to be too difficult. Let's say we have\nthree points, $(x_0 = 1, y_0 = 3)$, $(x_1 = 3, y_1 = 6)$, and\n$(x_2 = 5, y_2 = 2)$. We need $b_0(1) = 1$, $b_0(3) = 0$, $b_0(5) =\n0$.  As we've seen before, it's easy to create a polynomial if all we\nneed to do is control its roots.  In this case, $\\tilde{b}_0(x) = (x -\n3)(x - 5)$ guarantees that the polynomial is zero exactly where we\nneed it to be zero.  Unfortunately, $\\tilde{b}_0(1)$ is not equal to\none. But we know it is equal to $(1-3)(1-5)$. So we simply divide\n$\\tilde{b}_0$ by that, which doesn't change the value at the roots\nsince they were zero anyway. Do the same thing for $b_1$ and $b_2$,\nand then add all three of them together, and you get the Lagrange\npolynomial.\n\n## Why do we care?\n\nThe generalizable principle here is that any time we fit functions to\nobservations, it's worth it to study the procedure by understanding\nwhat it does to datasets where the function is equal to $1$ at one of\nthe points equal to $0$ elsewhere, because this usually creates a\nbasis in the sense of linear algebra: a finite set of elements whose\nlinear combinations covers (\"spans\") the set of all possible elements\nwe're concerned with.\n\nIn the case of Lagrange's polynomials, this basis is *data-dependent*:\nmore specifically, the basis changes as a function of the $x$\ncoordinates of the inputs, and the weights change as a function of the $y$\ncoordinates of the inputs. Seeing the method in this perspective lets us better understand its\nbehavior.\n\nFor example, in this case, we see that sufficiently far away from the\npoints $x_i$, this function will grow without bounds, because each of\nthe $b_i$ basis functions grows to infinity by themselves; this makes\nLagrange polynomials especially risky to use as *extrapolation*\nmethods. Other methods have different such properties.\n\nFor example, one way to think of\n[LOWESS](https://en.wikipedia.org/wiki/Local_regression) is that it\nforces these basis functions to be zero by multiplying the polynomial\nby a function that goes to zero away from the points at a rate faster\nthan the polynomials grow.\n\n## Demo\n\nTBF."
  },
  {
    "href": "explainers/linear-regression.html#modeling-data-for-linear-regression",
    "title": "Linear Regression",
    "section": "Modeling data for linear regression",
    "text": "There are many ways to build a model from data. In regression, the training data is given as a set of pairs of independent and dependent variables. Independent variables are the part of the data our finished model will use to make predictions; dependent variables are what the predictions should be. So our training data always has both values of the dependent and independent variables, but our testing data only has the values of the independent variables. The job of the model is exactly to predict the value of the dependent variable.\nLet’s get a little more concrete. Imagine you are designing a computerized hygrometer: an instrument to read moisture content from the air. You are in charge of writing the software that will read varying amounts of voltage from the sensor and convert those readings into humidity measurements. As input, you are given a set of voltage readings and associated humidity values (the humidity values having been collected by some other trusted method). Let us call \\(x\\_i\\) the voltage values, and \\(y\\_i\\) the humidity values.\nOur first decision is as to what model we will use for the predictions. This decision cannot come alone from the data (although we will later discuss methods to help with this). In linear regression, the fundamental assumption is that the model is, well, linear: we look for the linear combination1 of simpler, known models that best predicts the training examples.\nFor example, a model that says the humidity \\(y\\) will be predicted as\n\\[ y = a x + b \\]\nis a linear regression model, because we are trying to predict an unknown value (\\(y\\)) from a linear combination of known values (\\(x\\) and \\(1\\)). To solve a linear regression model is to find the values of \\(a\\) and \\(b\\) that best match the training data, in hopes that it will best predict the test data."
  },
  {
    "href": "explainers/linear-regression.html#solving-a-linear-regression-model",
    "title": "Linear Regression",
    "section": "Solving a linear regression model",
    "text": "In order to find the best values for \\(a\\) and \\(b\\), we first need to define what we mean by best. Here, we will use a very simple notion, which says that we want to minimize how bad our model does over the entirety of the training data, and that we will count how badly the model does at each training point by the square of the difference between the predicted value and the observed value:\n\\[ E = \\sum_i (y_i - (ax_i + b))^2 \\]\n\\(E\\) stands for the “energy” of the model, or the “error” of the model, and we want to find the model that minimizes the error.2 This is simply a matter of taking the derivative of \\(E\\) with respect to \\(a\\) and \\(b\\), setting those values to zero, and solving the resulting system of equations.\nThe crucial observation in linear models is that, since we know the values \\(y\\_i\\) and \\(x\\_i\\) at training time, when we take derivatives of \\(E\\), the resulting expressions are always linear functions of \\(a\\) and \\(b\\). This is true even if the model we are fitting uses non-linear functions. For example, imagine that our model was, instead,\n\\[ y = a x^2 + b x + c. \\]\nEven though the model would be fitting parabolic curves to the data (instead of linear fits), the process of combining the simpler models (\\(1\\), \\(x\\) and \\(x^2\\)) is still linear 3.\n\nSetting up the matrices\n\n\nSolving the system of equations\nTBF."
  },
  {
    "href": "explainers/linear-regression.html#demo",
    "title": "Linear Regression",
    "section": "Demo",
    "text": "TBF. Click on the plot to add points.\n\n\n\n\nSource Code\n---\ntitle: Linear Regression\n---\n\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.4/lodash.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js\"></script>\n\nLinear regression is one of simplest ways of building a model that can\nmake predictions from existing data (the \"training data\"). Regression\nmodels are used to predict numbers (\"what will the temperature be\ntomorrow?\"), while classification models are used to predict discrete\noutcomes (\"will it rain tomorrow?\").\n\nAlthough linear regression is an elementary method in data analysis\nthat has existed for 200 years, it is robust, flexible, easy to\ncompute, easy to understand, often performs quite well, and, just as\nimportantly, the foundation upon which many modern regression models\nare built.\n\nWe give a more general perspective [in a separate piece](linear_least_squares.html).\n\n## Modeling data for linear regression\n\nThere are many ways to build a model from data. In regression, the\ntraining data is given as a set of pairs of *independent* and\n*dependent* variables. Independent variables are the part of the data\nour finished model will use to make predictions; dependent variables\nare what the predictions should be. So our training data always has\nboth values of the dependent and independent variables, but our\ntesting data only has the values of the independent variables. The job\nof the model is exactly to predict the value of the dependent\nvariable.\n\nLet's get a little more concrete. Imagine you are designing a\ncomputerized hygrometer: an instrument to read moisture content from\nthe air. You are in charge of writing the software that will read\nvarying amounts of voltage from the sensor and convert those readings\ninto humidity measurements. As input, you are given a set of voltage\nreadings and associated humidity values (the humidity values having\nbeen collected by some other trusted method). Let us call $x\\_i$ the\nvoltage values, and $y\\_i$ the humidity values.\n\nOur first decision is as to what model we will use for the\npredictions. This decision cannot come alone from the data (although\n[we will later discuss methods to help with this](model_assessment.html)). In\nlinear regression, the fundamental assumption is that the model is, well,\n*linear*: we look for the linear combination[^1] of simpler, known models\nthat best predicts the training examples.\n\nFor example, a model that says the humidity $y$ will be predicted as\n\n$$ y = a x + b $$\n\nis a linear regression model, because we are trying to predict an\nunknown value ($y$) from a linear combination of known values ($x$ and\n$1$). To *solve* a linear regression model is to find the values of\n$a$ and $b$ that best match the training data, in hopes that it will best\npredict the test data.\n\n## Solving a linear regression model\n\nIn order to find the *best* values for $a$ and $b$, we first need to\ndefine what we mean by *best*. Here, we will use a very simple notion,\nwhich says that we want to minimize how bad our model does over the\nentirety of the training data, and that we will count how badly the\nmodel does at each training point by the square of the difference\nbetween the predicted value and the observed value:\n\n$$ E = \\sum_i (y_i - (ax_i + b))^2 $$\n\n$E$ stands for the \"energy\" of the model, or the \"error\" of the model,\nand we want to find the model that minimizes the error.[^2] This is simply\na matter of taking the derivative of $E$ with respect to $a$ and $b$,\nsetting those values to zero, and solving the resulting system of\nequations.\n\nThe crucial observation in linear models is that, since we know the\nvalues $y\\_i$ and $x\\_i$ at training time, when we take derivatives of\n$E$, the resulting expressions are always linear functions of $a$ and\n$b$. This is true even if the model we are fitting uses non-linear functions. \nFor example, imagine that our model was, instead,\n\n$$ y = a x^2 + b x + c. $$\n\nEven though the model would be fitting parabolic curves to the data\n(instead of linear fits), the process of combining the simpler models\n($1$, $x$ and $x^2$) is still linear [^3].\n\n### Setting up the matrices\n\n\n\n\n\n### Solving the system of equations\n\nTBF.\n\n## Demo\n\nTBF. Click on the plot to add points.\n\n<div id=\"linear-regression-linear-demo\"></div>\n\n[^1]: A linear combination of a set of vectors is a weighted sum of those vectors, where the weights can be arbitrary values.\n[^2]: Different generalizations of this error function make up a surprisingly large fraction of modern methods in data analysis.\n[^3]: See [linear least squares]()"
  },
  {
    "href": "explainers/newton-polynomials.html#how-is-this-different-from-lagrange-polynomials",
    "title": "Newton Polynomials",
    "section": "How is this different from Lagrange polynomials?",
    "text": "One important difference is that the basis for Lagrange polynomials has polynomials all of the same degree. Similarly, the Lagrange basis does not depend on the order you provide the points.\n\nSource Code\n---\ntitle: Newton Polynomials\n---\n\nTBW.\n\n## How is this different from Lagrange polynomials?\n\nOne important difference is that the basis for [Lagrange\npolynomials](lagrange-polynomials.qmd) has polynomials all of the same\ndegree. Similarly, the Lagrange basis does not depend on the order you\nprovide the points."
  },
  {
    "href": "explainers/shapley-values.html#shapley-value-as-expected-marginal-reward",
    "title": "Shapley Values",
    "section": "Shapley value as expected marginal reward",
    "text": "Here is one way to define of the Shapley value, which we will spend some time unpacking:\n\\[\\varphi\\_r(i) = E_{s : s \\subset \\{1 \\ldots N\\} , i \\notin s}[r(s) - r(s \\backslash \\{i\\})]\\]\nIn other words, the Shapley value \\(\\varphi\\_r(i)\\) of player \\(i\\) in a game with reward \\(r\\) is the expected additional reward player \\(i\\) would add to each possible team they could join. This makes intuitive sense; the part that is not very intuitive is that the probability distribution that we use for this expectation does not give equal weight to every team. Instead, it gives equal weight to every order in which players could be added to an initially empty team. Let’s consider a simple 3 player game and think about Player 1 and the possible orderings: \\((1, 2, 3)\\), \\((1, 3, 2)\\), \\((2, 1, 3)\\), \\((2, 3, 1)\\), \\((3, 1, 2)\\), and \\((3, 2, 1)\\).\nHow do we weigh the contributions from Player 1 when they’re the first team to be added? In that case, the expectation computes \\(r(\\{1\\}) - r(\\{\\})\\), but that value needs to be counted twice, because there are two separate permutations in which Player 1 participates as the first player in the team. When Player 1 is the second player in the team, we must count two marginal contributions separately: \\(r(\\{1, 2\\}) - r(\\{2\\})\\) and \\(r(\\{1, 3\\} - r(\\{3\\})\\), but each of these only counts once, because after Player 1 is added to the set, there’s only one way to add an additional player. Finally, when Player 1 is the last player to be added to the team, we have only one contribution to consider, \\(r(\\{1, 2, 3\\} - r(\\{2, 3\\})\\), but we must count this twice, since there are two permutations that arrive at the team configuration {2, 3}. After adding Player 1 to the team, there’s only one way to proceed (“we’re done”), so no additional weighting is necessary. In 4 player games and above, most of the times the weighting will take into account the number of ways to arrive at the current team, and the number of times the new team will participate in future permutations.\nThis gives the classic definition of Shapley values, presented here as defined by Molnar1, but using our notation:\n\\[\\varphi_r(i) = \\sum_{s \\subset \\{1, \\ldots, n \\}} \\frac{|s|! (n - |s| - 1)!}{n!} (r(s \\cup \\{i\\}) - r(s))\\]\n(Most presentations of the Shapley value assume the set is non-empty, but here we’re allowing empty teams. This implicitly allows empty teams to have non-zero rewards, which is also not usually considered, but we have good reasons to need this later.)\n\nSource Code\n---\ntitle: Shapley Values\n---\n\n# Shapley Values\n\nImagine you have a game where any of $n$ players can play. This is a\ncooperative game, so each player can choose whether or not to\nparticipate in playing. When a subset $S \\subset \\{1 \\ldots N\\}$ of\nthe players play the game, they are given a reward, a number\n$r(S)$ ($r$, then, is a function from sets of players to numbers).\n\nIf players 1 and 2 work well together, then this will be reflected in\nthat the reward for the two of them playing together outweighs either\nof them playing by themselves and in that case, $v(\\{1, 2\\}) \\ge\nv(\\{1\\}) + v(\\{2\\})$. If they don't play well together, then the\nreward function will reflect that as well.\n\nWhat we will ultimately want to find is a way to define and calculate\nthe \"worth\" of each individual. Shapley values are an attractive way\nto do so. It's a classic tool in game theory, but it has found\napplications in data science as well. In data science, we will think\nof each *feature* of a dataset (ie. each column in a table) as a\nplayer, and we want to understand how each feature contributes to the\nvalue in (typically) a regression task.\n\n## Shapley value as expected marginal reward\n\nHere is one way to define of the Shapley value, which we will\nspend some time unpacking:\n\n$$\\varphi\\_r(i) = E_{s : s \\subset \\{1 \\ldots N\\} , i \\notin s}[r(s) - r(s \\backslash \\{i\\})]$$\n\nIn other words, the Shapley value $\\varphi\\_r(i)$ of player $i$ in a game with reward $r$ is the\nexpected _additional reward_ player $i$ would add to each possible\nteam they could join. This makes intuitive sense; the part that is not\nvery intuitive is that the probability distribution that we use for\nthis expectation does _not_ give equal weight to every team. Instead,\nit gives equal weight to every *order* in which players could be added\nto an initially empty team. Let's consider a simple 3 player game and\nthink about Player 1 and the possible orderings: $(1, 2, 3)$, \n$(1, 3, 2)$, \n$(2, 1, 3)$, \n$(2, 3, 1)$, \n$(3, 1, 2)$, and\n$(3, 2, 1)$.\n\nHow do we weigh the contributions from Player 1 \nwhen they're the first team to be added? In that case, the expectation\ncomputes $r(\\{1\\}) - r(\\{\\})$, but that value needs to be counted\n_twice_, because there are two separate permutations in which Player 1\nparticipates as the first player in the team. When Player 1 is the\n_second_ player in the team, we must count two marginal contributions\nseparately: $r(\\{1, 2\\}) - r(\\{2\\})$ and $r(\\{1, 3\\} - r(\\{3\\})$, but\neach of these only counts once, because after Player 1 is added to the\nset, there's only one way to add an additional player. Finally, when\nPlayer 1 is the _last_ player to be added to the team, we have only\none contribution to consider, $r(\\{1, 2, 3\\} - r(\\{2, 3\\})$, but we\nmust count this twice, since there are two permutations that arrive at\nthe team configuration \\{2, 3\\}. After adding Player 1 to the team,\nthere's only one way to proceed (\"we're done\"), so no additional\nweighting is necessary. In 4 player games and above, most of the times\nthe weighting will take into account the number of ways to arrive at\nthe current team, _and_ the number of times the new team will\nparticipate in future permutations.\n\nThis gives the classic definition of Shapley values, presented here as defined by Molnar[^2], but using our notation:\n\n$$\\varphi_r(i) = \\sum_{s \\subset \\{1, \\ldots, n \\}} \\frac{|s|! (n - |s| - 1)!}{n!} (r(s \\cup \\{i\\}) - r(s))$$\n\n(Most presentations of the Shapley value assume the set\nis non-empty, but here we're allowing empty teams. This implicitly allows\nempty teams to have non-zero rewards, which is also not usually\nconsidered, but we have good reasons to need this later.)\n\n\n[^1]: Osborne, Martin J., and Ariel Rubinstein. _A course in game theory_. MIT press, 1994.\n\n[^2]: Molnar, Christoph. _[Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)_. Last accessed 2021-07-27."
  },
  {
    "href": "explainers/model-assessment.html",
    "title": "Model Assessment",
    "section": "",
    "text": "How do we know that our model is good, or bad?\nTBD.\n\nSource Code\n---\ntitle: Model Assessment\n---\n\nHow do we know that our model is good, or bad?\n\nTBD."
  },
  {
    "href": "explainers/pca.html#two-ways-to-pca",
    "title": "Principal Components Analysis",
    "section": "Two ways to PCA",
    "text": "If you’ve read about PCA before, you might remember something like “the principal components are the eigenvectors of the covariance matrix”. This statement is true, but it doesn’t actually help you understand what the PCA is doing. Instead, we will look at the PCA in two ways."
  },
  {
    "href": "explainers/pca.html#pca-via-the-svd.",
    "title": "Principal Components Analysis",
    "section": "PCA via the SVD.",
    "text": "The easiest way to see that these two approaches are identical is to consider the singular value decomposition (SVD) of \\(M\\). The SVD of a matrix is a bit like the eigendecomposition, but it is more general: the SVD exists for any matrix, rectangular or square, symmetric or not. Concretely, the SVD of a \\(m \\times n\\) matrix \\(M\\) is\n\\[M = U \\Sigma V^T,\\]\nor a set of three matrices: \\(U\\) is an orthogonal \\(m \\times m\\) matrix (whose columns are known as the left singular vectors); \\(\\Sigma\\) is a diagonal, rectangular \\(m \\times n\\) matrix with non-negative, non-increasing values in the diagonal (known as the singular values), and \\(V^T\\) is an \\(n \\times n\\) orthogonal matrix (whose rows are known as the right singular vectors). Like the eigendecomposition, the SVD has two orthogonal matrices and a diagonal matrix. Unlike the eigendecomposition, the orthogonal matrices in the SVD are different from each other’s transpose, and they might even have different dimensions. The entries of the diagonal matrix of the SVD are never negative, unlike eigendecomposition. Finally, the SVD of any matrix always exists, but even some square matrices lack an eigendecomposition.\nBut enough about the SVD: let’s put it to use. Specifically, let’s look at the SVD of \\(\\tilde{X}\\):\n\\[\\begin{align}\\tilde{X} &=& U \\Sigma V^T \\\\ \\tilde{X} \\tilde{X}^T & = & U \\Sigma V^T V \\Sigma^T U^T \\\\ \\tilde{X}^T \\tilde{X} &=& V \\Sigma^T U^T U \\Sigma V^T \\end{align}\\]\nSince \\(U\\) and \\(V\\) are orthogonal, a lot of terms cancel:\n\\[\\begin{align} \\tilde{X} \\tilde{X}^T & = & U \\Sigma_1^2 U^T \\\\ \\tilde{X}^T \\tilde{X} &=& V \\Sigma_2^2 V^T \\end{align}\\]\n(We use \\(\\Sigma_1^2\\) and \\(\\Sigma_2^2\\) to differentiate them since the former is \\(m \\times m\\), and the latter is \\(n \\times n\\).) From here, we can see that there are a lot of relationships between the SVD of \\(\\tilde{X}\\) and the eigendecompositions of \\(\\tilde{X}\\tilde{X}^T\\), and that of \\(\\tilde{X}^T\\tilde{X}\\). Specifically, the singular values of \\(\\tilde{X}\\) are equal to the square roots of the eigenvalues of both \\(\\tilde{X}\\tilde{X}^T\\) and \\(\\tilde{X}^T\\tilde{X}\\); the left singular vectors of \\(\\tilde{X}\\) are the eigenvectors of \\(\\tilde{X}\\tilde{X}^T\\), and the right singular vectors are the eigenvectors of \\(\\tilde{X}^T\\tilde{X}\\).\nPutting all of these things together, and multiplying both sides of the SVD \\(\\tilde{X} = U \\Sigma V^T\\) on the right by \\(V\\), we get:\n\\[\\begin{align} \\tilde{X} V &=& U \\Sigma V^T V \\\\ \\tilde{X} V &=& U \\Sigma \\end{align}\\]\nNow the left side of the equation is the result of PCA by covariance-matrix algorithm, and the right side of the equation is the result of PCA by the inner-product algorithm! So they are truly the same thing."
  },
  {
    "href": "explainers/pca.html#but-why",
    "title": "Principal Components Analysis",
    "section": "But why",
    "text": "The computation of the PCA via eigenvectors of covariance matrices is much more intuitive, so why do we care about these two different approaches?\nThe reason is a little strange, but extremely practical. Sometimes, we don’t have access to the rows or columns of \\(X\\), but we do have access to the inner products. It is extremely useful to know that in these scenarios we can still recover principal components! This is the central insight of classical multidimensional scaling."
  },
  {
    "href": "explainers/pca.html#odds-and-ends",
    "title": "Principal Components Analysis",
    "section": "Odds and ends",
    "text": "The Centering Matrix\nThe procedure we’ve used above to convert \\(X\\) to \\(\\tilde{X}\\) can be represented by a matrix. This is sometimes useful to know, especially when the centering operation happens in the middle of other matrix manipulation. Specifically, if \\(H = (I - \\vec{1}\\vec{1}^T / m)\\), where \\(m\\) is the number of rows of \\(X\\), and \\(\\vec{1}\\) is an \\(m\\)-dimensional vector of all ones \\((1, 1, \\cdots, 1)\\), then\n\\[\\tilde{X} = H X.\\]\nIt is easy to see why this is the case. Expand the definition:\n\\[\\begin{align}\\tilde{X} &=& (I - \\vec{1}\\vec{1}^T / m) X \\\\ \\tilde{X} &=& X - \\vec{1}\\vec{1}^T X / m,\\end{align}\\]\nand now note that \\(\\vec{1}^T X\\) is a vector that stores the column sums of \\(X\\), and so \\(\\vec{1}^T X / m\\) stores the column averages. In addition, \\(\\vec{1} v\\) creates a matrix with \\(m\\) copies of \\(v\\). Putting this back into the expression we see that the column averages are copied into a matrix of the same size as \\(X\\), and then subtracted from \\(X\\) itself.\nAn interesting feature of the centering matrix is that you can write it from the left side (in which case the vectors are as big as there are rows in \\(X\\)), and \\(HX\\) will be centering the columns of X. But you can also write it from the right side, in which case the vectors are as big as there are columns in \\(X\\), and \\(XH\\) will then be centering the rows of X."
  },
  {
    "href": "explainers/pca.html#acknowledgments",
    "title": "Principal Components Analysis",
    "section": "Acknowledgments",
    "text": "Mingwei Li provided careful proofreading and exercise suggestions."
  },
  {
    "href": "explainers/convolution/index.html#moving-averages",
    "title": "Convolution",
    "section": "Moving averages",
    "text": "Often, we will think of one of the functions as a filter that changes the other function. In the example above, the rectangle function \\(g\\) serves as a “local average”, which the convolution operation “spreads” throughout the domain of \\(f\\).\n(TODO: animate the rectangle sliding over \\(f\\) on the bottom as the user hovers)"
  },
  {
    "href": "explainers/convolution/index.html#smoothing-filters",
    "title": "Convolution",
    "section": "Smoothing filters",
    "text": "Taking a rectangle and convolving it with itself makes a progressively smoother function:"
  },
  {
    "href": "explainers/convolution/index.html#properties",
    "title": "Convolution",
    "section": "Properties",
    "text": "Convolution is commutative \\(f \\star g = g \\star f\\); this means that you can choose any of the two functions to interpret at the “one which is shifting”.\nConvolution distributes with function sum; \\(f \\star (g + h) = f \\star g + f \\star h\\).\nConvolution is associative: \\(f \\star (g \\star h) = (f \\star g) \\star h\\), and so we just write \\(f \\star g \\star h\\) without risk of ambiguity. This also lets us replace repeated convolutions with different filters with a single convolution of the convolution of the filters. This is an important idea.\nThe derivative of a convolution factors to either function. Using \\(Df = D(f)(t) = (df/dx)(t)\\), we have that \\(D(f \\star g)/dx = Df \\star g = f \\star Dg\\). Using our filter interpretation above, this means that the derivative of the filtered function is the convolution of the function with the derivative of the filter. This is also an important idea.\nIf you define \\(If = I(f)(x)= \\int_{-\\infty}^{x} f(y) dy\\) as the operator that integrates a function, then \\(I\\) also factors in the same way as above: \\(I(f \\star g) = I(f) \\star g = f \\star I(g)\\). Combined, these two properties allow us to, under a convolution cancel integrations on \\(f\\) with derivatives on \\(g\\) and vice-versa. Since \\(I(D(f)) = f = D(I(f))\\), clearly we have \\(I(f) \\star D(g) = f \\star I(D(g)) = f \\star g\\), etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource Code\n---\ntitle: Convolution\n---\n\nThe _convolution_ is an operation that shows up very commonly in\nsignal processing. It takes two functions as input and produces\nanother function as the output. Here, we will use $\\star$ to denote\nthe operation, so we will think of $f \\star g$ as producing a new\nfunction. The simplest definition of convolution is given element-wise:\n\n$$(f \\star g)(x) = \\int_{-\\infty}^\\infty f(\\tau)g(x - \\tau) d\\tau$$\n\nUnpacking this a little bit, each specific element of the domain of $f\n\\star g$ is given by an integral over the entire real line. The\nintegral operation is effectively a comparison between $f$ and a\nflipped, shifted version of $g$, where the amount by which we shift\n$g$ is the parameter for the convolution. (We say \"comparison\" here in\na specific sense: $\\int_{-\\infty}^\\infty f(x) g(x) dx$ is a way to\ngive a legitimate inner product to the vector space of functions [^1],\nand a good way to think of inner products is as a \"similarity\" between\ntwo objects.)\n\nThis is much easier to understand with simple examples.\n\n## Moving averages\n\n<div id=\"f-chart\" class=\"widechart\"></div>\n\n<div id=\"g-chart\" class=\"widechart\"></div>\n\n<div id=\"fg-chart\" class=\"widechart\"></div>\n\nOften, we will think of one of the functions as a _filter_ that\nchanges the other function. In the example above, the rectangle\nfunction $g$ serves as a \"local average\", which the convolution\noperation \"spreads\" throughout the domain of $f$.\n\n(TODO: animate the rectangle sliding over $f$ on the bottom as\nthe user hovers)\n\n## Smoothing filters\n\nTaking a rectangle and convolving it with itself makes a progressively\nsmoother function:\n\n<div id=\"bspline0a-chart\" class=\"widechart\"></div>  \n<div id=\"bspline1-chart\" class=\"widechart\"></div>  \n<div id=\"bspline2-chart\" class=\"widechart\"></div>  \n<div id=\"bspline3-chart\" class=\"widechart\"></div>\n\n## Properties\n\n* Convolution is commutative $f \\star g = g \\star f$; this means that\n  you can choose any of the two functions to interpret at the \"one\n  which is shifting\".\n  \n* Convolution distributes with function sum; $f \\star (g + h) = f\n  \\star g + f \\star h$.\n  \n* Convolution is associative: $f \\star (g \\star h) = (f \\star g) \\star\n  h$, and so we just write $f \\star g \\star h$ without risk of\n  ambiguity. This also lets us replace repeated convolutions with\n  different filters with a single convolution of the convolution of\n  the filters. This is an important idea.\n\n* The derivative of a convolution factors to _either_ function. Using $Df = D(f)(t) = (df/dx)(t)$,\n  we have that $D(f \\star g)/dx = Df \\star g = f \\star Dg$. Using our filter\n  interpretation above, this means that the derivative of the filtered\n  function is the convolution of the function with the derivative of\n  the filter. This is also an important idea.\n\n* If you define $If = I(f)(x)= \\int_{-\\infty}^{x} f(y) dy$ as the operator \n  that integrates a function, then $I$ also factors in the same way as above:\n  $I(f \\star g) = I(f) \\star g = f \\star I(g)$. Combined, these two properties allow us\n  to, under a convolution cancel integrations on $f$ with\n  derivatives on $g$ and vice-versa. Since $I(D(f)) = f = D(I(f))$, clearly we have\n  $I(f) \\star D(g) = f \\star I(D(g)) = f \\star g$, etc.\n\n[^1]: If you want to be mathematically precise about it, this definition of inner product only works in a slightly more complicated setting, because some functions can differ in small number of places and still have the same inner products. This is handled by sending functions into appropriate equivalence classes.\n\n```{ojs}\n//| output: false\n//| echo: false\nimport { cscheid } from \"/js/cscheid/cscheid.js\";\nimport { main } from \"./main.js\";\n{\n  cscheid.setup.setupGlobals({ d3 });\n  main();\n}\n```"
  },
  {
    "href": "explainers/linear-least-squares.html",
    "title": "Linear Least Squares",
    "section": "",
    "text": "See the regularization explainer for now.\n\nSource Code\n---\ntitle: Linear Least Squares\n---\n\nSee the [regularization](regularization/index.qmd) explainer for now."
  },
  {
    "href": "explainers/duality.html",
    "title": "Duality",
    "section": "",
    "text": "The duality principle states that the smallest distance from a point to a convex shape is also the largest distance from a point to a hyperplane tangent to the convex shape.\n(Go ahead, drag the black point around.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource Code\n---\ntitle: Duality\n---\n\n<div id=\"div-duality\"></div>\n\nThe duality principle states that the <span class=\"blue\">**smallest**</span> distance from a point\nto a convex shape is also the <span class=\"red\">**largest**</span> distance from a point to a\nhyperplane tangent to the convex shape.\n\n(Go ahead, drag the black point around.)\n\n```{ojs}\n//| output: false\n//| echo: false\nimport { cscheid } from \"/js/cscheid/cscheid.js\";\nimport { main } from \"./duality.js\";\n{\n  cscheid.setup.setupGlobals({ d3 });\n  main();\n}\n```"
  },
  {
    "href": "explainers/kmeans/k-lines.html",
    "title": "K-Means",
    "section": "",
    "text": "\\(k\\)-lines\n\n\n\n\nSource Code\n---\ntitle: K-Means\n---\n\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.4/lodash.min.js\"></script>\n\n# $k$-lines\n\n<div id=\"div-klines\"></div>"
  },
  {
    "href": "explainers/kmeans/index.html#optimization-formulation-for-k-means",
    "title": "K-Means",
    "section": "Optimization formulation for \\(k\\)-means",
    "text": "Although \\(k\\)-means is usually described operationally (“find cluster centers, find assignments, repeat”), there’s a cleaner way to describe it in terms of an optimization criterion.\n\nMeans are minimizers\nThe mean of a set of vectors is another object that is often described operationally (“add the vectors, divide by the count”), but not as commonly described as the best object from some candidate set. It turns out that the mean is the minimizer of a very natural function: the sum of squared distances. In other words, the mean of a set of vectors is the vector which minimizes the sum of squared distances from itself to the vectors under consideration. Given a set of \\(n\\) vectors \\(\\{ v\\_0, \\ldots, v\\_{n-1} \\}\\), we define the error function to minimize as\n\\[ E(c) = (1/2) \\sum_i ||c - v_i||^2 \\]\nTaking the gradient with respect to \\(c\\) directly gives the answer:\n\\[ \\nabla E(c) = \\sum_i (c - v_i) = 0 \\]\n\\[ \\hat{c} = \\frac{\\sum_i v_i}{n} \\]\nWe use the hat notation \\(\\hat{c}\\) in analogy to linear least squares estimators like \\(\\hat{\\beta}\\) to highlight the fact that the mean of a set of points can be seen as the linear least squares estimates for this set of points under a model consisting only of constant functions.\n\n\nFormulation\nThe optimal solution for \\(k\\)-means finds the best set of \\(k\\) “cluster centers” and the best assignment of input points to cluster centers, where “best” is defined as minimizing the sum of squared distances from centers to vectors assigned to it.\nLet’s first set up some notation. We let the variable \\(j\\) range over cluster indices, from \\(1\\) to \\(k\\). The variable \\(i\\) will range over data points, from \\(1\\) to \\(n\\). We will use the \\(a\\_i\\) to mean the assignment of point \\(i\\). If \\(k = 3\\), then \\(a\\_i \\in \\\\{1, 2, 3\\\\}\\). In addition, we will use \\(c_j\\) to mean the center of cluster \\(j\\). A potential solution of the \\(k\\)-means problem is then some assignment \\(a\\), and some cluster centers \\(c\\). The error of any given solution is the sum of the squared distances from each point to the center of the cluster they’re assigned to:\n\\[ E(c, a) = \\sum_{i=1}^n \\sum_{j=1}^k || v_i - c_{a_i} || ^2 \\]\nThis formulation does not have a closed-form solution. What this means is that we need an actual algorithm to solve it.\n\n\nAlternating Optimization\n\\(k\\)-means is the quintessential “alternating optimization” algorithm: if a formulation is hard to solve at its entirety, it’s often easier to solve it in steps. In the case of \\(k\\)-means, if we have a guess for the centers, then finding the best assignment is easy: we simply iterate over all pairs of data points and centers and compute the best assignment exhaustively. And if we have a guess of assignments, finding the best centers for those assignments is also easy: it’s just the mean.\nBut does this algorithm terminate? And does it give an optimal solution?"
  },
  {
    "href": "explainers/kmeans/index.html#validity",
    "title": "K-Means",
    "section": "Validity",
    "text": "TBF."
  },
  {
    "href": "explainers/kmeans/index.html#additional-reading",
    "title": "K-Means",
    "section": "Additional reading",
    "text": "The choice of initialization for \\(k\\)-means can greatly affect how fast it converges (and good the results are). \\(k\\)-means++ offers a simple rule for initialization that has provable approximation guarantees.\n\nSource Code\n---\ntitle: K-Means\n---\n\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/numeric/1.2.6/numeric.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.4/lodash.min.js\"></script>\n\n# $k$-means\n\n$k$-means is one of the simplest algorithms for finding clusters in a\ndataset. \"Cluster\" is not a particularly well-defined concept, but the\ngeneral idea is that some elements of a dataset are more similar to\neach other than they are to other elements -- they form a cluster. By\ncalculating which points belong to which clusters, we can then\ntry to understand the datasets by understanding the clusters, which\ncan represent a significant reduction in complexity.\n\n<div id=\"div-kmeans\"></div>\n\n<div><span id=\"button-step\" style=\"margin-top:1em\"></span> <span id=\"button-reset\" style=\"margin-top:1em\"></span></div>\n\n## Optimization formulation for $k$-means\n\nAlthough $k$-means is usually described operationally (\"find cluster\ncenters, find assignments, repeat\"), there's a cleaner way to describe\nit in terms of an optimization criterion.\n\n### Means are minimizers\n\nThe *mean* of a set of vectors is another object that is often\ndescribed operationally (\"add the vectors, divide by the count\"), but\nnot as commonly described as the *best* object from some candidate set. It turns\nout that the mean is the minimizer of a very natural function: the sum\nof squared distances. In other words, the mean of a set of vectors is\nthe vector which minimizes the sum of squared distances from itself to\nthe vectors under consideration. Given a set of $n$ vectors $\\{ v\\_0, \\ldots, v\\_{n-1} \\}$, we\ndefine the error function to minimize as\n\n$$ E(c) = (1/2) \\sum_i ||c - v_i||^2 $$\n\nTaking the gradient with respect to $c$ directly gives the answer:\n\n$$ \\nabla E(c) = \\sum_i (c - v_i) = 0 $$\n\n$$ \\hat{c} = \\frac{\\sum_i v_i}{n} $$\n\nWe use the hat notation $\\hat{c}$ in analogy to linear least squares\nestimators like $\\hat{\\beta}$ to highlight the fact that the mean of a\nset of points can be seen as the linear least squares estimates for\nthis set of points under a model consisting only of constant\nfunctions.\n\n### Formulation\n\nThe optimal solution for $k$-means finds the best set of $k$ \"cluster\ncenters\" and the best assignment of input points to cluster centers,\nwhere \"best\" is defined as minimizing the sum of squared distances\nfrom centers to vectors assigned to it.\n\nLet's first set up some notation. We let the variable $j$ range over\ncluster indices, from $1$ to $k$. The variable $i$ will range over\ndata points, from $1$ to $n$. We will use the $a\\_i$ to mean the\nassignment of point $i$. If $k = 3$, then $a\\_i \\in \\\\{1, 2, 3\\\\}$. \nIn addition, we will use $c_j$ to mean the center of cluster\n$j$. A potential solution of the $k$-means problem is then some\nassignment $a$, and some cluster centers $c$. The error of any given\nsolution is the sum of the squared distances from each point to the\ncenter of the cluster they're assigned to:\n\n$$ E(c, a) = \\sum_{i=1}^n \\sum_{j=1}^k || v_i - c_{a_i} || ^2 $$\n\nThis formulation does not have a closed-form solution. What this means\nis that we need an actual algorithm to solve it.\n\n### Alternating Optimization\n\n$k$-means is the quintessential \"alternating optimization\" algorithm:\nif a formulation is hard to solve at its entirety, it's often easier\nto solve it in steps. In the case of $k$-means, if we have a guess for\nthe centers, then finding the best assignment is easy: we simply\niterate over all pairs of data points and centers and compute the best\nassignment exhaustively. And if we have a guess of assignments,\nfinding the best centers for those assignments is also easy: it's just\nthe mean.\n\nBut does this algorithm terminate? And does it give an optimal\nsolution?\n\n## Validity\n\nTBF.\n\n## Additional reading\n\nThe choice of initialization for $k$-means can greatly affect how fast\nit converges (and good the results\nare). [$k$-means++](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf)\noffers a simple rule for initialization that has provable\napproximation guarantees."
  },
  {
    "href": "explainers/adjoints-and-inverses.html#adjoints-are-nothing-like-inverses-except-when-they-are",
    "title": "Adjoints and Inverses",
    "section": "Adjoints are (nothing) like inverses — except when they are",
    "text": "A lot of data science and linear algebra revolves around finding ways to relate the vectors in the row space of a matrix to the vectors in the column space of a matrix. (In real matrix algebra, the adjoint is just the transpose. With complex matrix algebra you have to take complex conjugates of the entries as well.)\nLet’s start with \\(Mx = y\\). Inverses give you the value \\(x\\) from \\(y\\): \\(x = M^{-1} y\\). This only works sometimes, because \\(M^{-1}\\) might not exist. Adjoints give you one value \\(x'\\) in the column space \\(x' = M^T y\\), though \\(x'\\) could be quite different from \\(x\\). This always works, because \\(M^T\\) always exists.\nOften, we get away with pretending the adjoint is “some kind of inverse”. For example, a number of “ML explanation” techniques, like layer-wise relevance propagation, are really “just” adjoints in disguise: they “invert” the ML classifier when a true inversion is not possible.\nBut why would that make any kind of sense, and when does that work well? Here we explain the situation for linear operators.\n\nWhen \\(M\\) is square, the story is simple\nThe inverse of a square matrix is the thing that says that if \\(Mx = y\\), then \\(x = M^{-1} y\\). But actually finding the inverse is hard, even if you don’t do it explicitly. If you know something about \\(M\\), often you want to compute something simpler.\nFor square matrices, if you know that \\(M\\) is orthogonal (that is, \\(\\norm{Mx} = \\norm{x}\\) for all \\(x\\)), then \\(M^T = M^{-1}\\): the adjoint is precisely the inverse! In that case, recovering \\(x\\) from \\(y\\) is also computationally trivial.\n\n\nWhen \\(M\\) isn’t square\nWhen if \\(M\\) is not square, things get harder. The linear least squares problem is the direct generalization of “recover \\(x\\) from \\(y\\)” to non-square matrices. We are given a nonsquare matrix \\(M\\) and “observations” \\(y\\), and then we set up the optimization problem\n\\[x^* = \\argmin_x \\norm{Mx - y}^2.\\]\nAfter some calculus and linear algebra, we get the answer:\n\\[x^* = (X^T X)^{-1} X^T y. \\]\nIf \\(X\\) has full column rank, then \\(X^T X\\) is invertible. And here’s the interesting thing. If \\((X^T X)^{-1} \\approx I\\), then \\(x^* \\approx X^T y\\). This is the condition that makes the adjoint behave “like” the inverse!"
  },
  {
    "href": "explainers/adjoints-and-inverses.html#the-svd-always-rescues-us",
    "title": "Adjoints and Inverses",
    "section": "The SVD always rescues us",
    "text": "One extra cool part about this is that if you replace the inverse \\((X^T X)^{-1}\\) with the Moore-Penrose pseudo-inverse, \\((X^T X)^+\\), then the least-squares solution is always well-defined (and \\(x^*\\) is then also the minimum-norm \\(x\\) satisfying the \\(\\argmin\\) condition above).\nThen, we remember that the Moore-Penrose pseudoinverse is very easy to state when we have the SVD of \\(X\\), \\(X = U \\Sigma V^T\\):\n\\[X^+ = V \\Sigma^+ U^T,\\]\nwhere\n\\[\\Sigma^+_{ii} = \\left \\{ \\begin{array}{rl} 1/\\Sigma_{ii}, &\\textrm{if}\\ \\Sigma_{ii} \\neq 0 \\\\ 0, &\\textrm{otherwise.} \\end{array} \\right .\\]\nWe also know that \\(U\\) and \\(V\\) are orthogonal. With some more linear algebra, we can then conclude that \\(X^T \\approx (X^T X)^+ X^T\\) whenever the singular values of \\(X\\) are close to \\(1\\) or \\(0\\).\nThis also works for square matrices. If all singular values are close to \\(1\\), then \\(\\norm{Mx} \\approx \\norm{x}\\), and \\(M^T x \\\\approx M^{-1} x\\)."
  },
  {
    "href": "explainers/adjoints-and-inverses.html#tying-it-back-together",
    "title": "Adjoints and Inverses",
    "section": "Tying it back together",
    "text": "The definition of orthogonal square matrices \\(X^T = X^{-1}\\) (and so \\(X^T X = I\\)) is equivalent to saying that all singular values are equal to \\(1\\) exactly. When you allow non-square matrices, the “equality” between the adjoint \\(X^T\\) and the “inverse” \\(X^{-1}\\) (which cannot exist because \\(X\\) is non-square!) also “allows” zero singular values."
  }
]